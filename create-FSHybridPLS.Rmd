---
title: "Creating the ``r params$package_name`` R package"
author: "Jongmin Mun"
date: "The Date"
knit: litr::render
output: litr::litr_html_document
params:
  package_name: "FSHybridPLS" # <-- change this to your package name
  package_parent_dir: "." # <-- relative to this file's location
---

<!-- This Rmd file contains all the code needed to define an R package.  Press "Knit" in RStudio or more generally run `litr::render("name-of-this-file.Rmd")` to generate the R package.  Remember that when you want to modify anything about the R package, you should modify this document rather than the package that is outputted.
-->

## Package setup

We start by specifying the information needed in the DESCRIPTION file of the R package.
 
 
```{r package-setup, message=FALSE, results='hide'}
usethis::create_package(
  path = ".",
  fields = list(
    Package = params$package_name,
    Version = "0.0.0.9000",
Title = "Hybrid Penalized Partial Least Squares for Functional and Scalar Data",
    Description = "Implements a Penalized Partial Least Squares (PLS) regression framework designed for hybrid predictors that combine both functional (infinite-dimensional) and scalar (finite-dimensional) covariates. The algorithm maximizes the covariance between the hybrid predictors and a scalar response variable while incorporating a roughness penalty to ensure the smoothness of the estimated functional coefficient curves. The package includes tools for data simulation using matrix-normal distributions, two-step data normalization, and cross-validated model fitting.",
    Imports = "fda (>= 6.1.3), Matrix, funData, foreach, irlba, refund, mvnfast, R6",
    `Authors@R` = person(
      given = "First",
      family = "Last",
      email = "you@gmail.com",
      role = c("aut", "cre")
      )
  )
)
usethis::use_mit_license(copyright_holder = "F. Last")
```

#### Table of Contents
- [Helper Functions](#helper)
 

# Helper Functions {#helper}

### compute_gram_matrix

**Description:**
Calculates the Gram matrix for a given functional basis. The Gram matrix $G$ is a symmetric positive semi-definite matrix containing the inner products of the basis functions. Specifically, the entry $(i, j)$ corresponds to the integral of the product of the $i$-th and $j$-th basis functions over the domain:
$$G_{ij} = \langle b_i, b_j \rangle = \int b_i(t) b_j(t) \, dt$$

**Inputs:**

- `basis`: A basis object of class `basisfd` (typically created using the `fda` package).

**Output:**

- Returns a square numeric matrix where the entry at row $i$ and column $j$ is the inner product of basis functions $i$ and $j$.

```{r}
#' Compute the Gram matrix for a basis object
#'
#' This function calculates the inner product of a basis with itself to form
#' the Gram matrix.
#'
#' @param basis A basis object of class `basisfd` from the `fda` package.
#' @return A square matrix where entry (i,j) is the inner product of basis functions i and j.
#' @export
compute_gram_matrix <- function(basis) {
  # Ensure the input is a valid basis object from the fda package
  stopifnot(inherits(basis, "basisfd"))
  
  # Compute the inner product of the basis with itself
  # This calculates the integral of b_i(t) * b_j(t) using the metric defined on the basis
  fda::inprod(basis, basis) 
}
```

 


### is_same_basis

**Description:**
Checks if two composite functional data objects (represented as lists containing functional data) share the exact same basis functions. It verifies that both lists have the same length and that the basis for every corresponding element is identical.

**Inputs:**

- `input`: A list containing functional data objects (expected to have a `$functional_list` structure).
- `other`: Another list containing functional data objects to compare against.

**Output:**

- Returns `TRUE` if both lists have the same length and every corresponding basis is equivalent; otherwise returns `FALSE`.

```{r}
#' Check if two functional data lists share the same basis
#'
#' @description
#' Checks if two composite functional data objects share the exact same basis functions
#' by comparing their internal list structures.
#'
#' @param input A list containing functional data objects (expected to have a `$functional_list` structure).
#' @param other Another list containing functional data objects to compare against.
#' @return `TRUE` if both lists have the same length and every corresponding basis is equivalent; otherwise `FALSE`.
#' @export
is_same_basis <- function(input, other) {
  all(
    # Check if both lists have the same number of functional elements
    length(input$functional_list) == length(other$functional_list),
    
    # Check if the basis for each corresponding element is identical
    all(
      mapply(function(fd1, fd2) {
        # Use fda package function to check basis equality
        fda::is.eqbasis(fd1$basis, fd2$basis)
      }, input$functional_list, other$functional_list)
    )
  )
}
```

### are_all_gram_matrices_identical

**Description:**
Iterates through a list of matrices to verify if they are all numerically identical to the first matrix in the list. This function handles floating-point inaccuracies by using a tolerance parameter.

**Inputs:**

- `gram_list`: A list containing matrix objects to be compared.
- `tolerance` (Optional): A numeric value defining the tolerance for floating-point comparison. Defaults to the square root of machine epsilon.

**Output:**

- Returns `TRUE` if the list is empty, has one element, or if all matrices are identical to the first. Returns `FALSE` if any matrix differs.

```{r}
#' Check if all matrices in a list are identical (with tolerance for numeric comparison)
#'
#' This function iterates through a list of matrices and checks if all subsequent
#' matrices are numerically identical to the first matrix in the list.
#' It uses `all.equal` for robust comparison of numeric matrices, which accounts
#' for small floating-point differences.
#'
#' @param gram_list A list of matrices.
#' @param tolerance A numeric tolerance for comparing floating-point numbers.
#'                  Defaults to `sqrt(.Machine$double.eps)`.
#' @return A logical value: `TRUE` if all matrices are identical, `FALSE` otherwise.
#' @export
are_all_gram_matrices_identical <- function(gram_list, tolerance = sqrt(.Machine$double.eps)) {
  # Handle edge cases: empty list or list with a single matrix
  if (length(gram_list) == 0) {
    message("The input list is empty. Returning TRUE as there are no differences.")
    return(TRUE)
  }
  if (length(gram_list) == 1) {
    message("The input list contains only one matrix. Returning TRUE.")
    return(TRUE)
  }
  
  # Get the first matrix as the reference
  first_matrix <- gram_list[[1]]
  
  # Ensure the first element is a matrix
  if (!is.matrix(first_matrix)) {
    stop("The first element of 'gram_list' is not a matrix.")
  }
  
  # Iterate from the second matrix onwards and compare with the first
  for (i in 2:length(gram_list)) {
    current_matrix <- gram_list[[i]]
    
    # Ensure current element is a matrix
    if (!is.matrix(current_matrix)) {
      stop(paste("Element", i, "of 'gram_list' is not a matrix."))
    }
    
    # Compare the current matrix with the first matrix
    # all.equal returns TRUE if identical, or a character string describing differences
    # We convert the result to a logical TRUE/FALSE
    if (!isTRUE(all.equal(first_matrix, current_matrix, tolerance = tolerance))) {
      # If not equal, return FALSE immediately
      message(paste("Matrices at index 1 and", i, "are not identical."))
      return(FALSE)
    }
  }
  
  # If the loop completes, all matrices are identical
  return(TRUE)
}
```

 
### rep_fd

**Description:**
Takes a list of functional data objects (where each object represents a single sample/curve) and broadcasts them by replicating their coefficients. This creates new functional objects containing $n$ identical copies of the original curve.

**Inputs:**

- `fd_list`: A list of `fd` objects (functional data objects). Each object must have exactly one column of coefficients (a single sample).
- `n`: An integer specifying the number of replications desired.

**Output:**

- Returns a list of `fd` objects, where each object now contains $n$ replications of the original coefficients.

```{r}
#' Replicate a list of single-sample fd objects multiple times
#'
#' This function broadcasts a list of single-sample functional predictors
#' by replicating their coefficient columns.
#'
#' @param fd_list A list of `fd` objects, each representing a single sample.
#' @param n The number of replications desired.
#'
#' @return A list of `fd` objects, each with `n` replications.
#' @export
rep_fd <- function(fd_list, n) {
  # Validate input: must be a list of 'fd' objects
  if (!is.list(fd_list) || any(!vapply(fd_list, inherits, logical(1), "fd"))) {
    stop("Input must be a list of 'fd' objects.")
  }
  
  lapply(fd_list, function(fd_obj) {
    coef_mat <- fd_obj$coefs
    
    # Ensure the object represents a single sample (one column of coefficients)
    if (is.null(dim(coef_mat)) || ncol(coef_mat) != 1) {
      stop("Each fd object in the list must have one column of coefficients.")
    }
    
    # Create a new coefficient matrix by repeating the column 'n' times
    new_coefs <- matrix(rep(coef_mat, n), nrow = nrow(coef_mat), ncol = n)
    
    # Return a new fd object with the replicated coefficients and original basis
    fd(coef = new_coefs, basisobj = fd_obj$basis)
  })
}
```

# Hybrid predictor class

## Class definition and constructor
### predictor_hybrid

**Description:**
The `predictor_hybrid` class represents a hybrid random object $\mathbf{W} = (X, \mathbf{Z})$ that combines functional and scalar covariates into a unified Hilbert space structure.

- **Functional Part:** Let $\{X^{(k)}\}_{k=1, \ldots, K}$ be a collection of random functions defined on unit interval $\mathcal{T}_k := [0,1]$. Each $X^{(k)}$ belongs to $L^2([0,1])$, a Hilbert space of square-integrable functions. The multivariate functional object $X$ resides in the cartesian product space $\mathcal{F} = L^2([0,1]) \times \cdots \times L^2([0,1])$.
- **Hybrid Object:** We define the hybrid object $\mathbf{W} = (X, \mathbf{Z})$, where $\mathbf{Z}$ is a $p$-dimensional scalar covariate vector. This object belongs to the product space $\mathcal{H} = \mathcal{F} \times \mathbb{R}^p$.
- **Vector Notation:** The object can be evaluated at a multi-dimensional argument $\mathbf{t}$ as a $(K+p)$-dimensional vector: $\mathbf{W}[\mathbf{t}] = (X(\mathbf{t}), \mathbf{Z})^\top$.
- **Implementation:** The class is implemented as an S3 object containing a matrix for scalar parts, a list of `fd` objects for functional parts, and pre-computed Gram matrices to facilitate Hilbert space operations.

**Inputs:**

- `Z`: A numeric matrix of dimension $n\_sample \times n\_scalar$ representing the scalar predictors.
- `functional_list`: A list of functional predictors (typically `fd` objects from the `fda` package).
- `eval_point`: A specification for evaluation points (stored as metadata).

**Output:**

- Returns an object of class `predictor_hybrid`, which includes the original data, pre-computed Gram matrices, and roughness penalty matrices.

```{r}
#' Create a predictor_hybrid object (S3 version, automatic basis size and Gram matrix)
#'
#' Constructs a hybrid predictor object that stores both scalar and functional predictors,
#' and automatically computes Gram matrices (inner products of basis functions).
#'
#' @param Z A numeric matrix of dimension \code{n_sample x n_scalar} representing scalar predictors.
#' @param functional_list A list of functional predictors (e.g., \code{fd} objects from the \code{fda} package).
#' @param eval_point Points at which the functions are evaluated (stored as metadata).
#'
#' @return An object of class \code{predictor_hybrid}, containing scalar and functional data with Gram matrices.
#' @export
predictor_hybrid <- function(Z, functional_list, eval_point) {
  # Validate scalar input
  stopifnot(is.matrix(Z), is.numeric(Z))
  
  # Validate functional input
  stopifnot(is.list(functional_list))
  
  n_sample <- nrow(Z)
  n_scalar <- ncol(Z)
  n_functional <- length(functional_list)
  
  # Initialize lists for storing matrices and metadata
  gram_list <- gram_deriv2_list <- vector("list", n_functional)
  n_basis_list <- numeric(n_functional)
  
  # Iterate through each functional predictor to compute necessary matrices
  for (i in seq_len(n_functional)) {
    fd_i <- functional_list[[i]]
    stopifnot(inherits(fd_i, "fd"))
    
    # Ensure the number of functional samples matches the scalar samples
    n_fd_sample <- ncol(coef(fd_i))
    if (n_fd_sample != n_sample) {
      stop(sprintf("Functional predictor %d has %d replicates, but Z has %d rows.",
                   i, n_fd_sample, n_sample))
    }
    
    # Extract basis and compute inner product matrices
    basis_i <- fd_i$basis
    gram_list[[i]] <- compute_gram_matrix(basis_i)
    gram_deriv2_list[[i]] <- fda::getbasispenalty(basis_i) # Compute roughness penalty matrix
    
    n_basis_list[i] <- basis_i$nbasis
  }
  
  # Construct the S3 object
  structure(
    list(
      Z = Z,
      functional_list = functional_list,
      eval_point = eval_point,
      gram_list = gram_list,
      gram_deriv2_list = gram_deriv2_list, # Store the penalty matrices
      n_basis_list = n_basis_list,
      n_sample = n_sample,
      n_functional = n_functional,
      n_scalar = n_scalar
    ),
    class = "predictor_hybrid"
  )
}
```



### predictor_hybrid_from_coef

**Description:**
An alternative constructor that creates a single-sample `predictor_hybrid` object from a flat numeric coefficient vector. It reconstructs both the functional and scalar components by mapping the coefficients back onto the basis functions defined in a template `format` object.

**Important:** This function strictly assumes that all functional predictors in the `format` object share the same number of basis functions (). It verifies this assumption before proceeding.

**Inputs:**

- `format`: A `predictor_hybrid` object serving as a template. It must contain the necessary basis information (`functional_list`) and structural metadata (`n_scalar`, `n_functional`).
- `coef`: A numeric vector of length , where:
-  is the number of functional predictors.
-  is the number of basis functions (strictly enforced to be identical across all functional predictors).
-  is the number of scalar predictors.



**Output:**

- Returns a `predictor_hybrid` object representing a single sample (`n_sample = 1`), containing:
- Functional predictors rebuilt from the first  coefficients.
- A scalar predictor matrix `Z` derived from the remaining  coefficients.

```{r}
#' Construct a Single-Sample Predictor Hybrid Object from Coefficients
#'
#' Reconstructs a \code{predictor_hybrid} object representing one observation, using a numeric
#' coefficient vector. This alternative constructor maps the coefficients back into their functional
#' and scalar predictor representations based on the structure of a template \code{predictor_hybrid} object.
#'
#' @param format A \code{predictor_hybrid} object that provides the structure and basis information.
#' @param coef A numeric vector containing coefficients for both functional and scalar predictors.
#'
#' @return A \code{predictor_hybrid} object with updated \code{functional_list}, \code{Z}, and \code{n_sample = 1}.
#' @export
predictor_hybrid_from_coef <- function(format, coef) {
  # Extract metadata from the template
  basis_counts <- format$n_basis_list
  
  # Check if all functional predictors have the same number of basis functions
  if (length(unique(basis_counts)) != 1) {
    stop("All functional predictors must have the same number of basis functions (M).")
  }
  
  M <- basis_counts[1]  # Number of basis functions (identical across predictors)
  K <- format$n_functional # Number of functional predictors
  
  # Reconstruct each functional predictor
  for (ii in 1:K) {
    # Calculate indices for the current functional predictor in the flat vector
    start_idx <- (ii - 1) * M + 1
    end_idx   <- ii * M
    
    # Create a new fd object using the sliced coefficients and original basis
    format$functional_list[[ii]] <- fd(
      coef = as.matrix(coef[start_idx:end_idx]),
      basisobj = format$functional_list[[ii]]$basis
    )
  }
  
  # Reconstruct the scalar predictor part (Z)
  # The remaining coefficients after K*M belong to the scalar predictors
  scalar_start_idx <- K * M + 1
  format$Z <- t(as.matrix(coef[scalar_start_idx:length(coef)]))
  
  # Update sample count to 1 as this constructor creates a single observation
  format$n_sample <- 1
  
  return(format)
}
```


 

## Basic arithmetic

### add.predictor_hybrid

**Description:**
Performs element-wise addition of two `predictor_hybrid` objects. Functional predictors are combined using `plus.fd()` and `times.fd()` from the `fda` package.

**Usage:**
```r
add.predictor_hybrid(input, other, alpha = 1)
```

**Arguments:**

- `input`: A `predictor_hybrid` object.
- `other`: Another `predictor_hybrid` object to be added.
- `alpha`: A scalar multiplier applied to `other` before addition (default is 1).

**Value:**

- Returns a new `predictor_hybrid` object representing the result of the addition.

**Details:**

- This function assumes both objects have the same number and structure of functional and scalar predictors.
- Functional parts are scaled by `alpha` using `times.fd()` and then summed using `plus.fd()`.
- Scalar predictors are added using standard matrix addition.
- Supports broadcasting if one object represents a single sample (`n_sample = 1`) and the other represents multiple samples.

```{r}
#' Add two predictor_hybrid objects
#'
#' Performs element-wise addition of two \code{predictor_hybrid} objects. 
#' Supports broadcasting if one object is a single sample.
#'
#' @param xi_1 A \code{predictor_hybrid} object.
#' @param xi_2 Another \code{predictor_hybrid} object to be added.
#' @param alpha A scalar multiplier applied to \code{xi_2} before addition (default is 1).
#'
#' @return A new \code{predictor_hybrid} object representing the result of the addition.
#' @export
add.predictor_hybrid <- function(xi_1, xi_2, alpha = 1) {
  # Safe access to is.eqbasis() from the fda namespace
  is_eqbasis <- getFromNamespace("is.eqbasis", "fda")
  
  # Type checks
  if (!inherits(xi_1, "predictor_hybrid") || !inherits(xi_2, "predictor_hybrid")) {
    stop("Both inputs must be of class 'predictor_hybrid'.")
  }
  
  # Structural checks: Number of functional and scalar predictors must match
  if (xi_1$n_functional != xi_2$n_functional) {
    stop("Mismatch in number of functional predictors.")
  }
  if (xi_1$n_scalar != xi_2$n_scalar) {
    stop("Mismatch in number of scalar predictors.")
  }
  
  # Basis checks: Ensure corresponding functional predictors share the same basis
  for (i in seq_len(xi_1$n_functional)) {
    if (!is_eqbasis(xi_1$functional_list[[i]]$basis, xi_2$functional_list[[i]]$basis)) {
      stop("Functional predictors must have the same basis.")
    }
  }
  
  # Broadcasting Logic:
  # Ensure xi_1 is always the larger object (or equal) to simplify broadcasting
  if (xi_1$n_sample == 1 && xi_2$n_sample > 1) {
    tmp <- xi_1
    xi_1 <- xi_2
    xi_2 <- tmp
  }
  
  n1 <- xi_1$n_sample
  n2 <- xi_2$n_sample
  
  # Validate sample compatibility
  if (!(n1 == n2 || n2 == 1)) {
    stop("Sample sizes are incompatible for broadcasting.")
  }
  
  # Prepare components
  f1 <- xi_1$functional_list
  f2 <- xi_2$functional_list
  Z1 <- xi_1$Z
  Z2 <- xi_2$Z
  
  # Replicate fd and Z if broadcasting is needed (n2 == 1)
  if (n2 == 1 && n1 > 1) {
    f2 <- rep_fd(f2, n1)
    Z2 <- matrix(rep(c(Z2), n1), nrow = n1, byrow = TRUE)
  }
  
  # Combine functional predictors: f1 + alpha * f2
  new_functional_list <- Map(
    function(fd1, fd2) plus.fd(fd1, times.fd(alpha, fd2)),
    f1,
    f2
  )
  
  # Combine scalar predictors: Z1 + alpha * Z2
  new_Z <- Z1 + alpha * Z2
  
  # Construct and return the new predictor_hybrid object
  # Note: eval_point is carried over from the first object
  predictor_hybrid(Z = new_Z, functional_list = new_functional_list, eval_point = xi_1$eval_point)
}
```

### subtr.predictor_hybrid

**Description:**
Performs element-wise subtraction of two `predictor_hybrid` objects. Internally uses `add.predictor_hybrid` to compute the result by negating the second operand.

**Usage:**
```r
subtr.predictor_hybrid(input, other, alpha = 1)
```

**Arguments:**

- `input`: A `predictor_hybrid` object.
- `other`: Another `predictor_hybrid` object to be subtracted.
- `alpha`: A scalar multiplier applied to `other` before subtraction (default is 1).

**Value:**

- Returns a new `predictor_hybrid` object representing the result of the subtraction.

**Details:**

- This function assumes both objects have the same number and structure of functional and scalar predictors.
- It performs subtraction by internally calling `add.predictor_hybrid()` with `-alpha`.
- Functional parts are scaled using `times.fd()` and subtracted via `plus.fd()` with a negated factor.
- Scalar predictors are subtracted using standard matrix arithmetic.

```{r}
#' Subtract two predictor_hybrid objects
#'
#' Performs element-wise subtraction of two `predictor_hybrid` objects.
#' Internally uses `add.predictor_hybrid(input, other, alpha = -1)`.
#'
#' @param input A `predictor_hybrid` object.
#' @param other Another `predictor_hybrid` object to subtract.
#' @param alpha A scalar multiplier applied to `other` before subtraction (default is 1).
#'
#' @return A new `predictor_hybrid` object representing the result of subtraction.
#' @export
subtr.predictor_hybrid <- function(input, other, alpha = 1) {
  add.predictor_hybrid(input, other, alpha = -alpha)
}
```

 

### scalar_mul.predictor_hybrid

**Description:**
Multiplies all components of a `predictor_hybrid` object by a scalar. Functional components are scaled using `times.fd()` from the `fda` package, and scalar predictors are multiplied directly using matrix operations.

**Usage:**
```r
scalar_mul.predictor_hybrid(input, scalar)

```

**Arguments:**

- `input`: A `predictor_hybrid` object.
- `scalar`: A numeric value used to scale both scalar and functional components.

**Value:**

- Returns a new `predictor_hybrid` object with all components scaled by `scalar`.

**Details:**

- Functional predictors are scaled using `times.fd(scalar, fd_obj)` for each element.
- Scalar predictors (the matrix `Z`) are scaled elementwise using matrix multiplication.

```{r}
#' Multiply a predictor_hybrid object by a scalar
#'
#' Performs scalar multiplication on both the scalar and functional components
#' of a `predictor_hybrid` object. Functional predictors are scaled using
#' `times.fd()` from the `fda` package.
#'
#' @param input A `predictor_hybrid` object.
#' @param scalar A numeric value to multiply all components by.
#'
#' @return A new `predictor_hybrid` object scaled by `scalar`.
#' @export
scalar_mul.predictor_hybrid <- function(input, scalar) {
  if (!inherits(input, "predictor_hybrid")) {
    stop("Input must be of class 'predictor_hybrid'.")
  }
  if (!is.numeric(scalar) || length(scalar) != 1) {
    stop("Scalar must be a single numeric value.")
  }

  # Scale functional components
  new_functional_list <- lapply(input$functional_list, function(fd_obj) {
    times.fd(scalar, fd_obj)
  })

  # Scale scalar predictors
  new_Z <- scalar * input$Z

  # Reconstruct hybrid object
  # Passing eval_point from the original input to maintain metadata
  predictor_hybrid(
    Z = new_Z,
    functional_list = new_functional_list,
    eval_point = input$eval_point
  )
}
```


### inprod.predictor_hybrid

**Description:**
Computes the inner product between two `predictor_hybrid` objects. The inner product in the hybrid space  is defined as the sum of the inner products of the functional components in  and the scalar components in :

**Inputs:**

- `xi_1`: A `predictor_hybrid` object.
- `xi_2`: (Optional) Another `predictor_hybrid` object. Defaults to `xi_1` (computes norm squared if `xi_1` is a single sample).

**Output:**

- A numeric vector of inner products. If both inputs are single samples, returns a scalar.

**Details:**

- Supports broadcasting: if one input has a single observation (`n_sample = 1`) and the other has multiple, the single observation is broadcast across the others.
- Uses `fda::inprod` for functional integration and matrix multiplication for scalar parts.
```{r}
#' Inner product between two predictor_hybrid objects (with broadcasting)
#'
#' Computes the inner product between two `predictor_hybrid` objects,
#' including both functional and scalar components. Supports broadcasting
#' when one of the inputs has a single observation.
#'
#' @param xi_1 A `predictor_hybrid` object.
#' @param xi_2 Another `predictor_hybrid` object. If missing, defaults to `xi_1`.
#'
#' @return A numeric vector of inner products, or a scalar if both inputs contain a single observation.
#' @export
inprod.predictor_hybrid <- function(xi_1, xi_2 = NULL) {
  # Handle self-inner product
  if (is.null(xi_2)) xi_2 <- xi_1

  # Type checks
  if (!inherits(xi_1, "predictor_hybrid") || !inherits(xi_2, "predictor_hybrid")) {
    stop("Both inputs must be of class 'predictor_hybrid'.")
  }

  # Structural checks
  if (xi_1$n_functional != xi_2$n_functional) {
    stop("Mismatch in number of functional predictors.")
  }
  if (xi_1$n_scalar != xi_2$n_scalar) {
    stop("Mismatch in number of scalar predictors.")
  }

  # Swap so that broadcasting always applies to xi_2 (the smaller one)
  if (xi_1$n_sample == 1 && xi_2$n_sample > 1) {
    tmp <- xi_1
    xi_1 <- xi_2
    xi_2 <- tmp
  }

  n1 <- xi_1$n_sample
  n2 <- xi_2$n_sample

  if (!(n1 == n2 || n2 == 1)) {
    stop("Sample sizes are incompatible for broadcasting.")
  }

  # Prepare components
  f1 <- xi_1$functional_list
  f2 <- xi_2$functional_list
  Z1 <- xi_1$Z
  Z2 <- xi_2$Z

  # Replicate fd and Z if needed for broadcasting
  if (n2 == 1 && n1 > 1) {
    f2 <- rep_fd(f2, n1)
    Z2 <- matrix(rep(c(Z2), n1), nrow = n1, byrow = TRUE)
  }

  # Compute functional inner products
  # Iterates over samples (i) and sums the inner products of all functional predictors (j)
  inprod_functional <- vapply(seq_len(n1), function(i) {
    sum(vapply(seq_along(f1), function(j) {
      fda::inprod(f1[[j]][i], f2[[j]][i])
    }, numeric(1)))
  }, numeric(1))

  # Compute scalar inner products using row-wise summation
  inprod_scalar <- rowSums(Z1 * Z2)

  # Combine functional and scalar results
  result <- inprod_functional + inprod_scalar
  
  # Return scalar if both inputs were single samples
  if (n1 == 1 && n2 == 1) as.numeric(result) else result
}
```

 
### inprod_pen.predictor_hybrid

**Description:**
Computes the penalized inner product between two `predictor_hybrid` objects. This adds a roughness penalty term to the standard inner product, typically used for regularization.


**Inputs:**

- `xi_1`: A `predictor_hybrid` object.
- `xi_2`: (Optional) Another `predictor_hybrid` object.
- `lambda`: A numeric vector of length  (number of functional predictors) specifying the smoothing parameter for each function.

**Output:**

- A numeric vector (or scalar) representing the penalized inner product.
```{r}
#' Penalized Inner product between two predictor_hybrid objects
#'
#' Computes the inner product between two `predictor_hybrid` objects,
#' adding a roughness penalty term based on the second derivative.
#'
#' @param xi_1 A `predictor_hybrid` object.
#' @param xi_2 Another `predictor_hybrid` object. If missing, defaults to `xi_1`.
#' @param lambda A numeric vector of smoothing parameters (one for each functional predictor).
#'
#' @return A numeric vector of inner products.
#' @export
inprod_pen.predictor_hybrid <- function(xi_1, xi_2 = NULL, lambda) {
  
  # Compute the standard inner product first
  base_inprod <- inprod.predictor_hybrid(xi_1, xi_2)
  
  # Handle self-reference for xi_2
  if (is.null(xi_2)) xi_2 <- xi_1
  
  f1 <- xi_1$functional_list
  f2 <- xi_2$functional_list
  
  # Add penalty terms for each functional predictor
  # The penalty is lambda * <D^2 f1, D^2 f2>
  for (j in seq_len(xi_1$n_functional)) {
    base_inprod <- base_inprod + lambda[j] * fda::inprod(
      fdobj1 = f1[[j]],
      fdobj2 = f2[[j]],
      Lfdobj1 = 2, # Second derivative
      Lfdobj2 = 2  # Second derivative
    )
  }
  
  return(as.numeric(base_inprod))
}
```


### subset_predictor_hybrid

**Description:**
Extracts a specific observation from a multi-sample `predictor_hybrid` object and returns it as a new, single-sample `predictor_hybrid` object.

**Inputs:**

- `W`: A `predictor_hybrid` object containing multiple samples.
- `i`: Integer index of the sample to extract.

**Output:**

- A `predictor_hybrid` object containing only the -th observation.

```{r}
#' Extract a single observation from a predictor_hybrid object
#'
#' @param W A predictor_hybrid object with multiple samples.
#' @param i Integer index of the sample to extract.
#' @return A single-sample predictor_hybrid object.
#' @export
subset_predictor_hybrid <- function(W, i) {
  # Extract the i-th row of the scalar matrix
  new_Z <- matrix(W$Z[i, ], nrow = 1)
  
  # Extract the i-th column of coefficients for each functional object
  new_functional_list <- lapply(W$functional_list, function(fdobj) {
    fd(coef = matrix(coef(fdobj)[, i], ncol = 1), basisobj = fdobj$basis)
  })
  
  # Construct new object
  new_predictor <- predictor_hybrid(
    Z = new_Z, 
    functional_list = new_functional_list,
    eval_point = W$eval_point
  )
  
  return(new_predictor)
}
```



### replace_obs_hybrid

**Description:**
Replaces a specific observation within a `predictor_hybrid` object with a new single-sample hybrid object. This effectively updates the -th row of the scalar matrix and the -th column of the functional coefficients.

**Inputs:**

- `W`: The target `predictor_hybrid` object.
- `i`: The integer index of the observation to replace.
- `new_W`: The source `predictor_hybrid` object (must be a single sample).

**Output:**

- The modified `W` object.

```{r}
#' Replace a single observation in a predictor_hybrid object
#'
#' Replaces the i-th observation of a predictor_hybrid object with a new
#' single-sample predictor_hybrid object.
#'
#' @param W A predictor_hybrid object with one or more samples.
#' @param i An integer index specifying the observation to replace.
#' @param new_W A single-sample predictor_hybrid object to use for replacement.
#'
#' @return A predictor_hybrid object with the i-th observation replaced.
#' @export
replace_obs_hybrid <- function(W, i, new_W) {
  # Input validation
  if (!inherits(W, "predictor_hybrid") || !inherits(new_W, "predictor_hybrid")) {
    stop("Both W and new_W must be of class 'predictor_hybrid'.")
  }
  if (new_W$n_sample != 1) {
    stop("new_W must be a single-sample predictor_hybrid object.")
  }
  if (i < 1 || i > W$n_sample) {
    stop(paste("Index i must be between 1 and", W$n_sample))
  }
  if (W$n_scalar != new_W$n_scalar) {
    stop("Mismatch in number of scalar predictors.")
  }
  if (W$n_functional != new_W$n_functional) {
    stop("Mismatch in number of functional predictors.")
  }

  # Check for compatible basis objects
  is_eqbasis <- getFromNamespace("is.eqbasis", "fda")
  for (j in seq_len(W$n_functional)) {
    if (!is_eqbasis(W$functional_list[[j]]$basis, new_W$functional_list[[j]]$basis)) {
      stop("Functional predictors must have the same basis objects.")
    }
  }

  # Replace the i-th observation in the scalar matrix Z
  W$Z[i, ] <- new_W$Z[1, ]

  # Replace the i-th observation in each functional predictor
  for (j in seq_along(W$functional_list)) {
    # The coefficients are stored as a matrix, with columns corresponding to samples
    W$functional_list[[j]]$coefs[, i] <- new_W$functional_list[[j]]$coefs[, 1]
  }

  return(W)
}
```


### add_broadcast (Matrix)

**Description:**
A function that performs row-wise addition of a vector to a matrix. It broadcasts a single observation (vector) across all rows of the target matrix, verifying that the input is a matrix and the operand is a single observation.

**Inputs:**

- `input`: A numeric matrix.
- `other`: A numeric vector or a 1-row matrix to be added to every row of `input`.
- `alpha`: A scalar multiplier applied to `other` before addition.

**Output:**

- A matrix of the same dimensions as `input`.

```{r}
#' Broadcast Addition for Matrices
#'
#' Adds a single observation (vector) to every row of a matrix.
#' Checks that the input is a matrix and the operand is a single observation.
#'
#' @param input A numeric matrix.
#' @param other A numeric vector or a 1-row matrix.
#' @param alpha A scalar multiplier (default 1).
#' @return A matrix with the broadcasted addition applied.
#' @export
add_broadcast <- function(input, other, alpha = 1) {
  
  # 1. Type Check: Ensure input is a matrix
  if (!is.matrix(input)) {
    stop("Argument 'input' must be a matrix.")
  }

  # 2. Structural Check: Ensure RHS is a single observation.
  # This uses length() for vectors, and dim()[1] for matrices.
  is_matrix_other <- is.matrix(other)
  if (is_matrix_other && dim(other)[1] > 1) {
    stop("Argument 'other' must be a vector or a single-row matrix.")
  }

  # 3. Conversion: Convert a single-row matrix into a vector for broadcasting.
  # If 'other' is already a vector, this does nothing.
  if (is_matrix_other) {
    other <- as.vector(other)
  }
  
  # 4. Dimension Check: Ensure vector length matches matrix columns
  if (length(other) != ncol(input)) {
    stop("Length of 'other' must match number of columns in 'input'.")
  }

  # 5. Native Arithmetic: Use simple R arithmetic for high performance.
  # R broadcasts the vector 'other' across the rows of the matrix 'input'.
  # Transposing twice (t(t(...) ...)) ensures correct column-wise recycling 
  # logic is applied to rows.
  return(t(t(input) + alpha * other))
}
```
 

### subtr_broadcast (Matrix)

**Description:**
A function that performs row-wise subtraction of a vector from a matrix. It is a wrapper around `add_broadcast` that applies a negative multiplier.

**Inputs:**

- `input`: A numeric matrix.
- `other`: A numeric vector or a 1-row matrix to subtract from every row of `input`.
- `alpha`: A scalar multiplier applied to `other` (default 1).

**Output:**

- A matrix of the same dimensions as `input`.

```{r}
#' Broadcast Subtraction for Matrices
#'
#' Subtracts a single observation (vector) from every row of a matrix.
#'
#' @param input A numeric matrix.
#' @param other A numeric vector or a 1-row matrix.
#' @param alpha A scalar multiplier (default 1).
#' @return A matrix with the broadcasted subtraction applied.
#' @export
subtr_broadcast <- function(input, other, alpha = 1) {
  # Reuse add_broadcast with negated alpha
  add_broadcast(input, other, (-1 * alpha))
}
```

# One iteration

## small functions

### get_gram_matrix_block

**Description:**
Constructs the block-diagonal Gram matrix  for a hybrid predictor object. This matrix represents the inner product structure of the combined functional and scalar space. It is defined as:



where:

-  is the Gram matrix of the basis functions for the -th functional predictor, with entries .
-  is the identity matrix corresponding to the  scalar predictors (implying a standard Euclidean inner product for the scalar part).

**Inputs:**

- `obj`: A `predictor_hybrid` object containing the pre-computed Gram matrices (`gram_list`) and metadata.

**Output:**

- A sparse block-diagonal matrix (class `dgCMatrix` from the `Matrix` package) representing the global Gram matrix.
```{r}
#' Construct block-diagonal Gram matrix for hybrid predictor
#'
#' Creates a unified Gram matrix by placing the pre-computed functional Gram matrices
#' and a scalar identity matrix into a block-diagonal structure.
#'
#' @param obj A `predictor_hybrid` object.
#'
#' @return A sparse block-diagonal matrix of size `(total_dim x total_dim)`, where
#' `total_dim` is the sum of all functional basis sizes plus the number of scalar predictors.
#' @export
get_gram_matrix_block <- function(obj) {
  # Input validation
  if (!inherits(obj, "predictor_hybrid")) {
    stop("Input must be of class 'predictor_hybrid'.")
  }

  # 1. Retrieve the list of functional Gram matrices (pre-computed in the object)
  #    These correspond to the J^(k) blocks.
  gram_blocks <- obj$gram_list
  
  # 2. Append the Identity matrix for the scalar components
  #    This corresponds to the I_p block.
  #    We use 'diag' to create an Identity matrix of size n_scalar x n_scalar.
  gram_blocks[[length(gram_blocks) + 1]] <- diag(obj$n_scalar)
  
  # 3. Construct the sparse block-diagonal matrix
  #    Matrix::bdiag efficiently handles the block construction.
  Matrix::bdiag(gram_blocks)
}
```


### get_smoothing_param_hybrid

**Description:**
Constructs the block-diagonal smoothing parameter matrix . This matrix defines the regularization penalties applied to the coefficients. It is defined as:



where:

-  is the smoothing parameter for the -th functional predictor.
-  is the identity matrix of size  (the number of basis functions for predictor ).
-  is a zero matrix, indicating that the scalar predictors are not penalized in this step.

**Inputs:**

- `W`: A `predictor_hybrid` object providing dimension information.
- `lambda`: A numeric vector of length  (number of functional predictors). Each entry represents the penalty weight .

**Output:**

- A sparse block-diagonal matrix where the diagonal blocks contain the penalty weights for functional parts and zeros for the scalar part.
```{r}
#' Construct block-diagonal smoothing parameter matrix
#'
#' Generates a block-diagonal matrix containing regularization parameters. 
#' Functional coefficients are penalized by scaled identity matrices, while 
#' scalar coefficients receive zero penalty.
#'
#' @param W A `predictor_hybrid` object.
#' @param lambda A numeric vector of smoothing parameters, one for each functional predictor.
#'
#' @return A sparse block-diagonal matrix. The top-left blocks are `lambda[k] * I`, 
#' and the bottom-right block is a zero matrix for scalar covariates.
#' @export
get_smoothing_param_hybrid <- function(W, lambda) {
  # Input validation
  if (!inherits(W, "predictor_hybrid")) {
    stop("Input W must be of class 'predictor_hybrid'.")
  }
  if (length(lambda) != W$n_functional) {
    stop("Length of lambda must match the number of functional predictors.")
  }

  # 1. Create penalty blocks for functional predictors
  #    For each predictor k, create a scaled identity matrix: lambda[k] * I_Mk
  lambda_blocks <- lapply(seq_len(W$n_functional), function(ii) {
    nb <- W$functional_list[[ii]]$basis$nbasis
    lambda[ii] * diag(nb)
  })

  # 2. Create zero block for scalar predictors
  #    Scalar predictors are not penalized in this matrix, so we append a p x p zero matrix.
  lambda_blocks[[W$n_functional + 1]] <- matrix(0, nrow = W$n_scalar, ncol = W$n_scalar)

  # 3. Construct the sparse block-diagonal matrix
  Matrix::bdiag(lambda_blocks)
}
```

## PLS component computation 
 

### get_xi_hat_linear_pen

**Description:**
Computes the penalized PLS weight vector (direction) , represented as a `predictor_hybrid` object. This function solves for the coefficients that maximize the covariance between the predictor and the response , subject to a roughness penalty.

The optimization problem leads to a system of linear equations for each functional component :



where:

-  is the Gram matrix (`gram_list`).
-  is the penalty matrix (`gram_deriv2_list`).
-  are the basis coefficients of the functional data.
-  are the coefficients for the -th functional weight.

For the scalar part, the weights are simply the inner products . The final coefficient vector is normalized by the -norm (defined by the penalized inner product structure).

**Inputs:**

- `W`: A `predictor_hybrid` object containing the predictors.
- `y`: A numeric vector of response values (length ).
- `lambda`: A numeric vector of smoothing parameters (one for each functional predictor).

**Output:**

- A single-sample `predictor_hybrid` object representing the estimated direction .

```{r}
#' Compute Penalized PLS Weight Vector (Linear)
#'
#' Solves for the PLS weight direction xi that maximizes the covariance with y,
#' subject to roughness penalties on the functional components.
#'
#' @param W A `predictor_hybrid` object.
#' @param y A numeric vector of response values.
#' @param lambda A numeric vector of smoothing parameters.
#'
#' @return A single-sample `predictor_hybrid` object representing the weight direction xi.
#' @export
get_xi_hat_linear_pen <- function(W, y, lambda) {
  
  n <- W$n_sample
  K <- W$n_functional
  u <- gamma <- list()

  # --- Scalar Component ---
  # Compute scalar weights: v = Z^T * y
  v <- t(W$Z) %*% y
  
  # Initialize squared norm q with scalar part contribution
  # q = ||xi||^2_pen = sum(v^2) + functional_parts
  q <- sum(v^2)

  # --- Functional Components ---
  for (j in 1:K) {
    # Theta_t: coefficient matrix of the functional data (basis x samples)
    Theta_t <- W$functional_list[[j]]$coefs
    
    # B: Gram matrix for the j-th basis
    B <- W$gram_list[[j]]
    
    # Compute RHS of the linear system: u = B * Theta * y
    # This represents the unpenalized gradient term
    u[[j]] <- B %*% Theta_t %*% y

    # Linear System Matrix: R = Gram + lambda * Penalty
    R <- W$gram_list[[j]] + lambda[j] * W$gram_deriv2_list[[j]]
    
    # Solve for gamma_j: (Gram + Penalty) * gamma = Gram * Theta * y
    gamma[[j]] <- solve(R, u[[j]])

    # Update squared norm q:
    # Add contribution gamma^T * R * gamma (penalized norm of functional weight)
    q <- q + sum(gamma[[j]] * (R %*% gamma[[j]]))
  }
  
  # --- Combine and Normalize ---
  # Flatten gamma list and append scalar weights v
  d_vec <- c(do.call(c, gamma), v)
  
  # Normalize the vector to have unit penalized norm
  d_vec <- d_vec / sqrt(q)
  
  # Reconstruct the result as a predictor_hybrid object
  xi_hat <- predictor_hybrid_from_coef(format = W, coef = d_vec)
  
  return(xi_hat)
}
```
 

### get_rho

**Description:**
Computes the vector of PLS scores  by projecting the hybrid predictor data  onto the weight direction vector  (which represents ).

The projection is computed as:



where:

-  is the coefficient matrix for the -th functional predictor ().
-  is the Gram matrix ().
-  are the coefficients for the -th functional weight.
-  are the coefficients for the scalar weight.

**Inputs:**

- `d_vec`: A numeric vector containing the concatenated coefficients for all functional and scalar weights.
- `W`: A `predictor_hybrid` object providing the data and Gram matrices.

**Output:**

- A numeric vector of length  containing the PLS scores.

```{r}
#' Exact Hybrid Inner Product
#' Computes <W, xi> using basis inner product matrices.
#' @param W The hybrid predictor object (contains coefs for N samples)
#' @param xi The hybrid weight object (contains coefs for 1 direction)
#' @return A vector of scores (length N)
get_rho <- function(W, xi) {
  
  # 1. Scalar Part (Standard dot product)
  # (N x p_scalar) %*% (p_scalar x 1)
  scores <- as.matrix(W$Z) %*% as.matrix(xi$Z)
  
  # 2. Functional Part (Basis Matrix Algebra)
  # Loop through each functional variable
  for(k in seq_along(W$functional_list)) {
    
    # Extract basis and coefficients
    # Coefs_W is (Basis_Dim x N)
    # Coefs_xi is (Basis_Dim x 1)
    fd_W  <- W$functional_list[[k]]
    fd_xi <- xi$functional_list[[k]]
    
    # A. Compute J Matrix (Basis Inner Product)
    # J[i,j] = Integral(phi_i(t) * phi_j(t) dt)
    # Note: computed once per variable, very fast for B-splines
    J <- fda::inprod(fd_W$basis, fd_W$basis)
    
    # B. Compute Integral via Matrix Multiplication
    # Formula: Score = Coefs_W^T * J * Coefs_xi
    # Dimensions: (N x B) * (B x B) * (B x 1) -> (N x 1)
    
    # Optimization: Pre-multiply J * xi to get a "weighted weight"
    weighted_xi <- J %*% fd_xi$coefs 
    
    # Project data onto this weighted weight
    scores <- scores + t(fd_W$coefs) %*% weighted_xi
  }
  
  return(as.vector(scores))
}
```


 
 

## Response Residualization

### get_nu

**Description:**
Calculates the scalar regression coefficient  obtained by regressing the response vector  onto the PLS score vector . This minimizes the least squares error .


**Inputs:**

- `y`: A numeric vector of response values.
- `rho`: A numeric vector of PLS scores.

**Output:**

- A scalar value .
```{r}
#' Compute scalar regression coefficient (nu)
#'
#' Calculates the projection coefficient of the response vector y onto the score vector rho.
#'
#' @param y A numeric vector of response values.
#' @param rho A numeric vector of PLS scores.
#'
#' @return A scalar regression coefficient.
#' @export
get_nu <- function(y, rho) {
  # Compute numerator: inner product of y and rho
  # Compute denominator: squared norm of rho
  nu <- sum(y * rho) / sum(rho * rho)
  return(nu)
}
```
 

### residualize_y

**Description:**
Computes the residual of the response vector  after removing the component explained by the current PLS score .


**Inputs:**

- `y`: The current response vector.
- `rho`: The current PLS score vector.
- `nu`: The regression coefficient computed by `get_nu`.

**Output:**

- A numeric vector of residuals.

```{r}
#' Residualize the response vector
#'
#' Subtracts the projection of y onto rho from y.
#'
#' @param y The current response vector.
#' @param rho The current PLS score vector.
#' @param nu The regression coefficient.
#'
#' @return The residualized response vector.
#' @export
residualize_y <- function(y, rho, nu) {
  y_next <- y - nu * rho
  return(y_next)
}
```
 
 

## Predictor Residualization

### get_delta

**Description:**
Computes the hybrid regression coefficient  for regressing the predictor  onto the PLS scores .  is a single-sample `predictor_hybrid` object representing the direction in the predictor space that corresponds to .


**Inputs:**

- `W`: A `predictor_hybrid` object containing  samples.
- `rho`: A numeric vector of PLS scores (length ).

**Output:**

- A single-sample `predictor_hybrid` object .
```{r}
#' Compute hybrid regression coefficient (delta)
#'
#' Calculates the weighted average of the predictor observations W_i,
#' weighted by the scores rho_i.
#'
#' @param W A `predictor_hybrid` object.
#' @param rho A numeric vector of PLS scores.
#'
#' @return A single-sample `predictor_hybrid` object representing delta.
#' @export
get_delta <- function(W, rho) {
  # Initialize delta with the first weighted observation: rho[1] * W[1]
  delta <- scalar_mul.predictor_hybrid(subset_predictor_hybrid(W, 1), rho[1])
  
  # Iteratively add the remaining weighted observations: delta += rho[i] * W[i]
  for (i in 2:length(rho)) {
    W_i <- subset_predictor_hybrid(W, i)
    delta <- add.predictor_hybrid(delta, W_i, rho[i])
  }
  
  # Normalize by the squared norm of rho
  delta <- scalar_mul.predictor_hybrid(delta, 1 / sum(rho * rho))
  
  return(delta)
}
```



### residualize_predictor

**Description:**
Computes the residuals of the predictor object  by subtracting the component explained by .


**Inputs:**

- `W`: The current `predictor_hybrid` object.
- `rho`: The current PLS score vector.
- `delta`: The hybrid regression coefficient (from `get_delta`).

**Output:**

- A `predictor_hybrid` object containing the residualized predictors.
```{r}
#' Residualize the hybrid predictor
#'
#' Updates each observation in W by subtracting the projection onto rho (rho_i * delta).
#'
#' @param W The current `predictor_hybrid` object.
#' @param rho The current PLS score vector.
#' @param delta The hybrid regression coefficient (single-sample object).
#'
#' @return The residualized `predictor_hybrid` object.
#' @export
residualize_predictor <- function(W, rho, delta) {
  n <- length(rho)
  W_res <- W
  
  # Iterate through each sample to compute residuals
  for (i in 1:n) {
    # Extract the i-th observation
    W_i <- subset_predictor_hybrid(W, i)
    
    # Calculate residual: W_i_new = W_i - rho[i] * delta
    W_res_i <- subtr.predictor_hybrid(W_i, delta, rho[i])
    
    # Replace the observation in the result object
    W_res <- replace_obs_hybrid(W_res, i, W_res_i)
  }
  
  return(W_res)
}
```

 
# Main algorithm

### fit.hybridPLS

**Description:**
Implements the Hybrid Penalized Partial Least Squares (PLS) algorithm. This function iteratively extracts latent components that maximize the covariance between the hybrid predictors and the response, subject to roughness penalties.

**Algorithm Steps:**
For each iteration :

1. **Weight Direction ():** Computes the penalized weight vector that maximizes covariance with the current response residual .
2. **Scores ():** Projects the current predictor residuals  onto the weight direction to obtain PLS scores.
3. **loadings ():** Computes the regression coefficients for projecting  and  onto the scores .
4. **Deflation:** Updates the predictor  and response  by removing the variance explained by the current component (residualization).
5. **Coefficient Update:** Computes the "whitened" direction  (transforming weights back to the original predictor space) and updates the cumulative regression coefficient .

**Inputs:**

- `W`: Initial `predictor_hybrid` object.
- `y`: Numeric vector of response values.
- `n_iter`: Integer specifying the number of PLS components to extract.
- `lambda`: Numeric vector of smoothing parameters (one for each functional predictor).

**Output:**

- A list containing:
- `rho`: List of score vectors for each component.
- `xi`: List of weight directions (hybrid objects).
- `W`: List of residualized predictor objects at each step.
- `beta`: List of cumulative regression coefficients (hybrid objects) at each step.

```{r}
#' Fit Hybrid Penalized PLS with Optional Validation
#'
#' Runs the iterative PLS algorithm. If 'validation_data' is provided,
#' it computes and stores the RMSE on the validation set for each component.
#'
#' @param W Initial `predictor_hybrid` object (Training).
#' @param y Numeric vector of response values (Training).
#' @param n_iter Integer, number of components to extract.
#' @param lambda Numeric vector of smoothing parameters.
#' @param validation_data (Optional) A list containing `W_test` (predictor_hybrid) and `y_test`.
#'
#' @return A list containing the model (rho, xi, beta) and optional `validation_rmse`.
#' @export
fit.hybridPLS <- function(W, y, n_iter, lambda, validation_data = NULL) {
  # 1. Initialize storage
  W_now <- rho <- xi <- delta <- nu <- iota <- beta <- list()
  validation_rmse <- numeric(n_iter)
  
  # 2. Initialize current state
  W_now[[1]] <- W
  y_now <- y
  
  # 3. Main PLS Loop
  for (l in 1:n_iter) {
    # --- Step A: Weight Direction (xi) ---
    xi[[l]] <- get_xi_hat_linear_pen(W_now[[l]], y_now, lambda)
    
    # --- Step B: Scores (rho) ---
    rho[[l]] <- inprod.predictor_hybrid(W_now[[l]], xi[[l]])
    
    # --- Step C: Loadings (delta, nu) ---
    delta[[l]] <- get_delta(W_now[[l]], rho[[l]])
    nu[[l]] <- get_nu(y_now, rho[[l]])
    
    # --- Step D: Deflation ---
    W_now[[l + 1]] <- residualize_predictor(W_now[[l]], rho[[l]], delta[[l]])
    y_now <- residualize_y(y_now, rho[[l]], nu[[l]])
    
    # --- Step E: Cumulative Coefficient (Beta) ---
    iota[[l]] <- xi[[l]]
    
    if (l == 1) {
      beta[[l]] <- scalar_mul.predictor_hybrid(iota[[l]], nu[[l]])
    } else {
      for (u in 1:(l - 1)) {
        adjustment <- inprod.predictor_hybrid(delta[[u]], xi[[l]])
        iota[[l]] <- subtr.predictor_hybrid(iota[[l]], iota[[u]], adjustment)
      }
      beta[[l]] <- add.predictor_hybrid(beta[[l - 1]], iota[[l]], nu[[l]])
    }
    
    # --- Step F: Validation (Optional) ---
    if (!is.null(validation_data)) {
      # Predict on test set using the current cumulative beta
      y_pred_test <- inprod.predictor_hybrid(validation_data$W_test, beta[[l]])
      
      # Compute RMSE (standardized scale usually, but here just raw RMSE)
      validation_rmse[l] <- sqrt(mean((validation_data$y_test - y_pred_test)^2))
    }
  }
  
  return(list(
    rho = rho,
    xi = xi,
    beta = beta,
    W = W_now,
    delta = delta,
    nu = nu,
    validation_rmse = if (!is.null(validation_data)) validation_rmse else NULL
  ))
}
```


```{r}
# ==============================================================================
# TEST SCRIPT FOR HYBRID PLS
# ==============================================================================

# Prerequisites
library(fda)
library(Matrix)

# 1. DATA SIMULATION
# ------------------------------------------------------------------------------
set.seed(2025)
n_sample <- 50
n_scalar <- 3

# --- Create Functional Predictor 1 (B-Spline Basis) ---
nbasis1 <- nbasis2 <-  10
basis1 <- create.bspline.basis(rangeval = c(0, 1), nbasis = nbasis1)
# Generate random coefficients (nbasis x n_sample)
coefs1 <- matrix(rnorm(nbasis1 * n_sample), nrow = nbasis1, ncol = n_sample)
fd1 <- fd(coef = coefs1, basisobj = basis1)

# --- Create Functional Predictor 2 (Fourier Basis) ---

 # Generate random coefficients
coefs2 <- matrix(rnorm(nbasis2 * n_sample), nrow = nbasis2, ncol = n_sample)
fd2 <- fd(coef = coefs2, basisobj = basis1)

# --- Create Scalar Predictors ---
Z <- matrix(rnorm(n_sample * n_scalar), nrow = n_sample, ncol = n_scalar)

# --- Construct the True Hybrid Predictor Object ---
# (We need this to generate a 'true' y for testing)
W_train <- predictor_hybrid(
  Z = Z, 
  functional_list = list(fd1, fd2), 
  eval_point = seq(0, 1, length.out = 100)
)

# --- Generate Synthetic Response y ---
# Let's create a random 'true' beta to generate y
# True beta is just a random instance of a hybrid object
true_beta_coefs <- rnorm(nbasis1 + nbasis2 + n_scalar)
true_beta <- predictor_hybrid_from_coef(W_train, true_beta_coefs) # Create a single-sample hybrid beta

# Calculate y = <W, beta> + noise
y_clean <- inprod.predictor_hybrid(W_train, true_beta)
y <- y_clean + rnorm(n_sample, sd = 0.5)

message("Data generation complete.")
message(paste("Samples:", n_sample))
message(paste("Functional Predictors: 2 (Basis sizes:", nbasis1, ",", nbasis2, ")"))
message(paste("Scalar Predictors:", n_scalar))

# 2. RUNNING FIT.HYBRIDPLS
# ------------------------------------------------------------------------------
n_components <- 3
# Lambda: Small smoothing penalty for demonstration (one per functional predictor)
lambda_vec <- c(0.01, 0.01) 

message("\nRunning fit.hybridPLS...")
start_time <- Sys.time()

fit_result <- fit.hybridPLS(
  W = W_train, 
  y = y, 
  n_iter = n_components, 
  lambda = lambda_vec
)

end_time <- Sys.time()
message(paste("Fitting complete in:", round(end_time - start_time, 4), "seconds."))

# 3. VERIFICATION & INSPECTION
# ------------------------------------------------------------------------------

# --- A. Check Structure ---
message("\n--- Structure Verification ---")
if (length(fit_result$rho) == n_components && length(fit_result$beta) == n_components) {
  message("PASS: Output list lengths match n_iter.")
} else {
  stop("FAIL: Output list lengths do not match n_iter.")
}

# --- B. Check Beta Dimensions ---
final_beta <- fit_result$beta[[n_components]]
if (inherits(final_beta, "predictor_hybrid")) {
  message("PASS: Final beta is a 'predictor_hybrid' object.")
}

# --- C. Prediction & Accuracy ---
# Calculate fitted values using the beta from the last component
# y_hat = <W_train, beta_final>
y_hat <- inprod.predictor_hybrid(W_train, final_beta)

# Calculate MSE and Correlation
mse <- mean((y - y_hat)^2)
cor_y <- cor(y, y_hat)

message("\n--- Model Performance ---")
message(paste("Mean Squared Error (MSE):", round(mse, 4)))
message(paste("Correlation (y vs y_hat):", round(cor_y, 4)))

# --- D. Visual Check (Optional Plotting) ---
if(interactive()) {
  plot(y, y_hat, main = "Observed vs Fitted", xlab = "Observed y", ylab = "Fitted y")
  abline(0, 1, col = "red", lty = 2)
  text(min(y), max(y_hat), paste("Cor =", round(cor_y, 3)), adj = c(0, 1))
}

# 4. COMPONENT ANALYSIS (SCORES)
# ------------------------------------------------------------------------------
message("\n--- Component Analysis ---")
# Check orthogonality of scores (approximate, due to deflation method)
rho_matrix <- do.call(cbind, fit_result$rho)
cor_scores <- cor(rho_matrix)
print(round(cor_scores, 2))
message("(Note: Scores might not be perfectly orthogonal depending on the specific PLS deflation scheme used, but off-diagonals should be relatively low or structured.)")

message("\nTest script completed successfully.")
```

```{r}
# ==============================================================================
# TEST: ORTHOGONALITY PROPERTIES OF HYBRID PLS (Shared Basis)
# ==============================================================================

# Prerequisites
library(fda)
library(Matrix)

# 1. SETUP & DATA SIMULATION
# ------------------------------------------------------------------------------
set.seed(2025) # Fixed seed for reproducibility

# Dimensions
n_sample <- 50
n_scalar <- 5
nbasis   <- 5 

# --- Create Shared Basis (B-spline) ---
# Using the same basis for both functional predictors as requested
basis_shared <- create.bspline.basis(rangeval = c(0, 1), nbasis = nbasis)

# --- Generate Functional Predictors ---
# Functional Predictor 1
coefs1 <- matrix(rnorm(nbasis * n_sample), nrow = nbasis, ncol = n_sample)
fd1 <- fd(coef = coefs1, basisobj = basis_shared)

# Functional Predictor 2 (Using same basis, different coefficients)
coefs2 <- matrix(rnorm(nbasis * n_sample), nrow = nbasis, ncol = n_sample)
fd2 <- fd(coef = coefs2, basisobj = basis_shared)

# --- Generate Scalar Predictors ---
Z <- matrix(rnorm(n_sample * n_scalar), nrow = n_sample, ncol = n_scalar)

# --- Construct Hybrid Object ---
W <- predictor_hybrid(
  Z = Z, 
  functional_list = list(fd1, fd2), 
  eval_point = seq(0, 1, length.out = 100)
)

# --- Generate Response ---
# Random response is sufficient to test orthogonality mechanics
y <- rnorm(n_sample) 

# --- Parameters ---
n_comp <- 6
lambda <- c(0.5, 2) # Smoothing penalties for the two functional predictors

# 2. RUN PLS ALGORITHM
# ------------------------------------------------------------------------------
message(paste("Running fit.hybridPLS with", n_comp, "components..."))
fit <- fit.hybridPLS(W, y, n_iter = n_comp, lambda = lambda)

# 3. VERIFY PROPOSITION 1: Orthonormality of Directions (xi)
#    Note: For NIPALS (Predictor Deflation), we expect this to FAIL.
# ------------------------------------------------------------------------------
message("\n--- Testing Proposition 1: Orthonormality of Directions (xi) ---")

xi_orth_matrix <- matrix(0, n_comp, n_comp)

for (i in 1:n_comp) {
  for (j in 1:n_comp) {
    xi_orth_matrix[i, j] <- inprod_pen.predictor_hybrid(
      fit$xi[[i]], 
      fit$xi[[j]], 
      lambda
    )
  }
}

print(round(xi_orth_matrix, 4))

# Check against Identity matrix
is_orthonormal_xi <- all.equal(xi_orth_matrix, diag(n_comp), tolerance = 5e-2)

if (isTRUE(is_orthonormal_xi)) {
  message(" PASS: Directions are Orthonormal.")
} else {
  message(" FAIL: Directions are NOT orthonormal (Expected for NIPALS algorithm).")
  message("   (The off-diagonal values indicate correlation between weight vectors.)")
}


# 4. VERIFY PROPOSITION 2: Orthogonality of Scores (rho)
#    Note: For NIPALS, we expect this to PASS.
# ------------------------------------------------------------------------------
message("\n--- Testing Proposition 2: Orthogonality of Scores (rho) ---")

# Combine list of score vectors into a matrix (n_sample x n_comp)
rho_matrix <- do.call(cbind, fit$rho)

# Compute Correlation Matrix
rho_cor_matrix <- cor(rho_matrix)

# Print rounded correlation matrix
print(round(rho_cor_matrix, 4))

# Check off-diagonals (exclude diagonal 1s)
off_diag_vals <- rho_cor_matrix[upper.tri(rho_cor_matrix)]
off_diag_max <- max(abs(off_diag_vals))

# Tolerance check
if (off_diag_max < 5e-2) {
  message(" PASS: Scores are Orthogonal (Max correlation ~ 0).")
  message("   This confirms the algorithm correctly implements Predictor Deflation.")
} else {
  message(paste("  NOTE: Max off-diagonal correlation is", round(off_diag_max, 6)))
  message("          This is higher than expected. Check deflation logic.")
}

# Optional: Visual check of score orthogonality
if (interactive()) {
  image(1:n_comp, 1:n_comp, abs(rho_cor_matrix), 
        main = "Abs(Correlation) of PLS Scores",
        xlab = "Component", ylab = "Component",
        col = hcl.colors(12, "Blues", rev = TRUE))
}
```

# Simulation tools

## sample splitting

### create_idx_train_test

**Description:**
Randomly splits the indices of a dataset into training and testing sets based on a specified ratio. This function ensures that the indices are sorted and returned as a list containing both sets.

**Usage:**

```r
create_idx_train_test(n_sample, train_ratio)

```

**Arguments:**

- `n_sample`: Integer. The total number of samples in the dataset.
- `train_ratio`: Numeric. The proportion of samples to include in the training set (between 0 and 1).

**Value:**

- A list containing:
  - `idx_train`: A sorted integer vector of training indices.
  - `idx_test`: A sorted integer vector of testing indices.

```{r}
#' Split Sample Indices into Train and Test Sets
#'
#' Randomly partitions the indices of a dataset into training and testing sets.
#'
#' @param n_sample Integer. The total number of samples.
#' @param train_ratio Numeric. The proportion of samples for the training set (0 to 1).
#'
#' @return A list with named elements `idx_train` and `idx_test`.
#' @export
create_idx_train_test <- function(n_sample, train_ratio) {
  # Calculate number of training samples
  n_train <- floor(n_sample * train_ratio)
  
  # Sample indices without replacement
  idx_train <- sort(sample(seq_len(n_sample), n_train, replace = FALSE))
  
  # The remaining indices form the test set
  idx_test <- sort(setdiff(seq_len(n_sample), idx_train))
  
  return(list(
    idx_train = idx_train,
    idx_test = idx_test
  ))
}
```




### get_idx_train

**Description:**
A simplified version of the splitting function that returns only the training indices. Useful when the testing set is defined implicitly as the complement or not immediately needed.

**Usage:**

```r
get_idx_train(n_sample, train_ratio)

```

**Arguments:**

- `n_sample`: Integer. The total number of samples.
- `train_ratio`: Numeric. The proportion of samples for the training set.

**Value:**

- A sorted integer vector of training indices.

```{r}
#' Generate Train Indices
#'
#' Randomly generates a sorted vector of indices for a training set.
#'
#' @param n_sample Integer. Number of samples.
#' @param train_ratio Numeric. Ratio of train samples.
#' @return Vector of sorted train indices.
#' @export
get_idx_train <- function(n_sample, train_ratio) {
  n_train <- floor(n_sample * train_ratio)
  idx_train <- sort(sample(seq_len(n_sample), n_train, replace = FALSE))
  return(idx_train)
}
```



### create_idx_kfold

**Description:**
Generates indices for K-fold cross-validation. It relies on the `caret` package to create balanced folds and organizes them into pairs of training and validation indices for each fold.

**Usage:**

```r
create_idx_kfold(n_sample, n_fold)

```

**Arguments:**

- `n_sample`: Integer. The total number of samples.
- `n_fold`: Integer. The number of folds to create.

**Value:**

- A list of length `n_fold`. Each element is a list containing:
  - `idx_train`: Indices for training in this fold.
  - `idx_valid`: Indices for validation in this fold.


```{r}
#' Generate K-Fold Cross-Validation Indices
#'
#' Creates a list of training and validation indices for K-fold cross-validation.
#' Uses `caret::createFolds` to ensure stratified or random splitting.
#'
#' @param n_sample Integer. The total number of samples.
#' @param n_fold Integer. The number of folds.
#'
#' @return A list of lists, where each sub-list contains `idx_train` and `idx_valid`.
#' @export
create_idx_kfold <- function(n_sample, n_fold) {
  # Generate folds using caret (returns validation indices by default)
  idx_list <- caret::createFolds(seq_len(n_sample), k = n_fold, list = TRUE, returnTrain = FALSE)
  
  idx_list_list <- vector("list", length(idx_list))
  
  for (i in seq_along(idx_list)) {
    valid_hyper_index <- idx_list[[i]]
    # Training set is the complement of the validation set
    train_hyper_index <- setdiff(seq_len(n_sample), valid_hyper_index)
    
    idx_list_list[[i]] <- list(
      "idx_train" = train_hyper_index, 
      "idx_valid" = valid_hyper_index
    )
  }
  
  return(idx_list_list)
}
```



### n_sample.fd

**Description:**
Extracts the number of samples (replications) from a functional data object (`fd` class from the `fda` package). This implementation robustly checks the dimensions of the coefficient matrix.

**Usage:**

```r
n_sample.fd(fd_obj)

```

**Arguments:**

- `fd_obj`: An object of class `fd`.

**Value:**

- Integer. The number of samples (curves) in the object.

```{r}
#' Get Number of Samples from an fd Object
#'
#' Extracts the number of observations (replications) stored in a functional data object.
#'
#' @param fd_obj An object of class `fd`.
#' @return Integer representing the number of samples.
#' @export
n_sample.fd <- function(fd_obj) {
  if (!inherits(fd_obj, "fd")) {
    stop("Input must be of class 'fd'.")
  }
  # The coefficient matrix has dimensions (nbasis x n_sample)
  return(ncol(fd_obj$coefs))
}
```

 
### split.all

**Description:**
Splits a `predictor_hybrid` object and a corresponding response vector into training and testing sets based on a specified ratio. This function ensures that both the scalar matrix and the list of functional predictors are subsetted consistently.

**Inputs:**

- `W_hybrid`: A `predictor_hybrid` object containing the covariates.
- `response`: A numeric vector of response values.
- `train_ratio`: A numeric scalar (0 < ratio < 1) determining the size of the training set.

**Output:**

- A list containing:
  - `predictor_train`: The `predictor_hybrid` object for training.
  - `predictor_test`: The `predictor_hybrid` object for testing.
  - `response_train`: The response vector for training.
  - `response_test`: The response vector for testing.


```{r}
#' Split Hybrid Predictor and Response Data
#'
#' Randomly partitions the hybrid predictor object and response vector into 
#' training and testing sets based on a given ratio. Handles any number of 
#' functional predictors.
#'
#' @param W_hybrid A `predictor_hybrid` object containing all functional and scalar data.
#' @param response A numeric vector. The response variable corresponding to the samples in W_hybrid.
#' @param train_ratio A numeric scalar between 0 and 1 indicating the proportion of data to use for training.
#'
#' @return A list containing split train and test sets for predictors and response.
#' @export
split.all <- function(W_hybrid, response, train_ratio) {
  # --- Input Validation ---
  if (!inherits(W_hybrid, "predictor_hybrid")) {
    stop("'W_hybrid' must be a predictor_hybrid object.")
  }
  if (!is.vector(response) && !is.factor(response)) {
    stop("'response' must be a vector.")
  }
  if (length(response) != W_hybrid$n_sample) {
    stop("Response length must match the number of samples in W_hybrid.")
  }
  if (!is.numeric(train_ratio) || train_ratio <= 0 || train_ratio >= 1) {
    stop("'train_ratio' must be a numeric value strictly between 0 and 1.")
  }

  N <- W_hybrid$n_sample
  N_train <- floor(N * train_ratio)

  # Generate random indices
  # set.seed() should be called by the user before this function if reproducibility is needed
  train_idx <- sort(sample(seq_len(N), N_train))
  test_idx  <- sort(setdiff(seq_len(N), train_idx))

  # --- Subset Scalar Predictors ---
  Z_train <- W_hybrid$Z[train_idx, , drop = FALSE]
  Z_test  <- W_hybrid$Z[test_idx,  , drop = FALSE]

  # --- Subset Functional Predictors ---
  # Use lapply to handle any number of functional predictors (K)
  # The '[' operator on an fd object subsets the samples (replicates)
  fun_list_train <- lapply(W_hybrid$functional_list, function(fd_obj) fd_obj[train_idx])
  fun_list_test  <- lapply(W_hybrid$functional_list, function(fd_obj) fd_obj[test_idx])

  # --- Reconstruct Hybrid Objects ---
  # We reuse the original evaluation points
  W_train <- predictor_hybrid(Z = Z_train, functional_list = fun_list_train, eval_point = W_hybrid$eval_point)
  W_test  <- predictor_hybrid(Z = Z_test,  functional_list = fun_list_test,  eval_point = W_hybrid$eval_point)

  return(list(
    predictor_train = W_train,
    predictor_test  = W_test,
    response_train  = response[train_idx],
    response_test   = response[test_idx],
    train_idx       = train_idx, # Useful to return indices for reference
    test_idx        = test_idx
  ))
}
```

## normalization

 The goal is to perform a two-step standardization: first normalizing *within* each data type (functional vs. scalar) to account for unit differences, and second normalizing *between* the two types to balance their variances for the PLS framework.

### curve_normalize

**Description:**
Performs pointwise centering and scalar scaling on a functional data object. It subtracts a mean function and divides by a scaling constant (denominator). This implementation manually broadcasts the mean subtraction to ensure compatibility with `fda` objects where standard `minus.fd` can sometimes struggle with broadcasting single mean functions across multiple samples.

**Usage:**

```r
curve_normalize(functional, mean_functional, deno)

```

**Arguments:**

- `functional`: The `fd` object to be normalized.
- `mean_functional`: An `fd` object representing the mean (centroid) to subtract.
- `deno`: A numeric scalar scaling factor (e.g., total standard deviation).

**Value:**

- A normalized `fd` object.

```{r}
#' Normalize a functional data object
#'
#' Subtracts a mean function and divides by a scalar constant.
#' Manual broadcasting is used to ensure robust subtraction across samples.
#'
#' @param functional An `fd` object to normalize.
#' @param mean_functional An `fd` object representing the mean.
#' @param deno A numeric scalar for scaling (division).
#'
#' @return A normalized `fd` object.
#' @export
curve_normalize <- function(functional, mean_functional, deno) {
  # Replicate the mean coefficients to match the number of samples in 'functional'
  # This creates a matrix of identical mean columns for broadcasting
  mean_coefs_matrix <- coef(mean_functional) %*% matrix(1, ncol = ncol(coef(functional)))
  
  # Subtract mean and create new fd object
  functional_normalized <- fd(
    coef = coef(functional) - mean_coefs_matrix,
    basisobj = functional$basis
  )
  
  # Apply scaling factor
  # times.fd usually handles scalar multiplication well
  functional_normalized <- times.fd(1 / deno, functional_normalized)
  
  return(functional_normalized)
}
```



### curve_normalize_train_test

**Description:**
Standardizes the *functional* components of hybrid predictors. It centers them to have mean zero and scales them to have unit integrated variance. Crucially, the mean and standard deviation are derived **only from the training data** to prevent data leakage. These training statistics are then applied to normalize the test set.

**Usage:**

```r
curve_normalize_train_test(train, test)

```

**Arguments:**

- `train`: A `predictor_hybrid` object (training set).
- `test`: A `predictor_hybrid` object (test set).

**Value:**

- A list containing the normalized training and testing objects, along with the scaling factors used.

```{r}
#' Normalize Functional Predictors (Train/Test Split)
#'
#' Standardizes functional predictors to have mean zero and unit integrated variance.
#' Statistics (mean and SD) are computed from the training set and applied to the test set.
#'
#' @param train A `predictor_hybrid` object representing the training set.
#' @param test A `predictor_hybrid` object representing the testing set.
#'
#' @return A list containing:
#'   \item{predictor_train}{The normalized training object.}
#'   \item{predictor_test}{The normalized testing object.}
#'   \item{mean_train}{List of mean functions used for centering.}
#'   \item{deno_train}{Vector of scaling factors used.}
#' @export
curve_normalize_train_test <- function(train, test) {
  stopifnot(
    inherits(train, "predictor_hybrid"),
    inherits(test, "predictor_hybrid"),
    train$n_functional == test$n_functional
  )

  train_normalized <- train
  test_normalized <- test
  deno_train <- numeric(train$n_functional)
  mean_train <- list()

  # Iterate through each functional predictor
  for (k in seq_len(train$n_functional)) {
    # 1. Compute Statistics from TRAINING data
    train_fd <- train$functional_list[[k]]
    
    mean_train[[k]] <- fda::mean.fd(train_fd)
    sd_train <- fda::sd.fd(train_fd)
    
    # Scaling factor is the L2 norm of the standard deviation function
    deno_train[k] <- sqrt(fda::inprod(sd_train, sd_train))

    # Safety check for zero variance
    if (deno_train[k] <= 1e-10) {
      warning(paste("Functional predictor", k, "has near-zero variance. Skipping scaling."))
      deno_train[k] <- 1 
    }

    # 2. Normalize TRAINING set
    train_normalized$functional_list[[k]] <- curve_normalize(
      train_fd, 
      mean_train[[k]], 
      deno_train[k]
    )

    # 3. Normalize TEST set using TRAIN stats
    test_normalized$functional_list[[k]] <- curve_normalize(
      test$functional_list[[k]], 
      mean_train[[k]], 
      deno_train[k]
    )
  }

  return(list(
    predictor_train = train_normalized,
    predictor_test = test_normalized,
    mean_train = mean_train,
    deno_train = deno_train
  ))
}
```



### scalar_normalize

**Description:**
Helper function to normalize a scalar matrix. It subtracts a mean vector and divides by a standard deviation vector.

**Usage:**

```r
scalar_normalize(scalar_predictor, mean, deno)

```

**Arguments:**

- `scalar_predictor`: A numeric matrix ().
- `mean`: A numeric vector of length  (means).
- `deno`: A numeric vector of length  (standard deviations).

**Value:**

- A normalized numeric matrix.

```{r}
#' Normalize a scalar predictor matrix
#'
#' Centers columns by 'mean' and scales them by 'deno'.
#'
#' @param scalar_predictor Numeric matrix.
#' @param mean Numeric vector of column means.
#' @param deno Numeric vector of column standard deviations.
#' @return Normalized matrix.
#' @export
scalar_normalize <- function(scalar_predictor, mean, deno) {
  # Subtract mean from each row (broadcasted)
  # Uses the helper subtr_broadcast function defined previously
  scalar_predictor_normalized <- subtr_broadcast(scalar_predictor, mean)
  
  # Create a matrix of denominators matching the dimensions of the predictor
  deno_mat <- matrix(rep(deno, each = nrow(scalar_predictor)), 
                     nrow = nrow(scalar_predictor), 
                     ncol = length(deno))
  
  # Element-wise division
  scalar_predictor_normalized <- scalar_predictor_normalized / deno_mat
  return(scalar_predictor_normalized)
}
```



### scalar_normalize_train_test

**Description:**
Standardizes the *scalar* components () of the hybrid predictors. It ensures each scalar covariate has mean zero and unit variance. As with the functional part, statistics are derived solely from the training set.

**Usage:**

```r
scalar_normalize_train_test(predictor_train, predictor_test)

```

**Arguments:**

- `predictor_train`: A `predictor_hybrid` object (train set).
- `predictor_test`: A `predictor_hybrid` object (test set).

**Value:**

- A list containing the normalized hybrid objects and the statistics used.

```{r}
#' Normalize Scalar Predictors (Train/Test Split)
#'
#' Standardizes scalar predictors (Z) to have mean zero and unit variance.
#' Statistics are computed from the training set.
#'
#' @param predictor_train A `predictor_hybrid` object (train).
#' @param predictor_test A `predictor_hybrid` object (test).
#'
#' @return A list containing normalized objects and statistics.
#' @export
scalar_normalize_train_test <- function(predictor_train, predictor_test) {
  stopifnot(
    inherits(predictor_train, "predictor_hybrid"),
    inherits(predictor_test, "predictor_hybrid"),
    predictor_train$n_scalar == predictor_test$n_scalar
  )

  predictor_train_normalized <- predictor_train
  predictor_test_normalized <- predictor_test

  # 1. Compute Statistics from TRAINING data
  mean_train <- colMeans(predictor_train$Z)
  sd_train <- apply(predictor_train$Z, 2, sd)
  
  # Handle zero variance columns
  sd_train[sd_train == 0] <- 1

  # 2. Normalize TRAINING set
  predictor_train_normalized$Z <- scalar_normalize(
    predictor_train$Z,
    mean_train,
    sd_train
  )

  # 3. Normalize TEST set using TRAIN stats
  predictor_test_normalized$Z <- scalar_normalize(
    predictor_test$Z,
    mean_train,
    sd_train
  )

  return(list(
    predictor_train = predictor_train_normalized,
    predictor_test = predictor_test_normalized,
    mean_train = mean_train,
    sd_train = sd_train
  ))
}
```



### btwn_normalize_train_test

**Description:**
Performs the second step of standardization: balancing the variance *between* functional and scalar predictors. It scales the scalar component  by a factor  so that the total variance of the scalar part matches the total variance of the functional parts.

The weight  is calculated as:



where the numerator is the sum of squared  norms of all functional predictors, and the denominator is the sum of squared Euclidean norms of the scalar predictors (calculated on the training set).

**Usage:**

```r
btwn_normalize_train_test(train, test)

```

**Arguments:**

- `train`: A `predictor_hybrid` object (train set).
- `test`: A `predictor_hybrid` object (test set).

**Value:**

- A list containing the scaled hybrid objects and the scaling factor .

```{r}
#' Inter-Modality Normalization (Between Functional and Scalar)
#'
#' Scales the scalar predictors (Z) so that their total variability is comparable
#' to the total variability of the functional predictors. This addresses the 
#' scale invariance issue in PLS by weighting the scalar part.
#'
#' @param train A `predictor_hybrid` object (train).
#' @param test A `predictor_hybrid` object (test).
#'
#' @return A list containing normalized objects and the scaling factor.
#' @export
btwn_normalize_train_test <- function(train, test) {
  stopifnot(inherits(train, "predictor_hybrid"), inherits(test, "predictor_hybrid"))

  train_normalized <- train
  test_normalized <- test
  Z_s <- train$Z

  # --- 1. Calculate Numerator (Total Variance of Functional Parts) ---
  omega_numerator <- 0
  for (k in seq_len(train$n_functional)) {
    # Compute inner product matrix <X_i, X_j>
    # The diagonal elements <X_i, X_i> represent the squared L2 norm ||X_i||^2
    functional_inprod_matrix <- fda::inprod(
      train$functional_list[[k]],
      train$functional_list[[k]]
    )
    omega_numerator <- omega_numerator + sum(diag(functional_inprod_matrix))
  }

  # --- 2. Calculate Denominator (Total Variance of Scalar Part) ---
  omega_denominator <- sum(Z_s^2)

  # --- 3. Compute Weight omega ---
  if (omega_denominator == 0) {
    warning("Scalar component has zero variance. Skipping inter-modality scaling.")
    omega <- 1
  } else {
    omega <- omega_numerator / omega_denominator
  }

  # --- 4. Apply Scaling (sqrt(omega)) ---
  # This effectively weights the scalar inner product by omega
  scaling_factor <- sqrt(omega)
  
  train_normalized$Z <- scaling_factor * train$Z
  test_normalized$Z  <- scaling_factor * test$Z

  return(list(
    predictor_train = train_normalized,
    predictor_test = test_normalized,
    scaling_factor = scaling_factor
  ))
}
```



### split_and_normalize.all

**Description:**
A wrapper function that orchestrates the entire data preprocessing pipeline. It performs the following steps in order:

1. **Splitting:** divides predictors and response into training and testing sets.
2. **Within-Normalization (Functional):** Standardizes functional predictors (mean 0, unit integrated var).
3. **Within-Normalization (Scalar):** Standardizes scalar predictors (mean 0, unit var).
4. **Between-Normalization:** Balances variance between functional and scalar components.
5. **Response Normalization:** Standardizes the response variable  (mean 0, unit var).

**Usage:**

```r
split_and_normalize.all(W_hybrid, response, train_ratio)

```

**Arguments:**

- `W_hybrid`: The full `predictor_hybrid` dataset.
- `response`: The full response vector.
- `train_ratio`: Ratio for training data split.

**Value:**

- A comprehensive list containing processed training/testing sets for predictors and response, along with all normalization statistics.

```{r}
#' Split and Preprocess Hybrid Data
#'
#' Executes the full preprocessing pipeline: splitting data, standardizing within
#' modalities (functional/scalar), balancing variance between modalities, and 
#' standardizing the response.
#'
#' @param W_hybrid The full `predictor_hybrid` object.
#' @param response The numeric response vector.
#' @param train_ratio Numeric scalar (0-1) for train/test split.
#'
#' @return A list containing processed datasets and normalization parameters.
#' @export
split_and_normalize.all <- function(W_hybrid, response, train_ratio) {
  
  # 1. Split Data
  # Note: Requires the previously defined split.all function
  split_result <- split.all(W_hybrid, response, train_ratio)

  # 2. Normalize Functional Predictors (Within)
  curve_norm <- curve_normalize_train_test(
    split_result$predictor_train, 
    split_result$predictor_test
  )
  
  # 3. Normalize Scalar Predictors (Within)
  scalar_norm <- scalar_normalize_train_test(
    curve_norm$predictor_train, 
    curve_norm$predictor_test
  )
  
  # 4. Normalize Between Modalities (Balance Variances)
  btwn_norm <- btwn_normalize_train_test(
    scalar_norm$predictor_train,
    scalar_norm$predictor_test
  )
  
  # 5. Normalize Response Variable
  response_mean_train <- mean(split_result$response_train)
  response_sd_train   <- sd(split_result$response_train)
  
  # Prevent division by zero if response is constant
  if (response_sd_train == 0) response_sd_train <- 1
  
  response_train_std <- (split_result$response_train - response_mean_train) / response_sd_train
  response_test_std  <- (split_result$response_test  - response_mean_train) / response_sd_train
  
  response_norm <- list(mean_train = response_mean_train, sd_train = response_sd_train)

  # Return comprehensive results
  return(list(
    predictor_train = btwn_norm$predictor_train,
    predictor_test  = btwn_norm$predictor_test,
    response_train  = response_train_std,
    response_test   = response_test_std,
    
    # Store intermediate results and statistics for reference/reconstruction
    details = list(
      curve_normalize_result  = curve_norm,
      scalar_normalize_result = scalar_norm,
      btwn_normalize_result   = btwn_norm,
      response_normalize_result = response_norm,
      split_indices = list(train = split_result$train_idx, test = split_result$test_idx)
    )
  ))
}
```



 


# Baseline method: FPCA regression


## Modified MFPCA Function for Simultaneous Training and Prediction

The MFPCA (Multivariate Functional Principal Component Analysis) function computes multivariate functional PCA for functions defined across different (dimensional) domains.
We have modified the original implementation to admit a new input, **`mFData_predict`**, allowing the function to calculate PCA scores for **both the training data and new test (or validation) data simultaneously** in a single run. This change was necessary because the standard `predict.MFPCAfit` function does not permit computing scores for new, out-of-sample functions.
The multivariate functional principal component analysis inherently relies on a **univariate basis expansion** for each functional element ($X^{(j)}$). To simplify the code modification, we use only the univariate functional PCA option and have removed other options provided by the original MFPCA function. We modifies the main function and two internal functions.


### MFPCA

The main driver function. I have updated the description to explicitly detail the "Simultaneous Training and Prediction" modification.

```{r}
#' Multivariate Functional Principal Component Analysis (Modified)
#'
#' @description
#' Computes Multivariate Functional Principal Component Analysis (MFPCA) for functions
#' defined on different domains.
#'
#' \strong{Modification:} This version has been modified to accept an additional input,
#' \code{mFData_predict}. This allows the function to calculate PCA scores for both
#' the training data and a test/validation set simultaneously. This bypasses the
#' limitation of the standard \code{predict.MFPCAfit} function, which often does not
#' support out-of-sample prediction for this specific implementation.
#'
#' The function utilizes univariate basis expansions for each functional element
#' and combines them via PCA on the weighted concatenated scores.
#'
#' @param mFData A \code{multiFunData} object containing the training functional data.
#' @param mFData_predict A \code{multiFunData} object containing the test/validation functional data.
#' @param M Integer. The number of multivariate functional principal components to calculate.
#' @param uniExpansions A list of the same length as \code{mFData}, specifying the univariate basis expansion method for each element (e.g., "uFPCA").
#' @param weights Numeric vector. Weights for each functional element (defaults to 1).
#' @param fit Logical. If \code{TRUE}, returns the truncated Karhunen-Love representation.
#' @param approx.eigen Logical. If \code{TRUE}, uses \code{irlba} for approximate singular value decomposition (faster for large M).
#' @param bootstrap Logical. If \code{TRUE}, computes bootstrap confidence bands.
#' @param nBootstrap Integer. Number of bootstrap iterations (required if \code{bootstrap = TRUE}).
#' @param bootstrapAlpha Numeric. Significance level for bootstrap confidence bands.
#' @param bootstrapStrat Factor. Stratification factor for bootstrapping.
#' @param verbose Logical. If \code{TRUE}, prints progress messages.
#'
#' @return An object of class \code{MFPCAfit} containing:
#' \item{values}{Eigenvalues of the principal components.}
#' \item{functions}{The estimated multivariate functional principal component functions.}
#' \item{scores}{A matrix of scores for the \code{mFData} (training) objects.}
#' \item{scores.pred}{A matrix of scores for the \code{mFData_predict} (test) objects.}
#' \item{vectors}{The eigenvectors associated with the combined scores.}
#' \item{normFactors}{Normalization factors used during calculation.}
#' \item{meanFunction}{The estimated mean function.}
#'
#' @export 
MFPCA <- function(mFData, mFData_predict, M, uniExpansions, weights = rep(1, length(mFData)), fit = FALSE, approx.eigen = FALSE,
                  bootstrap = FALSE, nBootstrap = NULL, bootstrapAlpha = 0.05, bootstrapStrat = NULL,
                  verbose = options()$verbose)
{
  if(! inherits(mFData, "multiFunData"))
    stop("Parameter 'mFData' must be passed as a multiFunData object.")

  # number of components
  p <- length(mFData)
  # number of observations
  N <- nObs(mFData)

  if(!all(is.numeric(M), length(M) == 1, M > 0))
    stop("Parameter 'M' must be passed as a number > 0.")

  if(!(is.list(uniExpansions) & length(uniExpansions) == p))
    stop("Parameter 'uniExpansions' must be passed as a list with the same length as 'mFData'.")

  if(!(is.numeric(weights) & length(weights) == p))
    stop("Parameter 'weights' must be passed as a vector with the same length as 'mFData'.")

  if(!is.logical(fit))
    stop("Parameter 'fit' must be passed as a logical.")

  if(!is.logical(approx.eigen))
    stop("Parameter 'approx.eigen' must be passed as a logical.")

  if(!is.logical(bootstrap))
    stop("Parameter 'bootstrap' must be passed as a logical.")

  if(bootstrap)
  {
    if(is.null(nBootstrap))
      stop("Specify number of bootstrap iterations.")

    if(any(!(0 < bootstrapAlpha & bootstrapAlpha < 1)))
      stop("Significance level for bootstrap confidence bands must be in (0,1).")

    if(!is.null(bootstrapStrat))
    {
      if(!is.factor(bootstrapStrat))
        stop("bootstrapStrat must be either NULL or a factor.")

      if(length(bootstrapStrat) != nObs(mFData))
        stop("bootstrapStrat must have the same length as the number of observations in the mFData object.")
    }
  }

  if(!is.logical(verbose))
    stop("Parameter 'verbose' must be passed as a logical.")

  # dimension for each component
  dimSupp <- dimSupp(mFData)

  # get type of univariate expansions
  type <- vapply(uniExpansions, function(l){l$type}, FUN.VALUE = "")

  # de-mean functions -> coefficients are also de-meaned!
  # do not de-mean in uFPCA, as PACE gives a smooth estimate of the mean (see below)
  m <- meanFunction(mFData, na.rm = TRUE) # ignore NAs in data
  for(j in seq_len(p))
  {
    if(type[j] != "uFPCA")
      mFData[[j]] <- mFData[[j]] - m[[j]]
  }

  if(verbose)
    cat("Calculating univariate basis expansions (", format(Sys.time(), "%T"), ")\n", sep = "")



  ########### Below is modified by Jongmin Mun #############################
  # calculate univariate basis expansion for all components
  # Modification: Passes both training (data) and test (data_predict) to univDecomp
  uniBasis <- mapply(
    function(expansion, data, data_predict){
      do.call(univDecomp, c(
        list(funDataObject = data, funDataObject_predict = data_predict),
        expansion)
      )
    },
    expansion = uniExpansions,
    data = mFData,
    data_predict = mFData_predict,
    SIMPLIFY = FALSE
  )
  ########### Above is modified by Jongmin Mun #############################





  # for uFPCA: replace estimated mean in m
  for(j in seq_len(p))
  {
    if(type[j] == "uFPCA")
      m[[j]] <- uniBasis[[j]]$meanFunction
  }

  # Multivariate FPCA
  npc <- vapply(uniBasis, function(x){dim(x$scores)[2]}, FUN.VALUE = 0) # get number of univariate basis functions

  if(M > sum(npc))
  {
    M <- sum(npc)
    warning("Function MFPCA: total number of univariate basis functions is smaller than given M. M was set to ", sum(npc), ".")
  }

  # check if non-orthonormal basis functions used
  if(all(foreach::foreach(j = seq_len(p), .combine = "c")%do%{uniBasis[[j]]$ortho}))
    Bchol = NULL
  else
  {
    # Cholesky decomposition of B = block diagonal of Cholesky decompositions
    Bchol <- Matrix::bdiag(lapply(uniBasis, function(l){
      if(l$ortho)
        res <- Matrix::Diagonal(n = ncol(l$scores))
      else
        res <- Matrix::chol(l$B)

      return(res)}))
  }

  if(verbose)
    cat("Calculating MFPCA (", format(Sys.time(), "%T"), ")\n", sep = "")

  mArgvals <- if (utils::packageVersion("funData") <= "1.2") {
    getArgvals(mFData)
  } else {
    funData::argvals(mFData)
  }

  res <- calcMFPCA(N = N, p = p, Bchol = Bchol, M = M, type = type, weights = weights,
                   npc = npc, argvals = mArgvals, uniBasis = uniBasis, fit = fit, approx.eigen = approx.eigen)

  res$meanFunction <- m # return mean function, too

  names(res$functions) <- names(mFData)

  #############modified by jongmin mun


  #####################################
  if(fit)
  {
    res$fit <- m + res$fit # add mean function to fits
    names(res$fit) <- names(mFData)
  }

  # give correct names
  namesList <- lapply(mFData, names)
  if(!all(vapply(namesList, FUN = is.null, FUN.VALUE = TRUE)))
  {
    if(length(unique(namesList)) != 1)
      warning("Elements have different curve names. Use names of the first element for the results.")

    row.names(res$scores) <- namesList[[1]]

    if(fit)
      for(i in seq_len(p))
      names(res$fit[[i]]) <- namesList[[1]]
  }


  if(type[1] == "uFPCA"){
    scores.pred<-calcMFPCA_predict(N = nObs(mFData_predict),
                                   p = p,
                                   M = M,
                                   weights = weights,
                                   npc = npc,
                                   uniBasis = uniBasis,
                                   normFactors = res$normFactors,
                                   vectors_train=res$vectors,
                                   values_train=res$values
    )
  }
  res$scores.pred <- scores.pred

  class(res) <- "MFPCAfit"
  return(res)
}

```

### fpcaBasis

```{r}
#' Calculate Functional PCA Basis for Training and Prediction Data
#'
#' @description
#' A wrapper around the PACE (Principal Analysis by Conditional Expectation) algorithm.
#' It computes FPCA scores and eigenfunctions for the training data and simultaneously
#' computes projected scores for a prediction dataset.
#'
#' @param funDataObject The training functional data object.
#' @param funDataObject_predict The prediction functional data object.
#' @param nbasis Number of basis functions to use for smoothing.
#' @param pve Proportion of Variance Explained.
#' @param npc Number of Principal Components.
#' @param makePD Logical. Enforce positive definiteness of covariance.
#' @param cov.weight.type Weighting scheme for covariance estimation.
#'
#' @return A list containing training scores, prediction scores, orthogonality status, functions, and the mean.
#' @export  
################# modified by Jongmin Mun #####################
fpcaBasis <- function(funDataObject, funDataObject_predict, nbasis = 10, pve = 0.99, npc = NULL, makePD = FALSE, cov.weight.type = "none")
{
  # Calculate FPCA for training data
  FPCA <- PACE(funDataObject, predData = NULL, nbasis, pve, npc, makePD, cov.weight.type)
  # Calculate FPCA scores for prediction data using the training parameters (implicit in PACE call structure)
  FPCA_pred <- PACE(funDataObject = funDataObject, predData = funDataObject_predict, nbasis, pve, npc, makePD, cov.weight.type)

  return(list(scores = FPCA$scores,
              scores.pred = FPCA_pred$scores,
              ortho = TRUE,
              functions = FPCA$functions,
              meanFunction = FPCA$mu
  ))
}

```

### univDecomp

```{r}
#' Univariate Decomposition
#'
#' @description
#' Calculates the univariate basis decomposition for each element of a multivariate
#' functional object.
#'
#' \strong{Modification:} Accepts \code{funDataObject_predict} to facilitate
#' the pass-through of test data to the specific decomposition method (e.g., uFPCA).
#'
#' @param type Character string specifying the decomposition method (e.g., "uFPCA").
#' @param funDataObject The training functional data.
#' @param funDataObject_predict The prediction functional data.
#' @param ... Additional arguments passed to the specific decomposition function.
#'
#' @return A list containing the basis decomposition results.
#' @export 
univDecomp <- function(type, funDataObject, funDataObject_predict, ...)
{
  # Parameter checking
  if(is.null(type))
    stop("Parameter 'type' is missing.")
  else
  {
    if(!is.character(type))
      stop("Parameter 'type' must be a character string. See ?univDecomp for details.")
  }

  if(class(funDataObject) != "funData")
    stop("Parameter 'funDataObject' must be a funData object.")


  # get all arguments (except for function call and type)
  params <- list(...)

  # check if type and data are of correct type
  if(is.null(type))
    stop("univDecomp: must specify 'type'.")

  if(!inherits(type, "character"))
    stop("univDecomp: 'type' must be of class character.")

  if(is.null(funDataObject))
    stop("univDecomp: must specify 'funDataObject'.")

  if(class(funDataObject) != "funData")
    stop("univDecomp: 'funDataObject' must be of class funData.")

  params$funDataObject <- funDataObject # add funDataObject (-> make sure is evaluated in correct env.)

  ############### modified by Jongmin Mun ######################
  # Add prediction object to parameters if uFPCA is selected
  if (type == "uFPCA"){params$funDataObject_predict <- funDataObject_predict}
  ############### modified by Jongmin Mun ######################

  res <- switch(type,
                "given" = do.call(givenBasis, params),
                "uFPCA" = do.call(fpcaBasis, params),
                "UMPCA" = do.call(umpcaBasis, params),
                "FCP_TPA" = do.call(fcptpaBasis, params),
                "splines1D" = do.call(splineBasis1D, params),
                "splines1Dpen" = do.call(splineBasis1Dpen, params),
                "splines2D" = do.call(splineBasis2D, params),
                "splines2Dpen" = do.call(splineBasis2Dpen, params),
                "fda" = do.call(fdaBasis, params),
                "DCT2D" = do.call(dctBasis2D, params),
                "DCT3D" = do.call(dctBasis3D, params),
                stop("Univariate Decomposition for 'type' = ", type, " not defined!")
  )

  if(res$ortho == FALSE & is.null(res$B))
    stop("UnivDecomp: must provide integral matrix B for non-orthonormal basis functions.")

  return(res)
}

```

## Unmodified functions

### calcBasisIntegrals

```{r}
# define global variable j, used by the foreach package and confusing R CMD CHECK
#globalVariables('j')

#' Calculate Matrix of Basis Inner Products
#'
#' @description
#' Calculates the Gram matrix (matrix of scalar products) for a given set of basis functions.
#' If the element \eqn{X^{(j)}} is expanded in basis functions \eqn{b_i^{(j)}(t)},
#' this function returns the matrix \eqn{B^{(j)}} with entries:
#' \deqn{B^{(j)}_{mn} = \int_{\mathcal{T}_j} b_m^{(j)}(t) b_n^{(j)}(t) \mathrm{d} t}
#'
#' @section Warning:
#' This function is implemented only for functions on one- or two-dimensional domains.
#'
#' @param basisFunctions Array of basis functions of dimensions \code{npc x M1} or \code{npc x M1 x M2}.
#' @param dimSupp Dimension of the support of the basis functions (1 or 2).
#' @param argvals List of corresponding x-values (domain points).
#'
#' @return A matrix containing the inner products of all combinations of basis functions.
#' @export 
calcBasisIntegrals <- function(basisFunctions, dimSupp, argvals)
{
  npc <- dim(basisFunctions)[1]

  #  integral basis matrix
  B <- array(0, dim = c(npc, npc))


  if(dimSupp == 1) # one-dimensional domain
  {
    w <- funData::.intWeights(argvals[[1]])

    for(m in seq_len(npc))
    {
      for(n in seq_len(m))
        B[m, n] <- B[n, m] <- (basisFunctions[m, ]* basisFunctions[n, ])%*%w
    }
  }
  else # two-dimesional domain (otherwise function stops before!)
  {
    w1 <- t(funData::.intWeights(argvals[[1]]))
    w2 <- funData::.intWeights(argvals[[2]])

    for(m in seq_len(npc))
    {
      for(n in seq_len(m))
        B[m, n] <- B[n, m] <-  w1 %*%(basisFunctions[m, , ]* basisFunctions[n, ,])%*%w2
    }
  }

  return(B)
}

```

### calcMFPCA

```{r}
#' Internal Calculation of MFPCA
#'
#' @description
#' Implements the core MFPCA algorithm. It combines the univariate scores from
#' each functional element, applies weights, and performs PCA on the weighted
#' concatenated scores to extract the multivariate functional principal components.
#'
#' @param N Number of observations.
#' @param p Number of functional elements.
#' @param Bchol Cholesky decomposition of the basis integral matrix (or NULL if orthogonal).
#' @param M Number of multivariate components to calculate.
#' @param type Vector of character strings specifying the univariate expansion types.
#' @param weights Vector of weights for each functional element.
#' @param npc Vector containing the number of univariate basis functions for each element.
#' @param argvals List of argument values.
#' @param uniBasis List containing the univariate basis decompositions.
#' @param fit Logical. If TRUE, computes the fitted values (reconstruction).
#' @param approx.eigen Logical. If TRUE, uses approximate eigen decomposition.
#'
#' @return A list containing eigenvalues, eigenfunctions, scores, vectors, and normalization factors.
#' @export
calcMFPCA <- function(N, p, Bchol, M, type, weights, npc, argvals, uniBasis, fit = FALSE, approx.eigen = FALSE)
{
  # combine all scores
  allScores <- foreach::foreach(j = seq_len(p), .combine = "cbind")%do%{uniBasis[[j]]$scores}

  # block vector of weights
  allWeights <- foreach::foreach(j = seq_len(p), .combine = "c")%do%{rep(sqrt(weights[j]), npc[j])}

  Z <- allScores %*% Matrix::Diagonal(x = allWeights) / sqrt(N-1)

  # check if approximation is appropriate (cf. irlba)
  if(approx.eigen & (M > min(N, sum(npc))/2))
  {
    warning("Calculating a large percentage of principal components, approximation may not be appropriate.
            'approx.eigen' set to FALSE.")
    approx.eigen = FALSE
  }

  # check if non-orthonormal basis functions used and calculate PCA on scores
  if(is.null(Bchol))
  {
    if(approx.eigen)
    {
      tmpSVD <- irlba::irlba(as.matrix(Z), nv = M)

      vectors <- tmpSVD$v
      values <- tmpSVD$d[seq_len(M)]^2
    }
    else
    {
      if(sum(npc) > 1000)
        warning("MFPCA with > 1000 univariate eigenfunctions and approx.eigen = FALSE. This may take some time...")

      e <- eigen(stats::cov(allScores) * outer(allWeights, allWeights, "*"))

      values <- e$values[seq_len(M)]
      vectors <- e$vectors[,seq_len(M)]
    }
  }
  else
  {
    if(approx.eigen)
    {
      tmpSVD <- irlba::irlba(as.matrix(Matrix::tcrossprod(Z, Bchol)), nv = M)

      vectors <- Matrix::crossprod(Bchol, tmpSVD$v)
      values <- tmpSVD$d[seq_len(M)]^2
    }
    else
    {
      if(sum(npc) > 1000)
        warning("MFPCA with > 1000 univariate eigenfunctions and approx.eigen = FALSE. This may take some time...")

      e <- eigen(Matrix::crossprod(Bchol) %*% (stats::cov(allScores) * outer(allWeights, allWeights, "*")))

      values <- Re(e$values[seq_len(M)])
      vectors <- Re(e$vectors[,seq_len(M)])
    }
  }

  # normalization factors
  normFactors <- 1/sqrt(diag(as.matrix(Matrix::crossprod(Z %*% vectors))))

  ### Calculate scores
  scores <- Z %*% vectors * sqrt(N-1) # see defintion of Z above!
  scores <- as.matrix(scores %*% diag(sqrt(values) * normFactors, nrow = M, ncol = M)) # normalization

  ### Calculate eigenfunctions (incl. normalization)
  npcCum <- cumsum(c(0, npc)) # indices for blocks (-1)

  tmpWeights <- as.matrix(Matrix::crossprod(Z, Z %*%vectors))
  eFunctions <- foreach::foreach(j = seq_len(p)) %do% {
    univExpansion(type = type[j],
                  scores = 1/sqrt(weights[j] * values) * normFactors * t(tmpWeights[npcCum[j]+seq_len(npc[j]), , drop = FALSE]),
                  argvals = argvals[[j]],
                  functions = uniBasis[[j]]$functions,
                  params = uniBasis[[j]]$settings)
  }

  res <- list(values = values,
              functions = multiFunData(eFunctions),
              scores = scores,
              vectors = vectors,
              values = values,
              normFactors = normFactors,
              uniBasis = uniBasis,
              uniExpansions = uniExpansions
  )

  # calculate truncated Karhunen-Loeve representation (no mean here)
  if(fit)
    res$fit <- multivExpansion(multiFuns = res$functions, scores = scores)

  return(res)
}

```

### calcMFPCA_predict

```{r}
#' Prediction of MFPCA Scores
#'
#' @description
#' Calculates Multivariate Functional PCA scores for new (prediction) data based on
#' the eigenvectors and normalization factors derived from the training data.
#'
#' @param N Number of observations in the prediction set.
#' @param p Number of functional elements.
#' @param M Number of multivariate components.
#' @param weights Weights for each functional element.
#' @param npc Vector containing the number of univariate basis functions.
#' @param uniBasis List containing univariate basis decompositions for the prediction data.
#' @param normFactors Normalization factors derived from training.
#' @param vectors_train Eigenvectors derived from training.
#' @param values_train Eigenvalues derived from training.
#'
#' @return A matrix of MFPCA scores for the prediction data.
#' @export
calcMFPCA_predict <- function(N, p, M, weights, npc, uniBasis,
                              normFactors_train, vectors_train, values_train)
{
  # combine all scores
  allScores <- foreach::foreach(j = seq_len(p), .combine = "cbind")%do%{uniBasis[[j]]$scores.pred}

  # block vector of weights
  allWeights <- foreach::foreach(j = seq_len(p), .combine = "c")%do%{rep(sqrt(weights[j]), npc[j])}

  Z <- allScores %*% Matrix::Diagonal(x = allWeights) / sqrt(N-1)


  ### Calculate scores
  # Project the prediction Z matrix onto the training eigenvectors
  scores <- Z %*% vectors_train * sqrt(N-1) # see defintion of Z above!
  scores <- as.matrix(scores %*% diag(sqrt(values_train) * normFactors_train, nrow = M, ncol = M)) # normalization

  return(scores)
}

```

### .PACE

```{r}
#' Principal Analysis by Conditional Expectation (Internal)
#'
#' @description
#' Implements the PACE method (Yao et al., 2005) for univariate Functional Principal
#' Component Analysis. This handles sparse or irregular functional data by smoothing
#' covariance surfaces.
#'
#' @param X Argument values (domain).
#' @param Y Matrix of functional data (observations).
#' @param Y.pred Matrix of functional data for prediction (optional).
#' @param nbasis Number of basis functions for smoothing.
#' @param pve Proportion of variance explained.
#' @param npc Number of principal components.
#' @param makePD Logical. Enforce positive definiteness.
#' @param cov.weight.type Weighting type for covariance smoothing.
#'
#' @return A list containing fitted components (mu, values, efunctions, scores, etc.).
#' @export 
.PACE <- function(X, Y, Y.pred = NULL, nbasis = 10, pve = 0.99, npc = NULL, makePD = FALSE, cov.weight.type = "none")
{
  if (is.null(Y.pred))
    Y.pred = Y
  D = NCOL(Y)
  if(D != length(X)) # check if number of observation points in X & Y are identical
    stop("different number of (potential) observation points differs in X and Y!")
  I = NROW(Y)
  I.pred = NROW(Y.pred)
  d.vec = rep(X, each = I) # use given X-values for estimation of mu
  gam0 = mgcv::gam(as.vector(Y) ~ s(d.vec, k = nbasis))
  mu = mgcv::predict.gam(gam0, newdata = data.frame(d.vec = X))
  Y.tilde = Y - matrix(mu, I, D, byrow = TRUE)
  cov.sum = cov.count = cov.mean = matrix(0, D, D)
  for (i in seq_len(I)) {
    obs.points = which(!is.na(Y[i, ]))
    cov.count[obs.points, obs.points] = cov.count[obs.points, obs.points] + 1
    cov.sum[obs.points, obs.points] = cov.sum[obs.points, obs.points] + tcrossprod(Y.tilde[i, obs.points])
  }
  G.0 = ifelse(cov.count == 0, NA, cov.sum/cov.count)
  diag.G0 = diag(G.0)
  diag(G.0) = NA
  row.vec = rep(X, each = D) # use given X-values
  col.vec = rep(X, D) # use given X-values
  cov.weights <- switch(cov.weight.type,
                        none = rep(1, D^2),
                        counts = as.vector(cov.count),
                        stop("cov.weight.type ", cov.weight.type, " unknown in smooth covariance estimation"))

  npc.0 = matrix(mgcv::predict.gam(mgcv::gam(as.vector(G.0)~te(row.vec, col.vec, k = nbasis), weights = cov.weights),
                                   newdata = data.frame(row.vec = row.vec, col.vec = col.vec)), D, D)
  npc.0 = (npc.0 + t(npc.0))/2
  # no extra-option (useSymm) as in fpca.sc-method
  if (makePD) { # see fpca.sc
    npc.0 <- {
      tmp <- Matrix::nearPD(npc.0, corr = FALSE, keepDiag = FALSE,
                            do2eigen = TRUE, trace = options()$verbose)
      as.matrix(tmp$mat)
    }
  }

  ### numerical integration for calculation of eigenvalues (see Ramsay & Silverman, Chapter 8)
  w <- funData::.intWeights(X, method = "trapezoidal")
  Wsqrt <- diag(sqrt(w))
  Winvsqrt <- diag(1/(sqrt(w)))
  V <- Wsqrt %*% npc.0 %*% Wsqrt
  evalues = eigen(V, symmetric = TRUE, only.values = TRUE)$values
  ###
  evalues = replace(evalues, which(evalues <= 0), 0)
  npc = ifelse(is.null(npc), min(which(cumsum(evalues)/sum(evalues) > pve)), npc)
  efunctions = matrix(Winvsqrt%*%eigen(V, symmetric = TRUE)$vectors[, seq(len = npc)], nrow = D, ncol = npc)
  evalues = eigen(V, symmetric = TRUE, only.values = TRUE)$values[seq_len(npc)]  # use correct matrix for eigenvalue problem
  cov.hat = efunctions %*% tcrossprod(diag(evalues, nrow = npc, ncol = npc), efunctions)
  ### numerical integration for estimation of sigma2
  T.len <- X[D] - X[1] # total interval length
  T1.min <- min(which(X >= X[1] + 0.25*T.len)) # left bound of narrower interval T1
  T1.max <- max(which(X <= X[D] - 0.25*T.len)) # right bound of narrower interval T1
  DIAG = (diag.G0 - diag(cov.hat))[T1.min :T1.max] # function values
  # weights
  w <- funData::.intWeights(X[T1.min:T1.max], method = "trapezoidal")
  sigma2 <- max(1/(X[T1.max]-X[T1.min]) * sum(DIAG*w, na.rm = TRUE), 0) #max(1/T.len * sum(DIAG*w), 0)
  ####
  D.inv = diag(1/evalues, nrow = npc, ncol = npc)
  Z = efunctions
  Y.tilde = Y.pred - matrix(mu, I.pred, D, byrow = TRUE)
  fit = matrix(0, nrow = I.pred, ncol = D)
  scores = matrix(NA, nrow = I.pred, ncol = npc)
  # no calculation of confidence bands, no variance matrix
  for (i.subj in seq_len(I.pred)) {
    obs.points = which(!is.na(Y.pred[i.subj, ]))
    if (sigma2 == 0 & length(obs.points) < npc) {
      stop("Measurement error estimated to be zero and there are fewer observed points than PCs; scores cannot be estimated.")
    }
    Zcur = matrix(Z[obs.points, ], nrow = length(obs.points),
                  ncol = dim(Z)[2])
    ZtZ_sD.inv = solve(crossprod(Zcur) + sigma2 * D.inv)
    scores[i.subj, ] = ZtZ_sD.inv %*% crossprod(Zcur, Y.tilde[i.subj, obs.points])
    fit[i.subj, ] = t(as.matrix(mu)) + tcrossprod(scores[i.subj, ], efunctions)
  }
  ret.objects = c("fit", "scores", "mu", "efunctions", "evalues",
                  "npc", "sigma2") # add sigma2 to output
  ret = lapply(seq_len(length(ret.objects)), function(u) get(ret.objects[u]))
  names(ret) = ret.objects
  ret$estVar <- diag(cov.hat)
  return(ret)
}

```

### PACE (Wrapper)

```{r}
#' PACE Wrapper for funData Objects
#'
#' @description
#' A wrapper function that applies the PACE algorithm (via \code{.PACE}) to
#' \code{funData} or \code{irregFunData} objects.
#'
#' @param funDataObject Training functional data object.
#' @param predData Prediction functional data object (optional).
#' @param nbasis Number of basis functions.
#' @param pve Proportion of variance explained.
#' @param npc Number of principal components.
#' @param makePD Logical. Enforce positive definiteness.
#' @param cov.weight.type Weighting type.
#'
#' @return A list containing the FPCA results formatted as \code{funData} objects.
#' @export 
PACE <- function(funDataObject, predData = NULL, nbasis = 10, pve = 0.99, npc = NULL, makePD = FALSE, cov.weight.type = "none")
{
  # check inputs
  if(! class(funDataObject) %in% c("funData", "irregFunData"))
    stop("Parameter 'funDataObject' must be a funData or irregFunData object.")
  if(dimSupp(funDataObject) != 1)
    stop("PACE: Implemented only for funData objects with one-dimensional support.")
  if(methods::is(funDataObject, "irregFunData")) # for irregular functional data, use funData representation
    funDataObject <- as.funData(funDataObject)

  if(is.null(predData))
    Y.pred = NULL # use only funDataObject
  else
  {
    if(!isTRUE(all.equal(funDataObject@argvals, predData@argvals)))
      stop("PACE: funDataObject and predData must be defined on the same domains!")

    Y.pred = predData@X
  }


  if(!all(is.numeric(nbasis), length(nbasis) == 1, nbasis > 0))
    stop("Parameter 'nbasis' must be passed as a number > 0.")

  if(!all(is.numeric(pve), length(pve) == 1, 0 <= pve, pve <= 1))
    stop("Parameter 'pve' must be passed as a number between 0 and 1.")

  if(!is.null(npc) & !all(is.numeric(npc), length(npc) == 1, npc > 0))
    stop("Parameter 'npc' must be either NULL or passed as a number > 0.")

  if(!is.logical(makePD))
    stop("Parameter 'makePD' must be passed as a logical.")

  if(!is.character(cov.weight.type))
    stop("Parameter 'cov.weight.type' must be passed as a character.")


  res <- .PACE(X = funDataObject@argvals[[1]],
               Y = funDataObject@X,
               Y.pred = Y.pred,
               nbasis = nbasis, pve = pve, npc = npc, makePD = makePD,
               cov.weight.type = cov.weight.type)

  return(list(mu = funData(funDataObject@argvals, matrix(res$mu, nrow = 1)),
              values = res$evalues,
              functions = funData(funDataObject@argvals, t(res$efunctions)),
              scores = res$scores,
              scores.pred = res$scores.pred,
              fit = funData(funDataObject@argvals, res$fit),
              npc = res$npc,
              sigma2 = res$sigma2,
              estVar = funData(funDataObject@argvals, matrix(res$estVar, nrow = 1))
  ))
}

```

### expandBasisFunction

```{r}
#' Reconstruct Functions from Scores and Basis
#'
#' @description
#' Calculates the linear combination of basis functions based on provided scores.
#' Used to reconstruct the approximated functions.
#'
#' @param scores Matrix of scores (coefficients).
#' @param argvals Argument values (domain).
#' @param functions Basis functions.
#'
#' @return A \code{funData} object containing the reconstructed functions.
#' @export 
expandBasisFunction <- function(scores, argvals = functions@argvals, functions)
{
  if(dim(scores)[2] != nObs(functions))
    stop("expandBasisFunction: number of scores for each observation and number of eigenfunctions does not match.")

  # collapse higher-dimensional functions, multiply with scores and resize the result
  d <- dim(functions@X)
  nd <- length(d)

  if(nd == 2)
    resX <- scores %*% functions@X

  if(nd == 3)
  {
    resX <- array(NA, dim = c(dim(scores)[1], d[-1]))

    for(i in seq_len(d[2]))
      resX[,i,] <- scores %*% functions@X[,i,]
  }

  if(nd == 4)
  {
    resX <- array(NA, dim = c(dim(scores)[1], d[-1]))

    for(i in seq_len(d[2]))
      for(j in seq_len(d[3]))
        resX[,i,j,] <- scores %*% functions@X[,i,j,]
  }

  if(nd > 4) # slow solution due to aperm
  {
    resX <- aperm(plyr::aaply(.data = functions@X, .margins = 3:nd,
                              .fun = function(x,y){y %*% x}, y = scores),
                  c(nd-1,nd, seq_len((nd-2))))
    dimnames(resX) <- NULL
  }

  return( funData(argvals, resX) )
}

```

### univExpansion

```{r}
#' Calculate Univariate Basis Expansion
#'
#' @description
#' Wrapper function that calls specific basis reconstruction methods (e.g., splines,
#' FPCA) to reconstruct functions from scores.
#'
#' @param type Decomposition type (e.g., "uFPCA", "splines1D").
#' @param scores Matrix of scores.
#' @param argvals Argument values.
#' @param functions Basis functions.
#' @param params Additional parameters.
#'
#' @return A \code{funData} object.
#' @export 
univExpansion <- function(type, scores, argvals = ifelse(!is.null(functions), functions@argvals, NULL), functions, params = NULL)
{
  # Parameter checking
  if(is.null(type))
    stop("Parameter 'type' is missing.")
  else
  {
    if(!is.character(type))
      stop("Parameter 'type' must be a character string. See ?univExpansion for details.")
  }

  if(is.null(scores))
    stop("Parameter 'scores' is missing.")
  else
  {
    if(!is.matrix(scores))
      stop("Parameter 'scores' must be passed as a matrix.")
  }

  if(is.numeric(argvals))
  {
    argvals <- list(argvals)
    warning("Parameter 'argvals' was passed as a vector and transformed to a list.")
  }

  if(is.null(functions))
  {
    if(is.null(argvals))
      stop("Must pass 'argvals' if 'functions' is NULL.")
    else
    {
      if(!is.list(argvals))
        stop("Parameter 'argvals' must be passed as a list.")
    }
  }
  else
  {
    if(class(functions) != "funData")
      stop("Parameter 'functions' must be a funData object.")

    # check interaction with other parameters
    if(nObs(functions) != NCOL(scores))
      stop("Number of scores per curve does not match the number of basis functions.")

    if(!is.null(argvals) & !isTRUE(all.equal(argvals, functions@argvals)))
      stop("The parameter 'argvals' does not match the argument values of 'functions'.")
  }

  if(!is.null(params) & !is.list(params))
    stop("The parameter 'params' must be passed as a list.")

  # start calculations
  params$scores <- scores
  params$functions <- functions

  if(is.numeric(argvals))
    argvals <- list(argvals)

  params$argvals <- argvals

  res <- switch(type,
                "given" = do.call(expandBasisFunction, params),
                "uFPCA" = do.call(expandBasisFunction, params),
                "UMPCA" = do.call(expandBasisFunction, params),
                "FCP_TPA" = do.call(expandBasisFunction, params),
                "splines1D" = do.call(splineFunction1D, params),
                "splines1Dpen" = do.call(splineFunction1D, params),
                "splines2D" = do.call(splineFunction2D, params),
                "splines2Dpen" = do.call(splineFunction2Dpen, params),
                "fda" = do.call(expandBasisFunction, params),
                "DCT2D" = do.call(dctFunction2D, params),
                "DCT3D" = do.call(dctFunction3D, params),
                "default" = do.call(expandBasisFunction, params),
                stop("Univariate Expansion for 'type' = ", type, " not defined!")
  )

  return(res)
}

```

## FPCA+PCA regression

### fit.mfpca_regression

**Description:**
This function implements an end-to-end regression pipeline using Multivariate Functional Principal Component Analysis (MFPCA) for functional predictors and standard Principal Component Analysis (PCA) for scalar predictors.

The workflow proceeds as follows:

1. **Preprocessing:** Splits the data into training and testing sets and applies two-step normalization (within-modality and between-modality balancing).
2. **Dimension Reduction:**
* **Functional:** Applies MFPCA to the functional components to extract functional principal component (FPC) scores. It uses the modified `MFPCA` function to simultaneously compute scores for the test set.
* **Scalar:** Applies standard PCA to the scalar components () to extract scalar principal component (SPC) scores.


3. **Model Selection:** Iteratively fits linear regression models using an increasing number of components ().
4. **Validation:** Selects the optimal model complexity () that minimizes the Root Mean Squared Error (RMSE) on the test set.

**Usage:**

```r
fit.mfpca_regression(W, y, train_ratio = 0.7, n_pc_max = 10)

```

**Arguments:**

- `W`: A `predictor_hybrid` object containing all functional and scalar predictors.
- `y`: A numeric vector representing the response variable.
- `train_ratio`: Numeric (0-1). The proportion of data used for training (default 0.7).
- `n_pc_max`: Integer. The maximum number of principal components to evaluate.

**Value:**

- A list containing the optimal number of components, the best test RMSE, the final linear model, and the sequence of RMSE values.

```{r fit.mfpca_regression}
#' End-to-End MFPCA Regression Pipeline
#'
#' Performs sample splitting, normalization, and dimension reduction via MFPCA (functional)
#' and PCA (scalar). Selects the optimal number of components by minimizing test set RMSE.
#'
#' @param W A `predictor_hybrid` object containing all predictors.
#' @param y Vector. Response variable.
#' @param train_ratio Numeric scalar between 0 and 1 for splitting data (default 0.7).
#' @param n_pc_max Integer. Max number of FPCs/PCs to consider (must be <= min(nbasis, n_scalar)).
#'
#' @return List containing the best model fit, optimal component count, and a list of all test RMSEs.
#' @export
fit.mfpca_regression <- function(W, y, train_ratio = 0.7, n_pc_max = 10) {

  # --- 1. Input Validation and Setup ---
  if (!inherits(W, "predictor_hybrid")) {
    stop("'W' must be a predictor_hybrid object.")
  }

  eval_point <- W$eval_point
  # Check basis size from the first functional predictor (assuming same basis for all)
  n_basis <- W$n_basis_list[[1]]
  n_scalar <- W$n_scalar

  # Safety check: Ensure n_pc_max does not exceed available dimensions
  # We need at least n_pc_max basis functions and n_pc_max scalar variables
  max_comp_safe <- min(n_pc_max, n_basis, n_scalar)
  if (n_pc_max > max_comp_safe) {
    warning(paste("n_pc_max adjusted from", n_pc_max, "to", max_comp_safe,
                  "due to dimensionality constraints (nbasis or n_scalar)."))
    n_pc_max <- max_comp_safe
  }
  if (n_pc_max < 1) stop("n_pc_max must be at least 1.")

  # Define univariate expansion parameters for MFPCA
  # We construct a list of settings for each functional predictor
  uniExpansions <- lapply(seq_len(W$n_functional), function(x) {
    list(type = "uFPCA", nbasis = n_basis, npc = n_pc_max)
  })

  # --- 2. Split and Normalize Data ---
  # Uses the split_and_normalize.all function defined previously
  processed_data <- split_and_normalize.all(W, y, train_ratio)

  # Extract processed components
  W_train <- processed_data$predictor_train
  W_test  <- processed_data$predictor_test
  y_train <- processed_data$response_train
  y_test  <- processed_data$response_test

  # --- 3. Format Functional Data for MFPCA ---
  # Convert 'fd' objects to 'funData' objects required by the MFPCA package
  # We use lapply to handle any number of functional predictors
  funData_train_list <- lapply(W_train$functional_list, function(fd_obj) {
    funData::fd2funData(fd_obj, eval_point)
  })
  funData_test_list <- lapply(W_test$functional_list, function(fd_obj) {
    funData::fd2funData(fd_obj, eval_point)
  })

  # Create multiFunData objects
  mFData_train <- funData::multiFunData(funData_train_list)
  mFData_test  <- funData::multiFunData(funData_test_list)

  # --- 4. Dimension Reduction ---

  # A. Functional Part: MFPCA
  # Compute max components (n_pc_max) once, then subset later
  MFPCA_fit <- MFPCA(mFData = mFData_train,
                     mFData_predict = mFData_test,
                     M = n_pc_max,
                     uniExpansions = uniExpansions)

  # Extract scores
  scores_fun_train <- MFPCA_fit$scores
  scores_fun_test  <- MFPCA_fit$scores.pred

  # B. Scalar Part: Standard PCA
  # Using prcomp on the normalized scalar matrix Z
  PCA_fit <- prcomp(W_train$Z, center = FALSE, scale. = FALSE)

  # Extract scores (first n_pc_max components)
  # Note: predicting on test set projects Z_test onto training rotation
  scores_scalar_train <- PCA_fit$x[, 1:n_pc_max, drop = FALSE]
  scores_scalar_test  <- predict(PCA_fit, newdata = W_test$Z)[, 1:n_pc_max, drop = FALSE]

  # --- 5. Model Selection (CV Loop) ---
  linear_fit_list <- list()
  test_rmse_list  <- numeric(n_pc_max)

  for (k in 1:n_pc_max) {
    # a. Subset scores for current complexity k
    X_train <- data.frame(
      scores_fun_train[, 1:k, drop = FALSE],
      scores_scalar_train[, 1:k, drop = FALSE]
    )
    X_test <- data.frame(
      scores_fun_test[, 1:k, drop = FALSE],
      scores_scalar_test[, 1:k, drop = FALSE]
    )

    # Set consistent column names to ensure predict() works correctly
    col_names <- c(paste0("FPC", 1:k), paste0("SPC", 1:k))
    colnames(X_train) <- col_names
    colnames(X_test)  <- col_names

    # Add response for training
    train_df <- cbind(response = y_train, X_train)

    # b. Fit Linear Model
    linear_fit_list[[k]] <- lm(response ~ ., data = train_df)

    # c. Prediction & Evaluation
    y_pred <- predict(linear_fit_list[[k]], newdata = X_test)
    test_rmse_list[k] <- sqrt(mean((y_test - y_pred)^2))
  }

  # --- 6. Final Results ---
  best_idx <- which.min(test_rmse_list)

  return(list(
    n_pc_best = best_idx,
    best_test_rmse = test_rmse_list[best_idx],
    test_rmse_by_n_pc = test_rmse_list,
    final_model = linear_fit_list[[best_idx]],
    all_models = linear_fit_list,
    # Return normalization stats to allow back-transformation of predictions if needed
    normalization_details = processed_data$details
  ))
}
```
 

# PFR

Here is the `PFRModel` class code, preserving your specific structure (`eval(parse(...))`, string concatenation for formulas, and `presmooth = "fpca.sc"`) while cleaning up the descriptions and ensuring correct integration with the previously defined `split_and_normalize.all` function.

### PFRModel

**Description:**
An R6 class wrapper for `refund::pfr`. It handles data preparation, model fitting via dynamic formula construction, and error evaluation. It specifically uses the `fpca.sc` presmoothing option within the linear functional terms (`lf`).

**Usage:**

```r
pfr_handler <- PFRModel$new(W, y, n_eval, train_ratio)
pfr_handler$fit(option = "all")
errors <- pfr_handler$computeError()

```

```{r}
#' @title PFR Model Handler
#' @description R6 class to prepare data, fit, and evaluate a Penalized Functional Regression (PFR) model.
#'
#' @importFrom R6 R6Class
#' @importFrom refund pfr
#' @export
PFRModel <- R6::R6Class(
  "PFRModel",

  public = list(
    # --- Public Fields ---
    trainData = NULL,
    testData = NULL,
    model = NULL,
    n_scalar_predictor = NULL,
    n_basis = NULL,
    eval_point = NULL,
    
    # Prediction storage
    y_train_pred = NULL,
    y_test_pred = NULL,
    
    # Normalization factors for error reporting (optional usage)
    response_range_train = 1, # Default to 1 (standardized data usually implies scale ~1)
    response_range_test = 1,

    #' @title Initialize PFR Model
    #' @description Constructs the instance, splits/normalizes data, and formats it for `refund::pfr`.
    #'
    #' @param W A `predictor_hybrid` object containing all predictors.
    #' @param y Vector. Response variable.
    #' @param n_eval Integer. Number of evaluation points for functional predictors.
    #' @param train_ratio Numeric. Split ratio for training data.
    initialize = function(W, y, n_eval, train_ratio) {
      
      # 1. Run Preprocessing Pipeline (Split & Normalize)
      # Assumes `split_and_normalize.all` is available in the environment
      processed <- split_and_normalize.all(W, y, train_ratio)
      
      # 2. Set Metadata
      self$n_scalar_predictor <- W$n_scalar
      # Assume all functional predictors share the same basis size
      self$n_basis <- W$n_basis_list[1]
      self$eval_point <- W$eval_point # Use the points from the object directly
      
      # If n_eval is different from W$eval_point length, you might need interpolation.
      # Here we assume W$eval_point is sufficient, but if you strictly need a new grid:
      # eval_min <- min(W$eval_point)
      # eval_max <- max(W$eval_point)
      # self$eval_point <- seq(eval_min, eval_max, length.out = n_eval)

      # 3. Prepare Training Data
      # PFR works best with lists or data frames. We use a list to allow matrix columns easily.
      self$trainData <- list()
      
      # Add Scalar Predictors (X1, X2, ...)
      # We extract columns from Z and name them explicitly
      for(i in 1:self$n_scalar_predictor) {
        self$trainData[[paste0("X", i)]] <- processed$predictor_train$Z[, i]
      }
      
      self$trainData$response <- processed$response_train
      
      # Add Functional Predictors (F1, F2) converted to dense matrices (Sample x Time)
      # Transpose is needed because eval.fd returns (Time x Sample)
      self$trainData$F1 <- t(fda::eval.fd(self$eval_point, processed$predictor_train$functional_list[[1]]))
      self$trainData$F2 <- t(fda::eval.fd(self$eval_point, processed$predictor_train$functional_list[[2]]))
      
      # 4. Prepare Test Data
      self$testData <- list()
      for(i in 1:self$n_scalar_predictor) {
        self$testData[[paste0("X", i)]] <- processed$predictor_test$Z[, i]
      }
      self$testData$response <- processed$response_test
      self$testData$F1 <- t(fda::eval.fd(self$eval_point, processed$predictor_test$functional_list[[1]]))
      self$testData$F2 <- t(fda::eval.fd(self$eval_point, processed$predictor_test$functional_list[[2]]))
      
      # Store response ranges if needed for normalized error calculation
      # Since data is already standardized, ranges might be small.
      self$response_range_train <- diff(range(self$trainData$response))
      self$response_range_test  <- diff(range(self$testData$response))
    },

    #' @title Generate PFR Model Formula
    #' @description Constructs the formula string dynamically based on the selected option.
    #' Uses `presmooth = 'fpca.sc'` for functional terms.
    #'
    #' @param option Character. "all", "functional_only", or "scalar_only".
    #' @return A character string representing the PFR call.
    get_model_string = function(option = "all") {
      
      # Validate option
      if (!option %in% c("all", "functional_only", "scalar_only")) {
        stop("Error: 'option' must be one of 'all', 'functional_only', or 'scalar_only'.")
      }

      # 1. Build Scalar Predictor String
      scalar_part <- ""
      if (option != "functional_only") {
        # "X1 + X2 + ... + Xp"
        scalar_part <- paste0(paste0("X", 1:self$n_scalar_predictor), collapse = " + ")
        # Add leading " + " if needed later
        if (option == "all") scalar_part <- paste0(" + ", scalar_part)
      }

      # 2. Build Functional Predictor String
      functional_part <- ""
      if (option != "scalar_only") {
        # Note: self$eval_point is injected directly via string interpolation requires it to be available in scope
        # Or we rely on the data object. 'lf' usually needs 'argvals' passed explicitly if not in data.
        # Since we use eval(parse()), we reference 'self$eval_point'.
        
        term_f1 <- paste0(
          "lf(F1, argvals = self$eval_point, presmooth = 'fpca.sc', presmooth.opts = list(nbasis = ", 
          self$n_basis, "))"
        )
        term_f2 <- paste0(
          "lf(F2, argvals = self$eval_point, presmooth = 'fpca.sc', presmooth.opts = list(nbasis = ", 
          self$n_basis, "))"
        )
        functional_part <- paste(term_f1, term_f2, sep = " + ")
      }

      # 3. Combine into Full Formula
      # Result: "pfr(response ~ lf(...) + lf(...) + X1 + ..., data = self$trainData)"
      full_formula <- paste0("refund::pfr(response ~ ", functional_part, scalar_part, ", data = self$trainData)")
      
      return(full_formula)
    },

    #' @title Fit PFR Model
    #' @description Evaluates the formula string to fit the model.
    #' @param option Character. Model configuration option.
    fit = function(option = "all") {
      # Get the command string
      cmd <- self$get_model_string(option)
      
      # Execute the command in the current environment
      self$model <- eval(parse(text = cmd))
    },

    #' @title Compute Prediction Error
    #' @description Predicts on training and testing sets and computes normalized RMSE.
    #' @return A list with `train_error` and `test_error`.
    computeError = function() {
      if (is.null(self$model)) stop("Model not fitted. Run fit() first.")

      # Predict
      self$y_train_pred <- predict(self$model, newdata = self$trainData)
      self$y_test_pred  <- predict(self$model, newdata = self$testData)

      # Compute RMSE (Normalized by range, as per original code)
      # Note: Ensure ranges are non-zero to avoid division by zero
      r_train <- if(self$response_range_train > 0) self$response_range_train else 1
      r_test  <- if(self$response_range_test > 0)  self$response_range_test  else 1

      train_rmse <- sqrt(mean((self$trainData$response - self$y_train_pred)^2)) / r_train
      test_rmse  <- sqrt(mean((self$testData$response - self$y_test_pred)^2))  / r_test

      return(list(train_error = train_rmse, test_error = test_rmse))
    }
  )
)
```

 


## Documenting the package and building

We finish by running commands that will document, build, and install the package.  It may also be a good idea to check the package from within this file.

```{r}
litr::document() # <-- use instead of devtools::document()
# devtools::build()
# devtools::install()
# devtools::check(document = FALSE)
```


