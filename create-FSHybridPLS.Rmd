---
title: "Creating the ``r params$package_name`` R package"
author: "Your Name"
date: "The Date"
knit: litr::render
output: litr::litr_html_document
params:
  package_name: "FSHybridPLS" # <-- change this to your package name
  package_parent_dir: "." # <-- relative to this file's location
---

<!-- This Rmd file contains all the code needed to define an R package.  Press "Knit" in RStudio or more generally run `litr::render("name-of-this-file.Rmd")` to generate the R package.  Remember that when you want to modify anything about the R package, you should modify this document rather than the package that is outputted.
-->

## Package setup

We start by specifying the information needed in the DESCRIPTION file of the R package.
 
```{r package-setup, message=FALSE, results='hide'}
usethis::create_package(
  path = ".",
  fields = list(
    Package = params$package_name,
    Version = "0.0.0.9000",
    Title = "A Package That Says Hello",
    Description = "This package says hello.  But its actual purpose is to show how an R package can be completely coded in a single R markdown file.",
    Imports = "fda (>= 6.1.3), Matrix, funData, foreach, irlba, refund",
    `Authors@R` = person(
      given = "First",
      family = "Last",
      email = "you@gmail.com",
      role = c("aut", "cre")
      )
  )
)
usethis::use_mit_license(copyright_holder = "F. Last")
```


# helper functions

### is_same_basis
```{r}
#' @export
is_same_basis <- function(input, other) {
  all(
    length(input$functional_list) == length(other$functional_list),
    all(
      mapply(function(fd1, fd2) fda::is.eqbasis(fd1$basis, fd2$basis),
             input$functional_list, other$functional_list)
    )
  )
}
```


### compute_gram_matrix
```{r}
#' Compute the Gram matrix for a basis object
#'
#' @param basis A basis object of class `basisfd` from the `fda` package
#' @return A square matrix where entry (i,j) is the inner product of basis functions i and j
#' @export
compute_gram_matrix <- function(basis) {
  stopifnot(inherits(basis, "basisfd"))
  inprod(basis, basis)  # computes ∫ b_i(t) b_j(t) dt
}

```





```{r}
#' Check if all matrices in a list are identical (with tolerance for numeric comparison)
#'
#' This function iterates through a list of matrices and checks if all subsequent
#' matrices are numerically identical to the first matrix in the list.
#' It uses `all.equal` for robust comparison of numeric matrices, which accounts
#' for small floating-point differences.
#'
#' @param gram_list A list of matrices.
#' @param tolerance A numeric tolerance for comparing floating-point numbers.
#'                  Defaults to `sqrt(.Machine$double.eps)`.
#' @return A logical value: `TRUE` if all matrices are identical, `FALSE` otherwise.
#' @export
are_all_gram_matrices_identical <- function(gram_list, tolerance = sqrt(.Machine$double.eps)) {
  # Handle edge cases: empty list or list with a single matrix
  if (length(gram_list) == 0) {
    message("The input list is empty. Returning TRUE as there are no differences.")
    return(TRUE)
  }
  if (length(gram_list) == 1) {
    message("The input list contains only one matrix. Returning TRUE.")
    return(TRUE)
  }

  # Get the first matrix as the reference
  first_matrix <- gram_list[[1]]

  # Ensure the first element is a matrix
  if (!is.matrix(first_matrix)) {
    stop("The first element of 'gram_list' is not a matrix.")
  }

  # Iterate from the second matrix onwards and compare with the first
  for (i in 2:length(gram_list)) {
    current_matrix <- gram_list[[i]]

    # Ensure current element is a matrix
    if (!is.matrix(current_matrix)) {
      stop(paste("Element", i, "of 'gram_list' is not a matrix."))
    }

    # Compare the current matrix with the first matrix
    # all.equal returns TRUE if identical, or a character string describing differences
    # We convert the result to a logical TRUE/FALSE
    if (!isTRUE(all.equal(first_matrix, current_matrix, tolerance = tolerance))) {
      # If not equal, return FALSE immediately
      message(paste("Matrices at index 1 and", i, "are not identical."))
      return(FALSE)
    }
  }

  # If the loop completes, all matrices are identical
  return(TRUE)
}
```


 

## helpers for functional data objects

### rep_fd

```{r}
#' Replicate a list of single-sample fd objects multiple times
#'
#' This function broadcasts a list of single-sample functional predictors
#' by replicating their coefficient columns.
#'
#' @param fd_list A list of `fd` objects, each representing a single sample.
#' @param n The number of replications desired.
#'
#' @return A list of `fd` objects, each with `n` replications.
#' @export
rep_fd <- function(fd_list, n) {
  if (!is.list(fd_list) || any(!vapply(fd_list, inherits, logical(1), "fd"))) {
    stop("Input must be a list of 'fd' objects.")
  }

  lapply(fd_list, function(fd_obj) {
    coef_mat <- fd_obj$coefs
    if (is.null(dim(coef_mat)) || ncol(coef_mat) != 1) {
      stop("Each fd object in the list must have one column of coefficients.")
    }

    new_coefs <- matrix(rep(coef_mat, n), nrow = nrow(coef_mat), ncol = n)
    fd(coef = new_coefs, basisobj = fd_obj$basis)
  })
}
```

| Case | Description                            | Input Type               | Expected Output                        | Notes                                            |
|------|----------------------------------------|--------------------------|----------------------------------------|--------------------------------------------------|
| 1    | Correct replication                    | List of single-sample fd | List of `fd` objects with `n` columns | All columns identical to original coefficients   |
| 2    | Each fd has one sample                 | List of fd               | No error                               | Verifies structure and content                  |
| 3    | One fd has multiple samples            | fd with >1 columns       | Error                                   | Should raise informative error                  |
| 4    | Non-fd object in list                  | List of non-`fd` items   | Error                                   | Validates class-checking robustness             |


 

# Class predictor_hybrid
- Let $\{X^{(k)}\}_{k=1, \ldots, K}$ be a collection of random functions defined on unit hypercube domains $\mathcal{T}_k := [0,1]^{d_k}$ ($d_k \in \mathbb{N})$; i.e., $X^{(k)}: \mathcal{T}_k \rightarrow \mathbb{R}$.

- Assume that each $X^{(k)}$ is in $L^2(\mathcal{T}_k)$, a Hilbert space of square integrable functions with respect to Lebesgue measure $dt_k$ on $\mathcal{T}_k$. 

- Write  $X=(X^{(1)}, \ldots, X^{(K)})$ as a multivariate functional object that belongs to $\mathcal{F} = L^2(\mathcal{T}_1) \times \cdots \times L^2(\mathcal{T}_K)$---a cartesian product of individual $L^2(\mathcal{T}_k)$ spaces. 

- We can also express the functional object $X$ evaluated on the multi-dimensional argument $\mathbf{t} = (t_1, \ldots, t_k)^\top \in \mathcal{T} = \mathcal{T}_1 \times \cdots \times \mathcal{T}_K$ as a $K$-dimensional vector $X(\mathbf{t})=(X^{(1)}(t_1), \ldots, X^{(K)}(t_K))^\top$. 

- **Our strategy is to formulate a hybrid random object**, $\mathbf{W} = (X, \mathbf{Z})$, which combines $X$ and $p$-dimensional scalar covariate $\mathbf{Z}$ into an ordered pair belonging to $\mathcal{H} = \mathcal{F} \times \mathbb{R}^p$.  

- An alternative notation for the hybrid object can be obtained by evaluating its functional part on $\mathbf{t}$ and expressing it as a $(K+p)$-dimensional vector: $\mathbf{W}[\mathbf{t}] = (X(\mathbf{t}), \mathbf{Z})^\top$, with $X(\mathbf{t}) = (X^{(1)}(t_1), \ldots, X^{(K)}(t_K))^\top \in \mathbb{R}^K$ and $\mathbf{Z} = (Z_1, \ldots, Z_p)^\top  \in \mathbb{R}^p$.

- The hybrid object forms a Hilbert space, characterized by well-defined addition, scalar multiplication, and inner product operations. 

- We define an S3 object `predictor_hybrid` that represent this structure, and implement Hilbert space operations for this class, and our algorithm is built on these foundational operations.

The class definition is a simple list that contains a matrix and a collection of \code{fd} objects from the \code{fda} package, along with the necessary metadata.
- gram matrices are saved as a list.
```{r classdef:predictor_hybrid}

#' Create a predictor_hybrid object (S3 version, automatic basis size and Gram matrix)
#'
#' Constructs a hybrid predictor object that stores both scalar and functional predictors,
#' and automatically computes Gram matrices (inner products of basis functions).
#'
#' @param Z A numeric matrix of dimension \code{n_sample × n_scalar} representing scalar predictors.
#' @param functional_list A list of functional predictors (e.g., \code{fd} objects from the \code{fda} package).
#'
#' @return An object of class \code{predictor_hybrid}, containing scalar and functional data with Gram matrices.
#' @export
predictor_hybrid <- function(Z, functional_list) {
  stopifnot(is.matrix(Z), is.numeric(Z))
  stopifnot(is.list(functional_list))

  n_sample <- nrow(Z)
  n_scalar <- ncol(Z)
  n_functional <- length(functional_list)

  gram_list <- gram_deriv2_list <- vector("list", n_functional)
  n_basis_list <- numeric(n_functional)

  for (i in seq_len(n_functional)) {
    fd_i <- functional_list[[i]]
    stopifnot(inherits(fd_i, "fd"))

    n_fd_sample <- ncol(coef(fd_i))
    if (n_fd_sample != n_sample) {
      stop(sprintf("Functional predictor %d has %d replicates, but Z has %d rows.",
                   i, n_fd_sample, n_sample))
    }

    basis_i <- fd_i$basis
    gram_list[[i]] <- compute_gram_matrix(basis_i)
    gram_deriv2_list[[i]] <- fda::getbasispenalty(basis_i) # New  
      
 

    n_basis_list[i] <- basis_i$nbasis
  }

  structure(
    list(
      Z = Z,
      functional_list = functional_list,
      gram_list = gram_list,
      gram_deriv2_list = gram_deriv2_list, # Store the new list
      n_basis_list = n_basis_list,
      n_sample = n_sample,
      n_functional = n_functional,
      n_scalar = n_scalar
    ),
    class = "predictor_hybrid"
  )
}
```

 


### predictor_hybrid_from_coef
An alternative constructor for a predictor_hybrid object based on a flat numeric coefficient vector. It reconstructs both the functional and scalar components for a single-sample hybrid predictor object. The function reads the basis information from the template `format` object.

- Inputs
  - **`format`**: A `predictor_hybrid` object containing:
    - `functional_list`: List of `fd` objects with basis info
    - `gram_list`: List of Gram matrices
    - `n_scalar`: Number of scalar predictors
  - **`coef`**: Numeric vector of length `K * M + d`, where:
    - `K` = number of functional predictors  
    - `M` = number of basis functions (assumed same across predictors)  
    - `d` = number of scalar predictors

- Output
  - A `predictor_hybrid` object with:
    - Functional predictors rebuilt from the first `K * M` coefficients
    - Scalar predictor `Z` from the remaining `d` coefficients
    - `n_sample` set to 1

```{r}
#' Construct a Single-Sample Predictor Hybrid Object from Coefficients
#'
#' Reconstructs a \code{predictor_hybrid} object representing one observation, using a numeric
#' coefficient vector. This alternative constructor maps the coefficients back into their functional
#' and scalar predictor representations based on the structure of a template \code{predictor_hybrid} object.
#'
#' @param format A \code{predictor_hybrid} object that provides the structure and basis information.
#' @param coef A numeric vector containing coefficients for both functional and scalar predictors.
#'
#' @return A \code{predictor_hybrid} object with updated \code{functional_list}, \code{Z}, and \code{n_sample = 1}.
#'
#'
#' @export
predictor_hybrid_from_coef <- function(format, coef){
  M <- format$n_basis_list[1] # number of basis functions
  K <- format$n_functional # number of functional predictors
  for (ii in 1:K){
    format$functional_list[[ii]] <- fd(
      coef = as.matrix(coef[((ii-1)*M + 1):(ii*M)]),
      basisobj = format$functional_list[[ii]]$basis
    )
  }
  format$Z <- t(as.matrix(coef[(K * M + 1):length(coef)]))
  format$n_sample <- 1
  return(format)
}
```
 
## Basic arithmetic

### add.predictor_hybrid
Performs element-wise addition of two `predictor_hybrid` objects.  Functional predictors are combined using `plus.fd()` and `times.fd()` from the `fda` package.

- Usage

```r
add.predictor_hybrid(input, other, alpha = 1)
```

- Arguments:

  - `input`: A `predictor_hybrid` object.
  - `other`: Another `predictor_hybrid` object to be added.
  - `alpha`: A scalar multiplier applied to `other` before addition (default is 1).

- Value:
Returns a new `predictor_hybrid` object representing the result of the addition.

- Details: 
  - This function assumes both objects have the same number and structure of functional and scalar predictors.  
  - Functional parts are scaled by `alpha` using `times.fd()` and then summed using `plus.fd()`.  
  - Scalar predictors are added using standard matrix addition.

```{r}
add.predictor_hybrid <- function(xi_1, xi_2, alpha = 1) {
  # Safe access to is.eqbasis()
  is_eqbasis <- getFromNamespace("is.eqbasis", "fda")

  for (i in seq_len(xi_1$n_functional)) {
    if (!is_eqbasis(xi_1$functional_list[[i]]$basis, xi_2$functional_list[[i]]$basis)) {
      stop("Functional predictors must have the same basis.")
    }
  }
  
   # Type checks
  if (!inherits(xi_1, "predictor_hybrid") || !inherits(xi_2, "predictor_hybrid")) {
    stop("Both inputs must be of class 'predictor_hybrid'.")
  }

  # Structural checks
  if (xi_1$n_functional != xi_2$n_functional) {
    stop("Mismatch in number of functional predictors.")
  }
  if (xi_1$n_scalar != xi_2$n_scalar) {
    stop("Mismatch in number of scalar predictors.")
  }

  # Swap so that broadcasting always applies to xi_2
  if (xi_1$n_sample == 1 && xi_2$n_sample > 1) {
    tmp <- xi_1
    xi_1 <- xi_2
    xi_2 <- tmp
  }

  n1 <- xi_1$n_sample
  n2 <- xi_2$n_sample

  if (!(n1 == n2 || n2 == 1)) {
    stop("Sample sizes are incompatible for broadcasting.")
  }

  # Prepare components
  f1 <- xi_1$functional_list
  f2 <- xi_2$functional_list
  Z1 <- xi_1$Z
  Z2 <- xi_2$Z

  # Replicate fd and Z if needed
  if (n2 == 1) {
    f2 <- rep_fd(f2, n1)
    Z2 <- matrix(rep(c(Z2), n1), nrow = n1, byrow = TRUE)
  }

  # Combine functional predictors
  new_functional_list <- Map(
    function(f1, f2) plus.fd(f1, times.fd(alpha, f2)),
    f1,
    f2
  )
 

  # Compute scalar inner products
  inprod_scalar <- rowSums(Z1 * Z2) 
  # Combine scalar predictors
  new_Z <- Z1 + alpha * Z2


  # Construct new object
  predictor_hybrid(Z = new_Z, functional_list = new_functional_list)
}
```

 
### subtr.predictor_hybrid
Performs element-wise subtraction of two `predictor_hybrid` objects.   Internally uses `add.predictor_hybrid(input, other, alpha = -1)` to compute the result by negating the second operand.

- Usage

```r
subtr.predictor_hybrid(input, other, alpha = 1)
```

- Arguments:

  - `input`: A `predictor_hybrid` object.
  - `other`: Another `predictor_hybrid` object to be subtracted.
  - `alpha`: A scalar multiplier applied to `other` before subtraction (default is 1).

- Value:  
Returns a new `predictor_hybrid` object representing the result of the subtraction.

- Details:  
  - This function assumes both objects have the same number and structure of functional and scalar predictors.  
  - It performs subtraction by internally calling `add.predictor_hybrid()` with `-alpha`.  
  - Functional parts are scaled using `times.fd()` and subtracted via `plus.fd()` with a negated factor.  
  - Scalar predictors are subtracted using standard matrix arithmetic.

```{r}
#' Subtract two predictor_hybrid objects
#'
#' Performs element-wise subtraction of two `predictor_hybrid` objects.
#' Internally uses `add.predictor_hybrid(input, other, alpha = -1)`.
#'
#' @param input A `predictor_hybrid` object.
#' @param other Another `predictor_hybrid` object to subtract.
#' @param alpha A scalar multiplier applied to `other` before subtraction (default is 1).
#'
#' @return A new `predictor_hybrid` object representing the result of subtraction.
#' @export
subtr.predictor_hybrid <- function(input, other, alpha = 1) {
  add.predictor_hybrid(input, other, alpha = -alpha)
}

```
We skip the unit test for this function.


### scalar_mul.predictor_hybrid
Multiplies all components of a `predictor_hybrid` object by a scalar. Functional components are scaled using `times.fd()` from the `fda` package, and scalar predictors are multiplied directly using matrix operations.

- Usage

```r
scalar_mul.predictor_hybrid(input, scalar)
```

- Arguments:

  - `input`: A `predictor_hybrid` object.
  - `scalar`: A numeric value used to scale both scalar and functional components.

- Value:  
Returns a new `predictor_hybrid` object with all components scaled by `scalar`.

- Details:  
  - Functional predictors are scaled using `times.fd(scalar, fd_obj)` for each element.  
  - Scalar predictors (the matrix `Z`) are scaled elementwise using matrix multiplication.

```{r}
#' Multiply a predictor_hybrid object by a scalar
#'
#' Performs scalar multiplication on both the scalar and functional components
#' of a `predictor_hybrid` object. Functional predictors are scaled using 
#' `times.fd()` from the `fda` package.
#'
#' @param input A `predictor_hybrid` object.
#' @param scalar A numeric value to multiply all components by.
#'
#' @return A new `predictor_hybrid` object scaled by `scalar`.
#' @export
scalar_mul.predictor_hybrid <- function(input, scalar) {
  if (!inherits(input, "predictor_hybrid")) {
    stop("Input must be of class 'predictor_hybrid'.")
  }
  if (!is.numeric(scalar) || length(scalar) != 1) {
    stop("Scalar must be a single numeric value.")
  }

  # Scale functional components
  new_functional_list <- lapply(input$functional_list, function(fd_obj) {
    times.fd(scalar, fd_obj)
  })

  # Scale scalar predictors
  new_Z <- scalar * input$Z

  # Reconstruct hybrid object
  predictor_hybrid(
    Z = new_Z,
    functional_list = new_functional_list
  )
}
```
 
### inprod.predictor_hybrid
- The inner product of $f_1=(f_1^{(1)}, \ldots,f_1^{(K)})$ and $f_2=(f_2^{(1)}, \ldots,f_2^{(K)})$ in $\mathcal{F}$ is defined as 
$$\langle f_1, f_2\rangle_\mathcal{F} =  \sum_{k=1}^K \langle f_1^{(k)}, f_2^{(k)}\rangle_{L^2} = \sum_{k=1}^K \int_{\mathcal{T}_k} f_1^{(k)}(t_k) f_2^{(k)}(t_k) dt_k,$$ with norm 
$$\Vert f_1 \Vert_\mathcal{F} = \langle f_1,f_1 \rangle_\mathcal{F}^{1/2} = \{ \sum_{k=1}^K \int_{\mathcal{T}_k} f_1^{(k)}(t_k)^2 dt_k\}^{1/2}.$$
- We define the inner product between any two hybrid objects, $\mathbf{h}_1 = (f_1, \mathbf{v}_1)$ and $\mathbf{h}_2 = (f_2, \mathbf{v}_2)$, as
$$
    \langle \mathbf{h}_1, \mathbf{h}_2\rangle_{\mathcal{H}} = \langle f_1, f_2\rangle_\mathcal{F} +  \langle  \mathbf{v_1}, \mathbf{v_2} \rangle = \sum \limits_{k=1}^K \int_{\mathcal{T}_k} f_1^{(k)}(t_k) f_2^{(k)}(t_k) dt_k + \sum \limits_{r=1}^p v_{1r}v_{2r}, 
$$
with norm $\Vert \cdot \Vert_\mathcal{H} = \langle \cdot,\cdot \rangle_\mathcal{H}^{1/2}$. 

- The method `inprod.predictor_hybrid` computes the inner product between two `predictor_hybrid` objects. It supports broadcasting when one of the inputs has a single observation.


- Usage

```r
inprod.predictor_hybrid(xi_1, xi_2)
```

- Arguments:

  - `xi_1`: A `predictor_hybrid` object.
  - `xi_2`: Another `predictor_hybrid` object. If omitted, defaults to xi_1 (computes self-inner product).

- Value:  A numeric vector of inner products (or a scalar if both inputs are single observations).

- Details:  
  - Functional components are summed using inprod() from the fda package.
  - Scalar components are handled via matrix multiplication.
  - Broadcasting is supported: if either xi_1 or xi_2 has only one sample, its values are broadcast across all rows of the other.
  - Ensures compatibility in the number of functional and scalar predictors before computing the result.
```{r}
#' Inner product between two predictor_hybrid objects (with broadcasting)
#'
#' Computes the inner product between two `predictor_hybrid` objects,
#' including both functional and scalar components. Supports broadcasting
#' when one of the inputs has a single observation.
#'
#' @param xi_1 A `predictor_hybrid` object.
#' @param xi_2 Another `predictor_hybrid` object. If missing, defaults to `xi_1`.
#'
#' @return A numeric vector of inner products, or a scalar if both inputs contain a single observation.
#' @export
inprod.predictor_hybrid <- function(xi_1, xi_2 = NULL) {
  # Handle self-inner product
  if (is.null(xi_2)) xi_2 <- xi_1 
  
  # Type checks
  if (!inherits(xi_1, "predictor_hybrid") || !inherits(xi_2, "predictor_hybrid")) {
    stop("Both inputs must be of class 'predictor_hybrid'.")
  }

  # Structural checks
  if (xi_1$n_functional != xi_2$n_functional) {
    stop("Mismatch in number of functional predictors.")
  }
  if (xi_1$n_scalar != xi_2$n_scalar) {
    stop("Mismatch in number of scalar predictors.")
  }

  # Swap so that broadcasting always applies to xi_2
  if (xi_1$n_sample == 1 && xi_2$n_sample > 1) {
    tmp <- xi_1
    xi_1 <- xi_2
    xi_2 <- tmp
  }

  n1 <- xi_1$n_sample
  n2 <- xi_2$n_sample

  if (!(n1 == n2 || n2 == 1)) {
    stop("Sample sizes are incompatible for broadcasting.")
  }

  # Prepare components
  f1 <- xi_1$functional_list
  f2 <- xi_2$functional_list
  Z1 <- xi_1$Z
  Z2 <- xi_2$Z

  # Replicate fd and Z if needed
  if (n2 == 1) {
    f2 <- rep_fd(f2, n1)
    Z2 <- matrix(rep(c(Z2), n1), nrow = n1, byrow = TRUE)
  }

  # Compute functional inner products
  inprod_functional <- vapply(seq_len(n1), function(i) {
    sum(vapply(seq_along(f1), function(j) {
      fda::inprod(f1[[j]][i], f2[[j]][i])
    }, numeric(1)))
  }, numeric(1))
 

  # Compute scalar inner products
  inprod_scalar <- rowSums(Z1 * Z2) 
 
  # Combine results
  result <- inprod_functional + inprod_scalar
  if (n1 == 1 && n2 == 1) as.numeric(result) else result
}
```

```{r}
#' Inner product between two predictor_hybrid objects (with broadcasting)
#'
#' Computes the inner product between two `predictor_hybrid` objects,
#' including both functional and scalar components. Supports broadcasting
#' when one of the inputs has a single observation.
#'
#' @param xi_1 A `predictor_hybrid` object.
#' @param xi_2 Another `predictor_hybrid` object. If missing, defaults to `xi_1`.
#'
#' @return A numeric vector of inner products, or a scalar if both inputs contain a single observation.
#' @export
inprod_pen.predictor_hybrid <- function(xi_1, xi_2 = NULL, lambda) {

  inprod <- inprod.predictor_hybrid(xi_1, xi_2)
  # Prepare components
  
  f1 <- xi_1$functional_list

  f2 <- xi_2$functional_list
  for (j in 1:xi_1$n_functional){
    inprod <- inprod + lambda[j] * fda::inprod(
        fdobj1 = f1[[j]], 
        fdobj2 = f2[[j]], 
        Lfdobj1 = 2,
        Lfdobj2 = 2
        )
   
    }
  

 

  # Combine results
   return(as.numeric(inprod))
}
```

### subset_predictor_hybrid

```{r}
#' Extract a single observation from a predictor_hybrid object
#'
#' @param W A predictor_hybrid object with multiple samples.
#' @param i Integer index of the sample to extract.
#' @return A single-sample predictor_hybrid object.
subset_predictor_hybrid <- function(W, i) {
  new_Z <- matrix(W$Z[i, ], nrow = 1)
  new_functional_list <- lapply(W$functional_list, function(fdobj) {
    fd(coef = matrix(coef(fdobj)[, i], ncol = 1), basisobj = fdobj$basis)
  })
  new_predictor <- predictor_hybrid(Z = new_Z, functional_list = new_functional_list)
  return(new_predictor)
}
```

### replace_obs_hybrid

```{r}
#' Replace a single observation in a predictor_hybrid object
#'
#' Replaces the i-th observation of a predictor_hybrid object with a new
#' single-sample predictor_hybrid object.
#'
#' @param W A predictor_hybrid object with one or more samples.
#' @param i An integer index specifying the observation to replace.
#' @param new_W A single-sample predictor_hybrid object to use for replacement.
#'
#' @return A predictor_hybrid object with the i-th observation replaced.
#' @export
replace_obs_hybrid <- function(W, i, new_W) {
  # Input validation
  if (!inherits(W, "predictor_hybrid") || !inherits(new_W, "predictor_hybrid")) {
    stop("Both W and new_W must be of class 'predictor_hybrid'.")
  }
  if (new_W$n_sample != 1) {
    stop("new_W must be a single-sample predictor_hybrid object.")
  }
  if (i < 1 || i > W$n_sample) {
    stop(paste("Index i must be between 1 and", W$n_sample))
  }
  if (W$n_scalar != new_W$n_scalar) {
    stop("Mismatch in number of scalar predictors.")
  }
  if (W$n_functional != new_W$n_functional) {
    stop("Mismatch in number of functional predictors.")
  }
  
  # Check for compatible basis objects
  is_eqbasis <- getFromNamespace("is.eqbasis", "fda")
  for (j in seq_len(W$n_functional)) {
    if (!is_eqbasis(W$functional_list[[j]]$basis, new_W$functional_list[[j]]$basis)) {
      stop("Functional predictors must have the same basis objects.")
    }
  }

  # Replace the i-th observation in the scalar matrix Z
  W$Z[i, ] <- new_W$Z[1, ]

  # Replace the i-th observation in each functional predictor
  for (j in seq_along(W$functional_list)) {
    # The coefficients are stored as a matrix, with columns corresponding to samples
    W$functional_list[[j]]$coefs[, i] <- new_W$functional_list[[j]]$coefs[, 1]
  }

  # The number of samples remains the same
  return(W)
}
```
 



# One iteration

## small functions



## penalty matrix construction
In this section, we provide an R function `get_constraint_matrix` that constructs the penalty matrix
$$J^*+\Lambda \ddot{J}^\ast,$$
as defined in Proposition 2 and used in the PLS component computation step of our proposed algorithm. We present functions  for computing $J^\ast$, $\Lambda$ and $\ddot{J}^\ast$ in sequence, and use these functions to define `get_constraint_matrix`.

### get_gram_matrix_block
Constructs a block-diagonal Gram matrix for a hybrid predictor object, defined as 
$$
    J^*=\mathrm{blkdiag}(J, I_p) \in \mathbb{R}^{(MK+p) \times (MK+p)}
$$
where
 $J = \mathrm{blkdiag}(J^{(1)}, \cdots, J^{(K)}) \in \mathbb{R}^{MK \times MK}$, and 
$$
J^{(k)} = \left[ \int_{\mathcal{T}k} b_m^{(k)}(t) , b_n^{(k)}(t) , dt \right]_{m,n=1}^M,
$$
as defined in \eqref{def:gram_basis} and \eqref{def:J_and_J_star}.

- Arguments
  - obj: A `predictor_hybrid` object containing both functional and scalar components.

- Value
  - A Matrix::bdiag sparse matrix representing the block-diagonal structure of the combined Gram matrix.

- Details: This function builds a block-diagonal matrix by:
  - Stacking the Gram matrices of each functional component (from obj$gram_list),
  - Appending an identity matrix of size equal to the number of scalar predictors (to represent unpenalized scalar covariates).
  The resulting matrix has size $(\texttt{total_dim} \times \texttt{total_dim})$, where total_dim = \sum_k M_k + p, with $M_k$ the number of basis functions for the $k$-th functional predictor and $p$ the number of scalar covariates.
  
- Usage
```r
block_gram <- get_gram_matrix_block(my_predictor)
```


**Code**
```{r}
#' Construct block-diagonal Gram matrix for hybrid predictor
#'
#' Returns a block-diagonal matrix containing the Gram  matrices for
#' each functional component and an identity matrix for the scalar part.
#'
#' @param obj A `predictor_hybrid` object.
#'
#' @return A block-diagonal matrix of size `(total_dim × total_dim)` where
#' functional and scalar components are arranged in order.
#' @export
get_gram_matrix_block <- function(obj) {
  if (!inherits(obj, "predictor_hybrid")) {
    stop("Input must be of class 'predictor_hybrid'.")
  }

  gram_blocks <- c(obj$gram_list, list(diag(obj$n_scalar)))
  Matrix::bdiag(gram_blocks)
}

```

**Unit test**
```{r}
testthat::test_that("get_gram_matrix_block constructs correct block-diagonal matrix", {
  suppressPackageStartupMessages(library(fda))
  suppressPackageStartupMessages(library(Matrix))

  # Construct hybrid predictor with two functional and three scalar predictors
  basis1 <- create.bspline.basis(c(0, 1), nbasis = 4)
  basis2 <- create.bspline.basis(c(0, 1), nbasis = 4)  # was 3; now valid

  fd1 <- fd(coef = matrix(1, 4, 2), basisobj = basis1)
  fd2 <- fd(coef = matrix(2, 4, 2), basisobj = basis2)
  Z <- matrix(rnorm(2 * 3), nrow = 2, ncol = 3)

  obj <- predictor_hybrid(Z = Z, functional_list = list(fd1, fd2))

  # Extract the block-diagonal Gram matrix
  G <- get_gram_matrix_block(obj)

  # Expected block sizes
  nb1 <- 4
  nb2 <- 4
  ns <- 3
  total_dim <- nb1 + nb2 + ns

  # Check matrix properties
  testthat::expect_equal(dim(G), c(total_dim, total_dim))

  # Check sub-block structure
  # Top-left: Gram matrix 1
  testthat::expect_equal(sum(G[1:nb1, 1:nb1]-obj$gram_list[[1]]), 0)

  # Next block: Gram matrix 2
  testthat::expect_equal(sum( G[(nb1 + 1):(nb1 + nb2), (nb1 + 1):(nb1 + nb2)]-obj$gram_list[[2]]),0 )

  # Final block: identity matrix for scalar part
  testthat::expect_equal( sum( G[(nb1 + nb2 + 1):total_dim, (nb1 + nb2 + 1):total_dim]  - diag(ns)), 0)
})

```
### get_smoothing_param_hybrid
Constructs a block-diagonal smoothing parameter matrix for use in penalized estimation involving hybrid predictors, denoted $\Lambda \in \mathbb{R}^{(MK+p) \times (MK+p)}$,  defined in \eqref{def:Lambda} as
$$
   \Lambda = \mathrm{blkdiag}(\lambda_1 I_M, \cdots, \lambda_K I_M, 0_{p \times p}) , 
$$

- Arguments
  - W: A predictor_hybrid object.
  - lambda: A numeric vector of length equal to the number of functional predictors. Each entry corresponds to a smoothing penalty weight.
- Value: A sparse block-diagonal matrix combining scaled identity matrices for the functional parts and a zero matrix for the scalar part.
- Details: This function generates a matrix used in regularized regression for functional predictors. Each functional component is penalized using a scaled identity matrix of its basis dimension. Scalar predictors are unpenalized.
- Usage:
```r
lambda_mat <- get_smoothing_param_hybrid(my_predictor, c(0.1, 0.2))
```
**code** 
```{r}
#' Construct block-diagonal smoothing parameter matrix
#'
#' Generates a block-diagonal matrix with smoothing parameters applied to each 
#' functional component. Each block is a scaled identity matrix, where the scaling 
#' factor corresponds to the regularization parameter for that functional component. 
#' The scalar components are not penalized and thus contribute a zero matrix block.
#'
#' @param W A `predictor_hybrid` object.
#' @param lambda A numeric vector of length equal to the number of functional components (`W$n_functional`), containing the smoothing parameters for each functional predictor.
#'
#' @return A block-diagonal matrix of size `(total_dim × total_dim)`, where the top-left blocks are scaled identity matrices for functional predictors and the bottom-right block is a zero matrix for scalar covariates.
#' @export
get_smoothing_param_hybrid <- function(W, lambda) {
  if (!inherits(W, "predictor_hybrid")) {
    stop("Input W must be of class 'predictor_hybrid'.")
  }

  if (length(lambda) != W$n_functional) {
    stop("Length of lambda must match the number of functional predictors.")
  }

  lambda_blocks <- lapply(seq_len(W$n_functional), function(ii) {
    nb <- W$functional_list[[ii]]$basis$nbasis
    lambda[ii] * diag(nb)
  })

  lambda_blocks[[W$n_functional + 1]] <- matrix(0, nrow = W$n_scalar, ncol = W$n_scalar)

  Matrix::bdiag(lambda_blocks)
}
```

**unit test**
```{r}
testthat::test_that("get_smoothing_param_hybrid returns correct block-diagonal structure", {
  suppressPackageStartupMessages(library(fda))
  basis1 <- create.bspline.basis(c(0, 1), nbasis = 5)
  basis2 <- create.bspline.basis(c(0, 1), nbasis = 4)

  fd1 <- fd(coef = matrix(1, 5, 3), basisobj = basis1)
  fd2 <- fd(coef = matrix(2, 4, 3), basisobj = basis2)

  Z <- matrix(1, nrow = 3, ncol = 2)

  obj <- predictor_hybrid(Z = Z, functional_list = list(fd1, fd2))
  lambda <- c(0.5, 2)

  lambda_mat <- get_smoothing_param_hybrid(obj, lambda)

  # Check overall dimension
  expected_dim <- sum(obj$n_basis_list) + obj$n_scalar
  testthat::expect_equal(dim(lambda_mat), c(expected_dim, expected_dim))

  # Check diagonal entries
  testthat::expect_equal(Matrix::diag(lambda_mat)[1:5], rep(0.5, 5))
  testthat::expect_equal(Matrix::diag(lambda_mat)[6:9], rep(2, 4))
  testthat::expect_equal(Matrix::diag(lambda_mat)[10:11], rep(0, 2))  # scalar part
})

```


### get_penalty_hybrid
Constructs a matrix defined in \label{def:J_dotdot_ast}
$$
    \ddot{J}^\ast=\mathrm{blkdiag}(\ddot{J}^{(1)}, \cdots, \ddot{J}^{(K)}, 0_{p \times p}) \in \mathbb{R}^{(MK+p) \times (MK+p)},
$$
where
 $\ddot{J}^{(k)}$ is  a $M \times M$ matrix defined in \label{def:dodot_J_k} as the gram matrix formed by the second derivativ of the basis functions
$$
\ddot{J}^{(k)} = \left[ \int_{\mathcal{T}_k} \ddot{b}^{(k)}_m(t) \ddot{b}^{(k)}_n(t) \, dt \right]_{m,n=1}^M.
$$



```{r}
testthat::test_that("get_penalty_hybrid computes correct dimensions and structure", {
  suppressPackageStartupMessages(library(fda))

  # Setup
  basis <- fda::create.bspline.basis(c(0, 1), nbasis = 5)
  n_sample <- 3
  n_scalar <- 2

  fd1 <- fda::fd(coef = matrix(1, 5, n_sample), basisobj = basis)
  fd2 <- fda::fd(coef = matrix(2, 5, n_sample), basisobj = basis)
  Z <- matrix(rnorm(n_sample * n_scalar), n_sample, n_scalar)
  obj <- predictor_hybrid(Z = Z, functional_list = list(fd1, fd2))

  # Run
  penalty <- get_penalty_hybrid(obj)

  # Expected dimensions
  total_dim <- sum(obj$n_basis_list) + obj$n_scalar
  testthat::expect_equal(dim(penalty), c(total_dim, total_dim))

  # Check block diagonal structure: bottom-right should be zero matrix
  scalar_block <- as.matrix(penalty[
    (total_dim - n_scalar + 1):total_dim,
    (total_dim - n_scalar + 1):total_dim
  ])
  testthat::expect_equal(scalar_block, matrix(0, n_scalar, n_scalar))

  # Check that the upper blocks are positive semidefinite
  eigenvalues <- eigen(as.matrix(penalty[1:(total_dim - n_scalar), 1:(total_dim - n_scalar)]))$values
  testthat::expect_true(all(eigenvalues >= -1e-8))  # Numerical tolerance
})

```

### get_constraint_matrix

```r
W <- predictor_hybrid(Z = Z, functional_list = list(fd1, fd2))
lambda <- c(0.1, 0.2)
constraint_matrix <- get_constraint_matrix_hybrid(W, lambda)
```

**code**
```{r}
#' Construct penalized constraint matrix for hybrid predictors
#'
#' Computes the denominator matrix used in penalized estimation for hybrid predictors,
#' combining the Gram matrix, smoothing parameter matrix, and penalty matrix.
#'
#' Specifically, this function returns the matrix:
#' \deqn{J^\ast + \Lambda \ddot{J}^\ast}
#' where \eqn{J^\ast} is the block-diagonal Gram matrix,
#' \eqn{\Lambda} is the block-diagonal smoothing parameter matrix,
#' and \eqn{\ddot{J}^\ast} is the block-diagonal penalty matrix of second derivative inner products.
#'
#' @param W A `predictor_hybrid` object.
#' @param lambda A numeric vector of smoothing parameters, one for each functional predictor.
#'
#' @return A matrix representing the penalized constraint matrix used in estimation.
#' @export
get_constraint_matrix_hybrid <- function(W, lambda) {
  J_star <- get_gram_matrix_block(W)
  J_dotdot_star <- get_penalty_hybrid(W)
  lambda_mat <- get_smoothing_param_hybrid(W, lambda)

  J_star + lambda_mat %*% J_dotdot_star
}

```

## PLS component computation

### Spectral method
The old implementation that computes the leading eigenvector of 

### helper: get_CJ
```{r}
get_CJ <- function(W){
  n <- W$n_sample
  K <- W$n_functional
  ## C_J : C multiplied by J. Scalable to arbitrary K
  predictor_first <- W$functional_list[[1]]
  C_J <- t(predictor_first$coefs) %*% W$gram_list[[1]]
  for (i in 2:K){
    predictor_now <- W$functional_list[[i]]
    C_J <- cbind(
      C_J,
      t(predictor_now$coefs) %*% W$gram_list[[i]]
    )
    }
    return(C_J)
}
```



### helper: get_pre_corrcov 
construct the matrix V_star:
$$
    V^\ast = n^{-2}\begin{bmatrix}
         J \widetilde{C}^\top \mathbf{y}\mathbf{y}^\top \widetilde{C} J & J \widetilde{C}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{Z} \\
        \widetilde{Z}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C} J &   \widetilde{Z}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{Z}
    \end{bmatrix} = n^{-2} J^* \widetilde{C}^{*\top} \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C}^{*} J^*
    \in \mathbb{R}^{(MK+p) \times (MK+p)},
$$
where $J$ is defined in `get_gram_matrix_block`
equation (7), 
```{r}
#' Compute V matrix (pre-corrected covariance estimator)
#'
#' Constructs the V matrix involving inner products of functional predictors
#' (weighted by their Gram matrices) and scalar predictors against response y.
#'
#' @param W A `predictor_hybrid` object.
#' @param y A numeric response vector of length equal to number of samples.
#'
#' @return A matrix representing the corrected cross-covariance structure.
#' @export
get_pre_corrcov <- function(W, y){
  n <- W$n_sample
  K <- W$n_functional

  # building blocks
  ## C_J : C multiplied by J. Scalable to arbitrary K
  C_J <- get_CJ(W)

  
  ## covariance
  J_Ct_y <- t(C_J) %*% y
  Zt_y <- t(W$Z) %*% y

  # V matrix
  upper_left <- J_Ct_y %*% t(J_Ct_y)
  upper_right <- J_Ct_y %*% t(Zt_y)
  lower_left <- t(upper_right)
  lower_right <- Zt_y %*% t(Zt_y)

  V_star <- cbind(
    rbind(upper_left, lower_left),
    rbind(upper_right, lower_right)
    )/(n^2)

  return(V_star)
  }
```




### main function (wrapper: get_pls_comp
Computes the first PLS component from a hybrid predictor and response vector using regularization.

- Inputs
  - **`W`**: A `predictor_hybrid` object with functional and scalar predictors.
  - **`y`**: A numeric response vector of length equal to `W$n_sample`.
  - **`L`**: A Cholesky decomposition of a positive definite regularization matrix.
- Output: 
  - `xi_hat`: The first PLS component as a `predictor_hybrid` object
  - `E`: The matrix `inv(L) %*% V_star %*% t(inv(L))`
  - `V_star`: Cross-covariance matrix between predictors and response
  - `eigen_val`: Leading eigenvalue of `E`
```{r}
#' Compute the First PLS Component from a Hybrid Predictor
#'
#' Computes the coefficients of the first Partial Least Squares (PLS) component based on 
#' a hybrid predictor object and a response vector, using a regularized generalized eigenvalue problem.
#'
#' @param W A predictor_hybrid object containing both functional and scalar predictors.
#' @param y A numeric response vector of length equal to the number of samples in W.
#' @param L cholesky decompisition of a regularization matrix (typically positive definite) used in the generalized eigenproblem.
#'
#' @return A list with the following elements:
#'
#'   xi_hat: The estimated first PLS component as a predictor_hybrid object.
#'   E: The generalized eigenvalue problem matrix E = inv(L) V* t(inv(L)).
#'   V_star: The cross-covariance matrix between predictors and response.}
#'   eigen_val: The leading eigenvalue of E.
#' 
#'
#' @export
get_pls_comp <- function(W, y, L){

  
  V_star <- get_pre_corrcov(W, y)

  #invL <- Matrix::chol2inv(L)
  invL <- solve(L)
  A <- V_star %*% t(invL)       # A = V* × t(inv(L))
  E <- invL %*% A               # E = inv(L) × A

  eigen_result <- eigen(E)
  e <- eigen_result$vectors[, 1]
  if (is.complex(e)){
    print("stop")
    return("stop")
  } 


  xi_star <- t(invL) %*% e      # xi_star solves t(L) xi = e
  xi_hat <- predictor_hybrid_from_coef(format = W, coef = xi_star)
  return(xi_hat)
}
```

## linear system method

### helper: get_xi_hat_linear
```{r}
get_xi_hat_linear_pen <- function(W, y, lambda){
  # new
  ## step 1: inner products
    n <- W$n_sample
  K <- W$n_functional
  u <- gamma <- list()
  
  v <- t(W$Z) %*% y
  q <- sum(v^2)
  for (j in 1:K){
    Theta_t <- W$functional_list[[j]]$coefs
    B <- W$gram_list[[j]]
    u[[j]] <- B %*% Theta_t %*% y
    

    # linear system
    R <- W$gram_list[[j]] + lambda[j] * W$gram_deriv2_list[[j]]
    gamma[[j]] <- solve(R, u[[j]])
    
     #q
    q <- q + sum( gamma[[j]] * ( R %*% gamma[[j]] ))
  }
  d_vec <- c(do.call(c, gamma), v)  # or: unlist(d)
  ## step 2: squared norm
  d_vec <- d_vec/ sqrt(q)
  xi_hat <- predictor_hybrid_from_coef(format = W, coef = d_vec)
  return(xi_hat)
}
```


the full vector of PLS scores $$\hat{\boldsymbol{\rho}}  = (\hat{\rho}_1, \ldots, \hat{\rho}_n)^\top$$
	is computed through the following matrix multiplication:
$$
		\hat{\boldsymbol{\rho}}^\top = \sum_{k=1}^K ( \hat{\boldsymbol{\gamma}}_j )^\top B \, \Theta_j^\top + \boldsymbol{\zeta}^\top \mathbf{Z}^\top.
$$	
```{r}
get_rho <- function(d_vec, W){
  K <- W$n_functional
  M <- W$n_basis_list[[1]]
  n_scalar <- W$n_scalar
  n <- W$n_sample
  rho <- rep(0, n)
  
  #functional part
  for (j in 1:K){
    start_index <- (j-1)*M + 1
    end_index <- j*M
    gamma_j <- d_vec[ start_index : end_index]
    Theta_j  <- t(W$functional_list[[j]]$coefs)
    B <- W$gram_list[[j]]
    rho <- rho + Theta_j %*% B %*% gamma_j
  }
      
  #scalar part
  rho <- rho + W$Z %*% d_vec[(K * M + 1):length(d_vec)]
  return (as.vector(rho))
}
```


```{r}
library(fda)

# --- Step 1: Simulate Data ---
set.seed(111)

n_sample <- 100
n_scalar <- 2
n_basis <- 20
n_functional <- 2

# Scalar predictors Z (centered)
Z <- matrix(rnorm(n_sample * n_scalar), ncol = n_scalar)
Z <- scale(Z, center = TRUE, scale = FALSE)

# Create functional predictors (centered)
basis <- create.bspline.basis(c(0, 1), nbasis = n_basis)
functional_list <- list()
for (j in 1:n_functional) {
  coefs <- matrix(rnorm(n_basis * n_sample), nrow = n_basis)
  fdobj <- fd(coef = coefs, basisobj = basis)
  mean_fd <- mean.fd(fdobj)
  mean_coef <- coef(mean_fd)
  centered_coefs <- coefs - matrix(mean_coef, nrow = n_basis, ncol = n_sample)
  functional_list[[j]] <- fd(coef = centered_coefs, basisobj = basis)
}

# --- Step 2: Construct hybrid object ---
W <- predictor_hybrid(Z = Z, functional_list = functional_list)

# --- Step 3: Simulate and center response ---
total_coef_len <- sum(W$n_basis_list) + W$n_scalar
true_coef <- rnorm(total_coef_len)

# Flatten design matrix
Xmat <- matrix(NA, n_sample, total_coef_len)
for (i in 1:n_sample) {
  W_i <- subset_predictor_hybrid(W, i)
  func_coefs <- do.call(c, lapply(W_i$functional_list, function(fdobj) as.vector(coef(fdobj))))
  Xmat[i, ] <- c(func_coefs, W_i$Z)
}

y <- Xmat %*% true_coef + rnorm(n_sample, sd = 0.1)
y <- as.vector(scale(y, center = TRUE, scale = FALSE))

# --- Step 4: Compute constraint matrix ---
lambda <- rep(0, W$n_functional)
#constr_mat <- get_constraint_matrix_hybrid(W, lambda)
#L <- chol(constr_mat)

# --- Step 5: Run eigen-based PLS ---
W_template <- W  # prevent mutation


pls_eig_result <- get_pls_comp(W, y, L)
xi_hat_eig <- pls_eig_result$xi_hat

xi_hat_pen <- get_xi_hat_linear_pen(W, y, lambda)
xi_hat_lin_new <- get_xi_hat_linear_new(W, y)
# Wrap into predictor_hybrid object using format template
xi_hat_direct <- predictor_hybrid_from_coef(format = W_template, coef = xi_hat_lin)
norm_val <- sqrt(inprod.predictor_hybrid(xi_hat_direct))
xi_hat_direct <- scalar_mul.predictor_hybrid(xi_hat_direct,  1 / norm_val)
xi_hat_direct_new <- predictor_hybrid_from_coef(format = W_template, coef = xi_hat_lin_new)

cat(xi_hat_direct_new$functional_list[[1]]$coefs- xi_hat_direct$functional_list[[1]]$coefs)
cat(xi_hat_direct_new$functional_list[[2]]$coefs- xi_hat_direct$functional_list[[2]]$coefs)


```

 

## Residualization

### response residualization

```{r}
get_nu <- function(y, rho){
            nu <- sum(y*rho) / sum(rho*rho)
            return(nu)  
}
```

```{r}
residualize_y <- function(y, rho, nu){
  y_next <- y - nu * rho
  return(y_next)
}
```

 


```{r}
LSE_ptws <- function(input, rho) {
  # old implementation
            rho_norm_sq <- (Matrix::norm(rho, type = "2"))^2
            nu <- as.numeric(t(rho) %*% input) / rho_norm_sq
            return(nu)
}

```

###  predictor residualization

$$
\widetilde{W}_i^{[l+1]} 
			:= 	\widetilde{W}_i^{[l]}  - 
			\widehat{\rho}_i^{[l]}
			\hat{\delta}^{[l]},~\text{where}~
			\delta^{[l]}
			:= 
			\frac{1}{\| 	\hat{\boldsymbol{\rho}}^{[l]}\|_2^2}
			\sum_{i=1}^n 
			\widehat{\rho}_i^{[l]}
			\widetilde{W}_i^{[l]},
$$


```{r}
get_delta <- function(W, rho){
  delta <- scalar_mul.predictor_hybrid(subset_predictor_hybrid(W, 1), rho[1])
   for (i in 2:length(rho)) {
  delta <- add.predictor_hybrid(delta, subset_predictor_hybrid(W, i),  rho[i]) 
   } 
  delta <- scalar_mul.predictor_hybrid(delta, 1/sum(rho*rho))
  return(delta)
}
```


```{r}
residualize_predictor <- function(W, rho, delta){
  n <- length(rho)
  W_res <- W
  for (i in 1:n){
    W_i <- subset_predictor_hybrid(W, i)
    W_res_i <-subtr.predictor_hybrid(W_i, delta,  rho[i]) 
    W_res <- replace_obs_hybrid(W_res, i, W_res_i)
  }
  return(W_res)
}
```

 


### LSE_hybrid

```{r}
LSE_hybrid <- function(W, rho) {
  # old implementation
  C_star_J_star <- matrix(NA, nrow = W$n_sample, ncol = sum(W$n_basis_list) + W$n_scalar)
  for (ii in (1 : W$n_functional)){
    cumsum_n_basis_now <- sum(W$n_basis_list[1:ii])
    n_basis_now <- W$n_basis_list[ii]
    C_star_J_star[, (cumsum_n_basis_now - n_basis_now + 1) :cumsum_n_basis_now] <-
      t(W$functional_list[[ii]]$coefs) %*% W$jacobian_list[[ii]]
  }
  C_star_J_star[ ,(sum(W$n_basis_list) + 1) : ncol(C_star_J_star)] <- W$Z
  rho_t_C_star_J_star <- t(rho) %*% C_star_J_star

  # matrices in the statement of Proposition 4
  J_star <-as.matrix(get_jacobian_hybrid(W))
  d_hat_star <- t( #basis coefficient of hybrid regression coefficientdelta
    solve(
      t((norm(rho, type="2"))^2 * J_star),
      t(rho_t_C_star_J_star)
      )
    )
  delta_hat <- hybrid_from_coef(W, d_hat_star) #hybrid regression coefficient
  return(delta_hat)
  }
```

 

# main algorithm

```{r eval=FALSE, include=FALSE}
#' @export
fit.hybridPLS <- function(W, y, n_iter, lambda) {
  # 1. Initialize storage
  W_now <- rho <- xi <- delta <- nu  <- iota <- beta <- list()
 
  # 3. Initialize residuals
  W_now[[1]] <- W
  y_now <- y
  for (l in 1:n_iter) {
    cat(paste(l, "th component", "\n"))

    xi[[l]]  <- get_xi_hat_linear_pen(W_now[[l]], y_now, lambda) # PLS direction
    rho[[l]] <- inprod.predictor_hybrid(W_now[[l]], xi[[l]]) #PLS score
    delta[[l]] <- get_delta(W_now[[l]], rho[[l]]);  
    W_now[[l + 1]] <- residualize_predictor(W_now[[l]], rho[[l]], delta[[l]]) # predictor residual
    nu[[l]] <- get_nu(y_now, rho[[l]]); y_now <- residualize_y(y_now, rho[[l]], nu[[l]]) # response residual
 
    iota[[l]] <- xi[[l]]
    if (l == 1) {
      beta[[l]] <- scalar_mul.predictor_hybrid( iota[[l]], nu[[l]] )
    } else {
      for (u in 1:(l - 1)) iota[[l]] <- subtr.predictor_hybrid( iota[[l]], iota[[u]], inprod.predictor_hybrid(delta[[u]], xi[[l]]))
      beta[[l]] <- add.predictor_hybrid(beta[[l - 1]], iota[[l]], nu[[l]])
    }
  }
  return (list(
    rho = rho,
    xi = xi,
    W = W_now,
    beta = beta
  ))
}

```


```{r eval=FALSE, include=FALSE}
#' @export
fit.hybridPLS_eigen <- function(W, y, n_iter, lambda) {
  # 1. Initialize storage
  W_now <- rho <- xi <- delta <- nu  <- sigma <- eta <- list()
 
  # 3. Initialize residuals
  W_now[[1]] <- W
  y_now <- y
  final_succesful_iteration <- 0
  constr_mat <- get_constraint_matrix_hybrid(W, lambda)
  constr_mat_chol <- t(chol(constr_mat))
  for (l in 1:n_iter) {
    cat(paste(l, "th component", "\n"))

    #xi[[l]]  <- get_xi_hat_linear_pen(W_now[[l]], y_now, lambda) # PLS direction
    xi[[l]]  <- get_pls_comp(W_now[[l]], y_now, constr_mat_chol) # PLS direction
    rho[[l]] <- inprod.predictor_hybrid(W_now[[l]], xi[[l]]) #PLS score
    delta[[l]] <- get_delta(W_now[[l]], rho[[l]]);  
    W_now[[l + 1]] <- residualize_predictor(W_now[[l]], rho[[l]], delta[[l]]) # predictor residual
    nu[[l]] <- get_nu(y_now, rho[[l]]); y_now <- residualize_y(y_now, rho[[l]], nu[[l]]) # response residual
 
    # Step 3: Orthogonalize and update prediction
    #sigma[[l]] <- xi[[l]]
    #if (l == 1) {
    #  eta[[l]] <- scalar_mul(sigma[[l]], nu[[l]])
    #} else {
    #  for (u in 1:(l - 1)) {
    #    sigma[[l]] <- subtr(sigma[[l]], sigma[[u]], hybrid_inner_prod(delta[[u]], xi[[l]]))
    #  }
    #  eta[[l]] <- add(eta[[l - 1]], sigma[[l]], nu[[l]])
    #}
  }
  return (list(
    rho = rho,
    xi = xi,
    W = W_now
  ))
}

```

```{r}
inprod.predictor_hybrid(res$W[[5]], res$xi[[3]])

sum(y * inprod.predictor_hybrid(res$W[[5]], res$xi[[3]]))

inprod_pen.predictor_hybrid(res$xi[[1]], res$xi[[8]], lambda) - inprod_pen.predictor_hybrid(res_eigen$xi[[1]], res_eigen$xi[[8]], lambda)
inprod_pen.predictor_hybrid(res$xi[[1]], res$xi[[8]], 10*lambda) - inprod_pen.predictor_hybrid(res_eigen$xi[[1]], res_eigen$xi[[8]], 10*lambda)
inprod_pen.predictor_hybrid(res$xi[[1]], res$xi[[8]], lambda/10) - inprod_pen.predictor_hybrid(res_eigen$xi[[1]], res_eigen$xi[[8]], lambda/10)
sum(y * inprod.predictor_hybrid(res$W[[8]], res$xi[[1]])) - sum(y * inprod.predictor_hybrid(res_eigen$W[[8]], res_eigen$xi[[1]]))






inprod_pen.predictor_hybrid(res$xi[[3]], res$xi[[4]], lambda) - inprod_pen.predictor_hybrid(res_eigen$xi[[3]], res_eigen$xi[[4]], lambda)
inprod_pen.predictor_hybrid(res$xi[[3]], res$xi[[4]], 10*lambda) - inprod_pen.predictor_hybrid(res_eigen$xi[[3]], res_eigen$xi[[4]], 10*lambda)
inprod_pen.predictor_hybrid(res$xi[[3]], res$xi[[4]], lambda/10)
sum(y * inprod.predictor_hybrid(res$W[[3]], res$xi[[1]]))


inprod_pen.predictor_hybrid(res$xi[[5]], res$xi[[10]], lambda)
inprod_pen.predictor_hybrid(res$xi[[5]], res$xi[[10]], 10*lambda)
inprod_pen.predictor_hybrid(res$xi[[5]], res$xi[[10]], lambda/10)
sum(y * inprod.predictor_hybrid(res$W[[5]], res$xi[[1]]))

inprod_pen.predictor_hybrid(res$xi[[4]], res$xi[[4]], lambda)
inprod_pen.predictor_hybrid(res$xi[[4]], res$xi[[4]], lambda/10)
inprod_pen.predictor_hybrid(res$xi[[4]], res$xi[[4]], 10*lambda )

```




```{r}
lambda <- c(0.5,0.5)
L=8
res <- fit.hybridPLS(W,y,L, lambda)
res_eigen <- fit.hybridPLS_eigen(W,y,L, lambda)
```


### nipals_pen_hybrid


- depends on:
  - 'get_constraint_matrix_hybrid'
  - 'get_pls_comp'
  - 'hybrid_inner_prod'
  - 'LSE_hybrid'
  
```{r eval=FALSE, include=FALSE}
#' Iterative Penalized NIPALS Algorithm for Hybrid PLS
#'
#' Performs iterative extraction of PLS components using a penalized NIPALS algorithm
#' for hybrid predictor objects that include both scalar and functional data.
#'
#' At each iteration:
#' \enumerate{
#'   \item Computes the first penalized PLS component using \code{get_pls_comp}.
#'   \item Computes PLS scores via hybrid inner products.
#'   \item Residualizes both predictors and response with respect to the extracted scores.
#'   \item Orthogonalizes and accumulates loading vectors to construct final linear predictors.
#' }
#' The same constraint matrix (computed once) is used across all iterations.
#'
#' @param W A \code{predictor_hybrid} object containing scalar and functional predictors.
#' @param y A numeric response vector of length equal to \code{W$n_sample}.
#' @param n_iter An integer specifying the number of PLS components (iterations) to compute.
#' @param lambda A numeric vector of smoothing parameters (length = number of functional predictors).
#' @param verbose Logical; if \code{TRUE}, prints progress for each component.
#'
#' @return This function operates by side effects. Internally, it constructs and updates:
#' \itemize{
#'   \item \code{xi}, \code{rho}, \code{delta}, \code{nu}: component vectors and regression effects,
#'   \item \code{sigma}, \code{eta}: orthogonalized components and accumulated prediction functions,
#'   \item \code{W_now}, \code{resid_y}: residualized predictor and response at each iteration,
#'   \item \code{E}, \code{V_star}, \code{eigen_val}: matrices for monitoring stability and eigenvalue progress.
#' }
#' The final number of successful iterations is tracked via \code{final_succesful_iteration}.
#'
#' @export
nipals_pen_hybrid <- function(W, y, n_iter, lambda, verbose) {
  # 1. Initialize storage
  rho <- xi <- delta <- nu  <- sigma <- eta <- list()
  E <- V_star <- eigen_val <- list()
  fitted_value_W <- fitted_value_y <- list()
  resid_y <- W_now <- list()
  first_eigen_val <- mse_W <- mse_y <- rep(NA, n_iter)

  # 2. Compute and store the constraint matrix once
  constr_mat <- get_constraint_matrix_hybrid(W, lambda)
  constr_mat_chol <- t(chol(constr_mat))

  # 3. Initialize residuals
  W_now[[1]] <- W
  y_now <- y
  final_succesful_iteration <- 0

  for (l in 1:n_iter) {
    if (verbose) cat(paste(l, "th component", "\n"))

    # Step 1: Compute penalized PLS direction and score
    pls_result_now <- get_pls_comp(W_now[[l]], y_now, constr_mat_chol, verbose)
    if (!is.list(pls_result_now)) break

    xi[[l]] <- pls_result_now$xi
    rho[[l]] <- hybrid_inner_prod(W_now[[l]], xi[[l]])

    # Step 2: Residualize predictor and response
    delta[[l]] <- LSE_hybrid(W_now[[l]], rho[[l]])
    fitted_value_W[[l]] <- fitted_value(delta[[l]], rho[[l]])
    W_now[[l + 1]] <- subtr(W_now[[l]], fitted_value_W[[l]])

    nu[[l]] <- LSE_ptws(y_now, rho[[l]])
    fitted_value_y[[l]] <- nu[[l]] * rho[[l]]
    y_now <- y_now - fitted_value_y[[l]]
    resid_y[[l]] <- y_now

    # Step 3: Orthogonalize and update prediction
    sigma[[l]] <- xi[[l]]
    if (l == 1) {
      eta[[l]] <- scalar_mul(sigma[[l]], nu[[l]])
    } else {
      for (u in 1:(l - 1)) {
        sigma[[l]] <- subtr(sigma[[l]], sigma[[u]], hybrid_inner_prod(delta[[u]], xi[[l]]))
      }
      eta[[l]] <- add(eta[[l - 1]], sigma[[l]], nu[[l]])
    }

    # Record diagnostics
    E[[l]] <- pls_result_now$E
    V_star[[l]] <- pls_result_now$V_star
    eigen_val[[l]] <- pls_result_now$eigen_val
    first_eigen_val[l] <- eigen_val[[l]][1]
    final_succesful_iteration <- l
  }
}

```


# Simulation tools

## sample splitting

```{r}
create_idx_train_test <-function(n_sample, train_ratio){
  n_train <- floor(n_sample*train_ratio)
  idx_train <- sort(sample(n_sample,n_train, replace=F))
  idx_test <- sort((1 : n_sample)[-idx_train])
  return(list(
    idx_train = idx_train, idx_test = idx_test
  ))
}
```



```{r}
create_idx_kfold <- function(n_sample, n_fold){
  idx_list <- caret::createFolds( (1 : n_sample), k = n_fold, list = TRUE, returnTrain = FALSE)
  idx_list_list <- list()
  for (i in 1:length(idx_list)){
    valid_hyper_index <- idx_list[[i]]
    train_hyper_index <- (1 : n_sample)[-valid_hyper_index]
    idx_list_list[[i]] <- list("idx_train" = train_hyper_index, "idx_valid" = valid_hyper_index)
  }
  return(idx_list_list)
}
```



```{r}
n_sample.fd <- function(fd_obj){
  length(fd_obj$fdnames$reps)
}
```


```{r}
normailize.all <- function(split.all_obj){
  curve_normalize_result_1 <- curve_normalize_train_test_fd(
    split.all_obj$data_fun_1_train,
    split.all_obj$data_fun_1_test)
  data_fun_1_train <- curve_normalize_result_1$train
  data_fun_1_test <- curve_normalize_result_1$test

  curve_normalize_result_2 <- curve_normalize_train_test_fd(
    split.all_obj$data_fun_2_train,
    split.all_obj$data_fun_2_test)
  data_fun_2_train <- curve_normalize_result_2$train
  data_fun_2_test <- curve_normalize_result_2$test

  scalar_normalize_result <- scalar_normalize_train_test(
    split.all_obj$data_scalar_train,
    split.all_obj$data_scalar_test)
  data_scalar_train <- scalar_normalize_result$train
  data_scalar_test <- scalar_normalize_result$test

  between_normalize_result <- btwn_normalize_train_test(
    list(data_fun_1_train, data_fun_2_train),
    data_scalar_train,
    data_scalar_test
  )
  data_scalar_train <- between_normalize_result$train
  data_scalar_test <- between_normalize_result$test

  response_mean_train <-  mean(split.all_obj$response_train)
  response_train <- split.all_obj$response_train - response_mean_train
  response_test <- split.all_obj$response_test - response_mean_train

  return(
    list(
      data_fun_1_train = data_fun_1_train,
      data_fun_2_train = data_fun_2_train,
      data_scalar_train= data_scalar_train,
      response_train   = response_train,
      data_fun_1_test  = data_fun_1_test,
      data_fun_2_test  = data_fun_2_test,
      data_scalar_test = data_scalar_test,
      response_test    = response_test
    )
  )
}
```







# Baseline method: FPCA regression


functions in MFPCA packages use an internal function calcBasisIntegrals. Somehow, this function did not work well on our simulations setting. Thus we modify the codes 
```{r}

# define global variable j, used by the foreach package and confusing R CMD CHECK
globalVariables('j')

#' Utility function that calculates matrix of basis-scalar products (one dimension)
#'
#' If the element \eqn{X^{(j)}}{X^{(j)}} is expanded in basis functions \eqn{b_i^{(j)}(t),~ i = 1, \ldots, K_j}{b_i(t)},
#' this function calculates the \eqn{K_j \times K_j}{K_j  \times K_j} matrix \eqn{B^{(jj)}}{B^{(jj)}} with entries
#' \deqn{B^{(jj)}_{mn} = \int_{\mathcal{T_j}} b_m^{(j)}(t) b_n^{(j)}(t) \mathrm{d} t}.
#'
#' @section Warning: This function is implemented only for functions on one- or two-dimensional domains.
#'
#' @param basisFunctions Array of \code{npc} basis functions of dimensions \code{npc x M1} or \code{npc x M1 x M2}.
#' @param dimSupp dimension of the support of the basis functions (1 or 2)
#' @param argvals List of corresponding x-values.
#'
#' @return A matrix containing the scalar product of all combinations of basis functions (matrix \eqn{B^{(j)}})
#'
#' @seealso \code{\link{MFPCA}}, \code{\link[funData]{dimSupp}}
#'
#' @keywords internal
calcBasisIntegrals <- function(basisFunctions, dimSupp, argvals)
{
  npc <- dim(basisFunctions)[1]

  #  integral basis matrix
  B <- array(0, dim = c(npc, npc))


  if(dimSupp == 1) # one-dimensional domain
  {
    w <- funData::.intWeights(argvals[[1]])

    for(m in seq_len(npc))
    {
      for(n in seq_len(m))
        B[m, n] <- B[n, m] <- (basisFunctions[m, ]* basisFunctions[n, ])%*%w
    }
  }
  else # two-dimesional domain (otherwise function stops before!)
  {
    w1 <- t(funData::.intWeights(argvals[[1]]))
    w2 <- funData::.intWeights(argvals[[2]])

    for(m in seq_len(npc))
    {
      for(n in seq_len(m))
        B[m, n] <- B[n, m] <-  w1 %*%(basisFunctions[m, , ]* basisFunctions[n, ,])%*%w2
    }
  }

  return(B)
}
```


###   Multivariate functional principal component analysis for functions on different (dimensional) domains

- This function calculates a multivariate functional principal component analysis (MFPCA) based on i.i.d. observations $x_1, \ldots, x_N$ of a multivariate functional data-generating process $X = (X^{(1)}, \ldots  X^{(p)})}{X = X^(1), \ldots, X^(p)$ with elements $X^{(j)} \in L^2(\mathcal{T}_j)}{X^(j) in L^2(calT_j)$ defined on a domain $\mathcal{T}_j \subset IR^{d_j}}{calT_j of IR^{d_j}$.
- In particular, the elements can be defined on different (dimensional) domains.
- The results contain the mean function, the estimated multivariate functional principal components \eqn{\hat \psi_1, \ldots, \hat \psi_M} (having the same structure as $x_i$), the associated eigenvalues $\hat \nu_1 \geq \ldots \geq \hat \nu_M > 0$ and the individual scores \eqn{\hat \rho_{im} = \widehat{<x_i, \psi_m>}}{\hat \rho_{im} = \hat{<x_i, \psi_m>}}.



- Univariate Expansions
    - The multivariate functional principal component analysis relies on a univariate basis expansion for each element $X^{(j)}$.
    - The univariate basis representation is calculated using the univDecomp function, that passes the univariate functional observations and optional parameters to the specific function.
    - The univariate decompositions are specified via the uniExpansions argument in the MFPCA function.
    - It is a list of the same length as the mFData object, i.e. having one entry for each element of the multivariate functional data.
    - For each element, uniExpansion must specify at least the type of basis functions to use.
    - Additionally, one may add further parameters.
    - The following basis representations are supported: \itemize{
        -  Given basis functions. Then \code{uniExpansions[[j]] = list(type = "given", functions)}, where \code{functions} is a \code{funData} object on the same domain as \code{mFData}, containing the given basis functions. 
        - Univariate functional principal component analysis. Then \code{uniExpansions[[j]] = list(type = "uFPCA", nbasis, pve, npc, makePD)}, where \code{nbasis,pve,npc,makePD} are parameters passed to the \code{\link{PACE}} function for calculating the univariate functional principal component analysis.
        - Basis functions expansions from the
  package \pkg{fda}. Then \code{uniExpansions[[j]] = list(type = "fda", ...)},
#' where \code{...} are passed to \code{\link[funData]{funData2fd}}, which
#' heavily builds on \code{\link[fda]{eval.fd}}. If \pkg{fda} is not available,
#' a warning is thrown.
\item Spline basis functions (not penalized). Then
#' \code{uniExpansions[[j]] = list(type = "splines1D", bs, m, k)}, where
#' \code{bs,m,k} are passed to the functions \code{\link{univDecomp}} and
#' \code{\link{univExpansion}}. For two-dimensional tensor product splines, use
#' \code{type = "splines2D"}. 
\item Spline basis functions (with smoothness
#' penalty). Then \code{uniExpansions[[j]] = list(type = "splines1Dpen", bs, m,
#' k)}, where \code{bs,m,k} are passed to the functions \code{\link{univDecomp}}
#' and \code{\link{univExpansion}}. Analogously to the unpenalized case, use
#' \code{type = "splines2Dpen"} for 2D penalized tensor product splines. 
\item
#' Cosine basis functions. Use \code{uniExpansions[[j]] = list(type = "DCT2D",
#' qThresh, parallel)} for functions one two-dimensional domains (images) and
#' \code{type = "DCT3D"} for 3D images. The calculation is based on the discrete
#' cosine transform (DCT) implemented in the C-library \code{fftw3}. If this
#' library is not available, the function will throw  a warning. \code{qThresh}
#' gives the quantile for hard thresholding the basis coefficients based on
#' their absolute value. If \code{parallel = TRUE}, the coefficients for
#' different images are calculated in parallel.} See \code{\link{univDecomp}}
#' and \code{\link{univExpansion}} for details.}

```{r}
#'
#' @export MFPCA
MFPCA <- function(mFData, mFData_predict, M, uniExpansions, weights = rep(1, length(mFData)), fit = FALSE, approx.eigen = FALSE,
                  bootstrap = FALSE, nBootstrap = NULL, bootstrapAlpha = 0.05, bootstrapStrat = NULL,
                  verbose = options()$verbose)
{
  if(! inherits(mFData, "multiFunData"))
    stop("Parameter 'mFData' must be passed as a multiFunData object.")

  # number of components
  p <- length(mFData)
  # number of observations
  N <- nObs(mFData)

  if(!all(is.numeric(M), length(M) == 1, M > 0))
    stop("Parameter 'M' must be passed as a number > 0.")

  if(!(is.list(uniExpansions) & length(uniExpansions) == p))
    stop("Parameter 'uniExpansions' must be passed as a list with the same length as 'mFData'.")

  if(!(is.numeric(weights) & length(weights) == p))
    stop("Parameter 'weights' must be passed as a vector with the same length as 'mFData'.")

  if(!is.logical(fit))
    stop("Parameter 'fit' must be passed as a logical.")

  if(!is.logical(approx.eigen))
    stop("Parameter 'approx.eigen' must be passed as a logical.")

  if(!is.logical(bootstrap))
    stop("Parameter 'bootstrap' must be passed as a logical.")

  if(bootstrap)
  {
    if(is.null(nBootstrap))
      stop("Specify number of bootstrap iterations.")

    if(any(!(0 < bootstrapAlpha & bootstrapAlpha < 1)))
      stop("Significance level for bootstrap confidence bands must be in (0,1).")

    if(!is.null(bootstrapStrat))
    {
      if(!is.factor(bootstrapStrat))
        stop("bootstrapStrat must be either NULL or a factor.")

      if(length(bootstrapStrat) != nObs(mFData))
        stop("bootstrapStrat must have the same length as the number of observations in the mFData object.")
    }
  }

  if(!is.logical(verbose))
    stop("Parameter 'verbose' must be passed as a logical.")

  # dimension for each component
  dimSupp <- dimSupp(mFData)

  # get type of univariate expansions
  type <- vapply(uniExpansions, function(l){l$type}, FUN.VALUE = "")

  # de-mean functions -> coefficients are also de-meaned!
  # do not de-mean in uFPCA, as PACE gives a smooth estimate of the mean (see below)
  m <- meanFunction(mFData, na.rm = TRUE) # ignore NAs in data
  for(j in seq_len(p))
  {
    if(type[j] != "uFPCA")
      mFData[[j]] <- mFData[[j]] - m[[j]]
  }

  if(verbose)
    cat("Calculating univariate basis expansions (", format(Sys.time(), "%T"), ")\n", sep = "")



########### modified by Jongmin Mun #############################
  # calculate univariate basis expansion for all components
  uniBasis <- mapply(
    function(expansion, data, data_predict){
      do.call(univDecomp, c(
        list(funDataObject = data, funDataObject_predict = data_predict),
        expansion)
        )
      },
    expansion = uniExpansions,
    data = mFData,
    data_predict = mFData_predict,
    SIMPLIFY = FALSE
    )
########### modified by Jongmin Mun #############################





  # for uFPCA: replace estimated mean in m
  for(j in seq_len(p))
  {
    if(type[j] == "uFPCA")
      m[[j]] <- uniBasis[[j]]$meanFunction
  }

  # Multivariate FPCA
  npc <- vapply(uniBasis, function(x){dim(x$scores)[2]}, FUN.VALUE = 0) # get number of univariate basis functions

  if(M > sum(npc))
  {
    M <- sum(npc)
    warning("Function MFPCA: total number of univariate basis functions is smaller than given M. M was set to ", sum(npc), ".")
  }

  # check if non-orthonormal basis functions used
  if(all(foreach::foreach(j = seq_len(p), .combine = "c")%do%{uniBasis[[j]]$ortho}))
    Bchol = NULL
  else
  {
    # Cholesky decomposition of B = block diagonal of Cholesky decompositions
    Bchol <- Matrix::bdiag(lapply(uniBasis, function(l){
      if(l$ortho)
        res <- Matrix::Diagonal(n = ncol(l$scores))
      else
        res <- Matrix::chol(l$B)

      return(res)}))
  }

  if(verbose)
    cat("Calculating MFPCA (", format(Sys.time(), "%T"), ")\n", sep = "")

  mArgvals <- if (utils::packageVersion("funData") <= "1.2") {
    getArgvals(mFData)
  } else {
    funData::argvals(mFData)
  }

  res <- calcMFPCA(N = N, p = p, Bchol = Bchol, M = M, type = type, weights = weights,
                   npc = npc, argvals = mArgvals, uniBasis = uniBasis, fit = fit, approx.eigen = approx.eigen)

  res$meanFunction <- m # return mean function, too

  names(res$functions) <- names(mFData)

  #############modified by jongmin mun


  #####################################
  if(fit)
  {
    res$fit <- m + res$fit # add mean function to fits
    names(res$fit) <- names(mFData)
  }

  # give correct names
  namesList <- lapply(mFData, names)
  if(!all(vapply(namesList, FUN = is.null, FUN.VALUE = TRUE)))
  {
    if(length(unique(namesList)) != 1)
      warning("Elements have different curve names. Use names of the first element for the results.")

    row.names(res$scores) <- namesList[[1]]

    if(fit)
      for(i in seq_len(p))
        names(res$fit[[i]]) <- namesList[[1]]
  }


  if(type[1] == "uFPCA"){
    scores.pred<-calcMFPCA_predict(N = nObs(mFData_predict),
                      p = p,
                      M = M,
                      weights = weights,
                      npc = npc,
                      uniBasis = uniBasis,
                      normFactors = res$normFactors,
                      vectors_train=res$vectors,
                      values_train=res$values
    )
  }
  res$scores.pred <- scores.pred

  class(res) <- "MFPCAfit"
    return(res)
}

```


### calcMFPCA
```{r}
#' Internal function that implements the MFPCA algorithm for given univariate decompositions
#' @export
calcMFPCA <- function(N, p, Bchol, M, type, weights, npc, argvals, uniBasis, fit = FALSE, approx.eigen = FALSE)
{
  # combine all scores
  allScores <- foreach::foreach(j = seq_len(p), .combine = "cbind")%do%{uniBasis[[j]]$scores}

  # block vector of weights
  allWeights <- foreach::foreach(j = seq_len(p), .combine = "c")%do%{rep(sqrt(weights[j]), npc[j])}

  Z <- allScores %*% Matrix::Diagonal(x = allWeights) / sqrt(N-1)

  # check if approximation is appropriate (cf. irlba)
  if(approx.eigen & (M > min(N, sum(npc))/2))
  {
    warning("Calculating a large percentage of principal components, approximation may not be appropriate.
            'approx.eigen' set to FALSE.")
    approx.eigen = FALSE
  }

  # check if non-orthonormal basis functions used and calculate PCA on scores
  if(is.null(Bchol))
  {
    if(approx.eigen)
    {
      tmpSVD <- irlba::irlba(as.matrix(Z), nv = M)

      vectors <- tmpSVD$v
      values <- tmpSVD$d[seq_len(M)]^2
    }
    else
    {
      if(sum(npc) > 1000)
        warning("MFPCA with > 1000 univariate eigenfunctions and approx.eigen = FALSE. This may take some time...")

      e <- eigen(stats::cov(allScores) * outer(allWeights, allWeights, "*"))

      values <- e$values[seq_len(M)]
      vectors <- e$vectors[,seq_len(M)]
    }
  }
  else
  {
    if(approx.eigen)
    {
      tmpSVD <- irlba::irlba(as.matrix(Matrix::tcrossprod(Z, Bchol)), nv = M)

      vectors <- Matrix::crossprod(Bchol, tmpSVD$v)
      values <- tmpSVD$d[seq_len(M)]^2
    }
    else
    {
      if(sum(npc) > 1000)
        warning("MFPCA with > 1000 univariate eigenfunctions and approx.eigen = FALSE. This may take some time...")

      e <- eigen(Matrix::crossprod(Bchol) %*% (stats::cov(allScores) * outer(allWeights, allWeights, "*")))

      values <- Re(e$values[seq_len(M)])
      vectors <- Re(e$vectors[,seq_len(M)])
    }
  }

  # normalization factors
  normFactors <- 1/sqrt(diag(as.matrix(Matrix::crossprod(Z %*% vectors))))

  ### Calculate scores
  scores <- Z %*% vectors * sqrt(N-1) # see defintion of Z above!
  scores <- as.matrix(scores %*% diag(sqrt(values) * normFactors, nrow = M, ncol = M)) # normalization

  ### Calculate eigenfunctions (incl. normalization)
  npcCum <- cumsum(c(0, npc)) # indices for blocks (-1)

  tmpWeights <- as.matrix(Matrix::crossprod(Z, Z %*%vectors))
  eFunctions <- foreach::foreach(j = seq_len(p)) %do% {
    univExpansion(type = type[j],
                  scores = 1/sqrt(weights[j] * values) * normFactors * t(tmpWeights[npcCum[j]+seq_len(npc[j]), , drop = FALSE]),
                  argvals = argvals[[j]],
                  functions = uniBasis[[j]]$functions,
                  params = uniBasis[[j]]$settings)
  }

  res <- list(values = values,
              functions = multiFunData(eFunctions),
              scores = scores,
              vectors = vectors,
              values = values,
              normFactors = normFactors,
              uniBasis = uniBasis,
              uniExpansions = uniExpansions
  )

  # calculate truncated Karhunen-Loeve representation (no mean here)
  if(fit)
    res$fit <- multivExpansion(multiFuns = res$functions, scores = scores)

  return(res)
}
```


### calcMFPCA

```{r}
#' Internal function that implements the MFPCA algorithm for given univariate decompositions
#' @export
#' 
calcMFPCA <- function(N, p, Bchol, M, type, weights, npc, argvals, uniBasis, fit = FALSE, approx.eigen = FALSE)
{
  # combine all scores
  allScores <- foreach::foreach(j = seq_len(p), .combine = "cbind")%do%{uniBasis[[j]]$scores}

  # block vector of weights
  allWeights <- foreach::foreach(j = seq_len(p), .combine = "c")%do%{rep(sqrt(weights[j]), npc[j])}

  Z <- allScores %*% Matrix::Diagonal(x = allWeights) / sqrt(N-1)

  # check if approximation is appropriate (cf. irlba)
  if(approx.eigen & (M > min(N, sum(npc))/2))
  {
    warning("Calculating a large percentage of principal components, approximation may not be appropriate.
            'approx.eigen' set to FALSE.")
    approx.eigen = FALSE
  }

  # check if non-orthonormal basis functions used and calculate PCA on scores
  if(is.null(Bchol))
  {
    if(approx.eigen)
    {
      tmpSVD <- irlba::irlba(as.matrix(Z), nv = M)

      vectors <- tmpSVD$v
      values <- tmpSVD$d[seq_len(M)]^2
    }
    else
    {
      if(sum(npc) > 1000)
        warning("MFPCA with > 1000 univariate eigenfunctions and approx.eigen = FALSE. This may take some time...")

      e <- eigen(stats::cov(allScores) * outer(allWeights, allWeights, "*"))

      values <- e$values[seq_len(M)]
      vectors <- e$vectors[,seq_len(M)]
    }
  }
  else
  {
    if(approx.eigen)
    {
      tmpSVD <- irlba::irlba(as.matrix(Matrix::tcrossprod(Z, Bchol)), nv = M)

      vectors <- Matrix::crossprod(Bchol, tmpSVD$v)
      values <- tmpSVD$d[seq_len(M)]^2
    }
    else
    {
      if(sum(npc) > 1000)
        warning("MFPCA with > 1000 univariate eigenfunctions and approx.eigen = FALSE. This may take some time...")

      e <- eigen(Matrix::crossprod(Bchol) %*% (stats::cov(allScores) * outer(allWeights, allWeights, "*")))

      values <- Re(e$values[seq_len(M)])
      vectors <- Re(e$vectors[,seq_len(M)])
    }
  }

  # normalization factors
  normFactors <- 1/sqrt(diag(as.matrix(Matrix::crossprod(Z %*% vectors))))

  ### Calculate scores
  scores <- Z %*% vectors * sqrt(N-1) # see defintion of Z above!
  scores <- as.matrix(scores %*% diag(sqrt(values) * normFactors, nrow = M, ncol = M)) # normalization

  ### Calculate eigenfunctions (incl. normalization)
  npcCum <- cumsum(c(0, npc)) # indices for blocks (-1)

  tmpWeights <- as.matrix(Matrix::crossprod(Z, Z %*%vectors))
  eFunctions <- foreach::foreach(j = seq_len(p)) %do% {
    univExpansion(type = type[j],
                  scores = 1/sqrt(weights[j] * values) * normFactors * t(tmpWeights[npcCum[j]+seq_len(npc[j]), , drop = FALSE]),
                  argvals = argvals[[j]],
                  functions = uniBasis[[j]]$functions,
                  params = uniBasis[[j]]$settings)
  }

  res <- list(values = values,
              functions = multiFunData(eFunctions),
              scores = scores,
              vectors = vectors,
              values = values,
              normFactors = normFactors,
              uniBasis = uniBasis,
              uniExpansions = uniExpansions
  )

  # calculate truncated Karhunen-Loeve representation (no mean here)
  if(fit)
    res$fit <- multivExpansion(multiFuns = res$functions, scores = scores)

  return(res)
}
```

### calcMFPCA_predict

```{r}
#' calcMFPCA_predict
#' @export
#' 
calcMFPCA_predict <- function(N, p, M, weights, npc, uniBasis,
                              normFactors_train, vectors_train, values_train)
{
  # combine all scores
  allScores <- foreach::foreach(j = seq_len(p), .combine = "cbind")%do%{uniBasis[[j]]$scores.pred}

  # block vector of weights
  allWeights <- foreach::foreach(j = seq_len(p), .combine = "c")%do%{rep(sqrt(weights[j]), npc[j])}

  Z <- allScores %*% Matrix::Diagonal(x = allWeights) / sqrt(N-1)


  ### Calculate scores
  scores <- Z %*% vectors_train * sqrt(N-1) # see defintion of Z above!
  scores <- as.matrix(scores %*% diag(sqrt(values_train) * normFactors_train, nrow = M, ncol = M)) # normalization

  return(scores)
}

```


```{r}
load("C:/Users/jongmin/Documents/GitHub/fsPLS/simul/graphical_high_correlation/graphical_data.RData")
```

```{r}
library(refund)
eval_point<-seq(1, 100, length.out = 100)
n_iter_max = 40
n_basis_predictor = 10
n_scalar <- 9
library(foreach)
library(funData)
uniExpansions <- list(
  list(type = "uFPCA", nbasis = n_basis_predictor, npc = n_iter_max),
  list(type = "uFPCA", nbasis = n_basis_predictor, npc = n_iter_max)
)

n_fold = 5
n_scalar <- 9
rmse_simul_fpca <- matrix(NA, nrow = n_rep, ncol = 3)
colnames(rmse_simul_fpca) <- c("FPC+PC", "FPC_only", "PC_only")
```



```{r}




  train_test_idx <- create_idx_train_test(n_sample, train.ratio)
  train_test_split <- split.all(data_fun_1[[rep_number]], data_fun_2[[rep_number]], data_scalar[[rep_number]], response[[rep_number]], train_test_idx)
  train_test_split_normalized <- normailize.all(train_test_split)

  kfold_idx <- create_idx_kfold(length(train_test_idx$idx_train), n_fold)
  cv_mat <- matrix(0, nrow = n_iter_max, ncol = n_scalar)
  for (fold_num in 1:n_fold){
    cat(paste0(fold_num, "th fold\n"))
    #train-valid split (5 fold)
    train_valid_idx <- kfold_idx[[fold_num]]
    cv_train_valid_split <- split.all(
      train_test_split$data_fun_1_train, train_test_split$data_fun_2_train, train_test_split$data_scalar_train, train_test_split$response_train,
      train_valid_idx)

    #normalizing
    cv_train_valid_split_normalized <- normailize.all(cv_train_valid_split)

    #format



    predictor_cv_train <- multiFunData(
      fd2funData(cv_train_valid_split_normalized$data_fun_1_train, eval_point),
      fd2funData(cv_train_valid_split_normalized$data_fun_2_train, eval_point)
    )
    predictor_cv_valid <- multiFunData(
      fd2funData(cv_train_valid_split_normalized$data_fun_1_test, eval_point),
      fd2funData(cv_train_valid_split_normalized$data_fun_2_test, eval_point)
    )

    # train FPCA

    MFPCA_fit_cv <- MFPCA(mFData = predictor_cv_train, mFData_predict = predictor_cv_valid, M = n_iter_max, uniExpansions = uniExpansions)

    #train scalar PCA
    PCA_fit_train <- prcomp(cv_train_valid_split_normalized$data_scalar_train, center=F, scale = F)
    #validate
    for (n_fpc in 1:n_iter_max){
      for (n_spc in 1:n_scalar){
        cat(paste(n_fpc, n_spc, "\n"))
        FPC_score_cv <- (MFPCA_fit_cv$scores)[ ,(1:n_fpc)]
        PC_score_cv <- (PCA_fit_train$x)[ , (1:n_spc)]
        pc_score_cv_combined <- data.frame(cv_train_valid_split_normalized$response_train, FPC_score_cv, PC_score_cv)
        colnames(pc_score_cv_combined) <- c(
          c("response"),
          paste0( rep("FPC", n_fpc), as.character(1:n_fpc)),
          paste0( rep("PC", n_spc), as.character(1:n_spc))
        )

        # fit linear regression
        rank_now <- rankMatrix(pc_score_cv_combined[,-1])[1]
        print(rank_now)
        cat(paste("rank deficient:", (n_fpc + n_spc)-rank_now, "\n"))
        linear_fit_cv <- lm(response~., data = pc_score_cv_combined)


        #validation set pc scores
        PFC_score_valid <- (MFPCA_fit_cv$scores.pred)[ ,(1:n_fpc)]
        PC_score_valid <- (predict(PCA_fit_train, cv_train_valid_split_normalized$data_scalar_test))[ , (1:n_spc)]
        pc_score_valid_combined <- data.frame(PFC_score_valid, PC_score_valid)
        colnames(pc_score_valid_combined) <- c(
          paste0( rep("FPC", n_fpc), as.character(1:n_fpc)),
          paste0( rep("PC", n_spc), as.character(1:n_spc))
          )

        y_pred <- predict(linear_fit_cv, pc_score_valid_combined)
        cv_mat[n_fpc, n_spc] <- cv_mat[n_fpc, n_spc] +
          sqrt( mean( sum( (y_pred - cv_train_valid_split_normalized$response_test)^2 ) ))
        }
    }
  }
    n_iter_best <- arrayInd(which.min(cv_mat), dim(cv_mat))
    n_fpc_best  <- n_iter_best[1]
    n_spc_best  <- n_iter_best[2]
    print(n_fpc_best)
    print(n_spc_best)

    predictor_final_train <- multiFunData(
      fd2funData(train_test_split_normalized$data_fun_1_train, eval_point),
      fd2funData(train_test_split_normalized$data_fun_2_train, eval_point)
    )
    predictor_final_test <- multiFunData(
      fd2funData(train_test_split_normalized$data_fun_1_test, eval_point),
      fd2funData(train_test_split_normalized$data_fun_2_test, eval_point)
    )

    # train FPCA and scalar PCA
    MFPCA_fit_final <- MFPCA(mFData = predictor_final_train, mFData_predict = predictor_final_test, M = n_fpc_best, uniExpansions = uniExpansions)
    PCA_fit_final <- prcomp(train_test_split_normalized$data_scalar_train, center=F, scale = F)
    PFC_score_final_train <- MFPCA_fit_final$scores
    PC_score_final_train <- (predict(PCA_fit_final, train_test_split_normalized$data_scalar_train))[,c(1:n_spc_best)]
    pc_score_final_train_combined <- data.frame(train_test_split_normalized$response_train, PFC_score_final_train, PC_score_final_train)
    colnames(pc_score_final_train_combined) <- c(
      c("response"),
      paste0( rep("FPC", n_fpc_best), as.character(1:n_fpc_best)),
      paste0( rep("PC", n_spc_best), as.character(1:n_spc_best))
    )
    linear_fit_final <- lm(response~., data = pc_score_final_train_combined)

    # test set pc scores
    PFC_score_final_test <- MFPCA_fit_final$scores.pred
    PC_score_final_test <- (predict(PCA_fit_final, train_test_split_normalized$data_scalar_test))[,c(1:n_spc_best)]
    pc_score_final_test_combined <- data.frame(PFC_score_final_test, PC_score_final_test)
    colnames(pc_score_final_test_combined) <- c(
      paste0( rep("FPC", n_fpc_best), as.character(1:n_fpc_best)),
      paste0( rep("PC", n_spc_best), as.character(1:n_spc_best))
    )
    y_pred_final <- predict(linear_fit_final, pc_score_final_test_combined)
    rmse_simul_fpca[rep_number,1] <- sqrt(
      mean((y_pred_final - train_test_split_normalized$response_test)^2)
      )
    print(rmse_simul_fpca[rep_number,1])


    plot(-0.2:0.2, -0.2:0.2, pch = "", xlim = c(-0.2, 0.2),  ylim = c(-0.2, 0.2))
    plot(train_test_split_normalized$response_test, y_pred_final)
    abline(a=0, b=1)


    pc_score_final_train_combined_fpc_only <- data.frame(train_test_split_normalized$response_train, PFC_score_final_train)
    colnames(pc_score_final_train_combined_fpc_only) <- c(
      c("response"),
      paste0( rep("FPC", n_fpc_best), as.character(1:n_fpc_best))
    )
    linear_fit_final_fpc_only <- lm(response~., data = pc_score_final_train_combined_fpc_only)
    pc_score_final_test_combined_fpc_only <- data.frame(PFC_score_final_test)
    colnames(pc_score_final_test_combined_fpc_only) <- c(
      paste0( rep("FPC", n_fpc_best), as.character(1:n_fpc_best))
    )
    y_pred_final_fpc_only <- predict(linear_fit_final_fpc_only, pc_score_final_test_combined_fpc_only)
    rmse_simul_fpca[rep_number,2] <- sqrt(
      mean((y_pred_final_fpc_only - train_test_split_normalized$response_test)^2)
    )
    print(rmse_simul_fpca[rep_number,2])

    pc_score_final_train_combined_spc_only <- data.frame(train_test_split_normalized$response_train, PC_score_final_train)
    colnames(pc_score_final_train_combined_spc_only) <- c(
      c("response"),
      paste0( rep("PC", n_spc_best), as.character(1:n_spc_best))
    )
    linear_fit_final_spc_only <- lm(response~., data = pc_score_final_train_combined_spc_only)
    pc_score_final_test_combined_spc_only <- data.frame(PC_score_final_test)
    colnames(pc_score_final_test_combined_spc_only) <- c(
      paste0( rep("PC", n_spc_best), as.character(1:n_spc_best))
    )
    y_pred_final_spc_only <- predict(linear_fit_final_spc_only, pc_score_final_test_combined_spc_only)
    rmse_simul_fpca[rep_number,3] <- sqrt(
      mean((y_pred_final_spc_only - train_test_split_normalized$response_test)^2)
    )
    print(rmse_simul_fpca[rep_number,3])




```


## Documenting the package and building

We finish by running commands that will document, build, and install the package.  It may also be a good idea to check the package from within this file.

```{r}
litr::document() # <-- use instead of devtools::document()
# devtools::build()
# devtools::install()
# devtools::check(document = FALSE)
```


