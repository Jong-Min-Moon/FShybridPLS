%
\documentclass[aoas]{imsart}

%% Packages
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\RequirePackage[authoryear]{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\RequirePackage{graphicx}


%custom packages
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{bbm}
% \usepackage{url} % not crucial - just used below for the URL 
% \usepackage{bibunits}
\usepackage{subcaption}
% \usepackage{adjustbox}
\usepackage{verbatim} % to comment out

%%%%


% \usepackage{multirow}
% \usepackage{hhline}
%\usepackage{enumitem}

% \usepackage{bm}
% \usepackage{multirow,multicol}
% \usepackage{verbatim}
% \usepackage{amsfonts,amssymb,amsthm,color,authblk}

% \definecolor{myblue}{RGB}{0, 0, 0}
% \newcommand{\blue}[1]{{\color{myblue}#1}}
\startlocaldefs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Uncomment next line to change            %%
%% the type of equation numbering           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\numberwithin{equation}{section}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Axiom, Claim, Corollary, Hypothesis, %%
%% Lemma, Theorem, Proposition              %%
%% use \theoremstyle{plain}                 %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{axiom}{Axiom}
\newtheorem{claim}[axiom]{Claim}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}%custom
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% For Assumption, Definition, Example,     %%
%% Notation, Property, Remark, Fact         %%
%% use \theoremstyle{definition}            %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{example}{Example}
\newtheorem*{fact}{Fact}
\newtheorem{remark}{Remark}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Please put your definitions here:        %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand{\jongmin}[1]{
	{ \color{teal} \textit{(JM:) #1}}
}


\endlocaldefs

\begin{document}
	
	\begin{frontmatter}
		\title{ Hybrid Partial Least Squares Regression with Multiple Functional and Scalar Predictors}
		%\title{A sample article title with some additional note\thanksref{t1}}
		\runtitle{Hybrid Partial Least Squares}
		%\thankstext{T1}{A sample additional note to the title.}
		
		\begin{aug}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%% Only one address is permitted per author. %%
			%% Only division, organization and e-mail is %%
			%% included in the address.                  %%
			%% Additional information can be included in %%
			%% the Acknowledgments section if necessary. %%
			%% ORCID can be inserted by command:         %%
			%% \orcid{0000-0000-0000-0000}               %%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\author[A]{\fnms{Jongmin }~\snm{Mun}  \ead[label=e1]{jongmin.mun@marshall.usc.edu}}
			\and
			\author[B]{\fnms{Jeong Hoon}~\snm{Jang}\ead[label=e2]{jejang@utmb.edu }}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%% Addresses                                %%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\address[A]{Data Sciences and Operations Department, Marshall School of Business, University of Southern California\printead[presep={,\ }]{e1}}
			
			\address[B]{Department of Biostatistics and Data Science, University of Texas Medical Branch\printead[presep={,\ }]{e2}}
		\end{aug}
		
		\begin{abstract}
			Regression of a scalar response on mixed functional- and scalar-valued predictors, such as medical imaging with auxiliary patient information, introduces the new challenge of handling cross-modality correlations. To address this, we propose a hybrid partial least squares (PLS) regression framework that integrates functional and scalar predictors within a unified Hilbert space.
			We then extend the classical nonlinear iterative PLS (NIPALS) algorithm to this hybrid Hilbert space by iteratively maximizing the empirical cross-covariance between the hybrid predictor and the response. As a result, our method identifies low-dimensional representations that capture both within- and between-modality variation, as well as the response-predictor correlation. The procedure is computationally efficient, requiring only the solution of linear systems at each step. We provide theoretical properties to justify our algorithm and demonstrate its effectiveness through simulations and an application to clinical outcome prediction using renal imaging and scalar covariates from the Emory University renal study.
		\end{abstract}
		
		\begin{keyword}
			\kwd{dimension reduction}
			\kwd{functional data analysis}
			\kwd{multiple data modalities}
			\kwd{multivariate data analysis}
			\kwd{multivariate functional data}
			\kwd{partial least square}
		\end{keyword}
		
	\end{frontmatter}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% Please use \tableofcontents for articles %%
	%% with 50 pages and more                   %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%\tableofcontents
	\section{Introduction} \label{sec: Introduction}
	%Functional data analysis (FDA) treats each observed process as a realization of a random function over a continuous domain, such as time or space. The smoothness and infinite-dimensional nature of functional data allow researchers to exploit rich structural features, including . This enables regression analysis for predicting clinical outcomes using complex functional predictors collected with advanced recording technologies, such as medical imaging data.
	
	Modern biomedical studies frequently collect diverse data types from each subject.
	As an illustrative example, the Emory University renal study \citep{changBayesianLatentClass2020, jangPrincipalComponentAnalysis2021}  records both multiple   renogram curves (functional data) and multiple renogram variables (scalar data)  for each kidney. 
	To effectively analyze such distinct yet related physiological signals, we construct a joint linear regression model incorporating both functional and scalar-valued i.i.d. covariates:
	\begin{equation}	\label{eq:hybrid_regression_model}
		Y_i
		= 
		\boldsymbol{\beta}^\top \mathbf{Z}_i
		+ 
		\sum 
		\limits_{k=1}^K 
		\int  \beta_k(t) X_{ik}(t) dt + \epsilon_i,
		\quad
		i = 1, \ldots, n,
		%= \langle \boldsymbol{\alpha}, \mathbf{Z} \rangle +  \langle \beta, X \rangle_{\mathcal{F}}  + \epsilon
	\end{equation}
	where $Y_i$ is a scalar response, $\mathbf{Z}_i=(Z_{i1}, \ldots, Z_{ip})^\top$ is a Euclidean vector covariate, $X_{i1}(t), \ldots, X_{iK}(t)$ are functional predictors that belong to $\mathbb{L}^2[0,1]$, and $\epsilon_i$ is observational noise.
	In other words, this is a scalar-on-hybrid regression where the hybrid covariates belong to $\bigl( \mathbb{L}^2[0,1] \bigr)^{  K} \times \mathbb{R}^p$.
	For notational convenience, we assume throughout this paper that the responses and predictors have been centered, allowing the intercept term to be ignored.
	%where $\boldsymbol{\alpha}=(\alpha_1, \ldots, \alpha_p)^\top \in \mathbb{R}^p$ and $\beta=(\beta^{(1)}, \ldots, \beta^{(K)}) \in \mathcal{F}$ are respectively the regression coefficient vector and function that characterize the effect of scalar and functional predictors on the outcome.
	%which is defined as the space of real-valued functions on $[0,1]$
	%	which are square-integrable with respect to the Lebesgue measure $dt$ on $[0,1]$, 
	%	and possess second  derivatives.
	
	%denote a $p$-dimensional multivariate scalar data in $\mathbb{R}^p$, assumed to be a random vector with finite first two moments, equipped with the Euclidean inner product and norm.
	
	%Infinite dimensional beta estimation problem (ill-posed problem)
	%Multicollinearity and high-dimensionality in scalar predictors are not handled.
	%Also, there may be high correlation between functional and scalar predictors.
	This hybrid model poses several challenges. 
	First, estimating the infinite-dimensional slope function $\beta_k(t)$ from a finite number of data
	is an ill-posed problem.% that requires dimension reduction or regularization based on structural assumptions, such as derivatives and covariance patterns.
	Second, we face the high-dimensionality of the predictors with $K+p$ dimensions.
	Third, strong correlations between functional and scalar predictors are common but often overlooked by separate modeling approaches.
	This paper addresses these three issues in a unified way by introducing a hybrid partial least squares (PLS) regression framework defined on a novel hybrid Hilbert space. Before summarizing the key contributions of this framework, we first review previous approaches and their limitations.
	
	\subsection{Previous works and limitations}
	To address the ill-posedness stemming from the infinite-dimensionality of the functional components, common remedies include basis expansion,
	using power-series  \citep{goldsmithPenalizedFunctionalRegression2011},
	B-splines \citep{cardotSplineEstimatorsFunctional2003, caiPredictionFunctionalLinear2006}, or 
	wavelets \citep{zhaoWaveletbasedLASSOFunctional2012},
	and structure-aware regularization, such as roughness penalties \jongmin{comment:let's add citations here}.
	
	For the high-dimensionality, a major solution is using derived inputs.
	A straightforward approach in this direction is principal component analysis (PCA) regression, which applies PCA separately to the functional- (by, for example, \citealt{happ_multivariate_2018}) and scalar predictors, and then performs a classical multivariate regression on the combined scores.
	PCA regression is well-studied for linear regression with functional predictors alone \citep{hallMethodologyConvergenceRates2007, reissFunctionalPrincipalComponent2007, febrero-bandeFunctionalPrincipalComponent2017}.
	However, since the PCA-derived inputs are not informed by the response variable,   they are not guaranteed to capture the core regression relationship with a small number of derived inputs.
	In terms of predictive power, partial least squares (PLS) regression is a powerful alternative. It iteratively constructs a set of orthogonal latent components from the predictors that have the maximal covariance with the response variable, and use the resulting scores for regression.
	%
	PLS for linear regression in the context of functional predictor alone was introduced by \citet{predaPLSRegressionStochastic2005} for the case of a single predictor. Motivated by regression on chemometric spectra, \citet{reissFunctionalPrincipalComponent2007}, \citet{aguileraUsingBasisExpansions2010}, and \citet{aguileraPenalizedVersionsFunctional2016} extended the framework by incorporating basis approximations and roughness penalties to promote smoothness. \citet{delaigleMethodologyTheoryPartial2012} provided the first thorough theoretical analysis, while \citet{saricamPartialLeastsquaresEstimation2022} proposed a computationally efficient procedure based on Golub-Kahan bidiagonalization. For a comprehensive review, see \citet{febrero-bandeFunctionalPrincipalComponent2017}. More recently, \citet{beyaztasRobustFunctionalPartial2022} extended the framework to accommodate multiple functional predictors.
	
	However,  these existing approaches overlook the potentially strong correlations between functional and scalar components, which can lead to multicollinearity and suboptimal predictive performance. Multimodal correlations have mostly been addressed within an unsupervised learning framework. For instance, \citet{kolar_graph_2014} studied the estimation of joint undirected graphical models for  functional and vector data, while \citet{gengJointNonparametricPrecision2020} considered joint precision matrix estimation for brain measurements and confounding scalar covariate. \citet{jangPrincipalComponentAnalysis2021} proposed a joint PCA method that accounts for correlations between functional and scalar data. However, the resulting components are still not informed by the response and may fail to capture  correlation with the outcome.
	
	
	\subsection{Our contributions}
	To address the gaps mentioned above, we unify the remedies proposed for these three issues: basis expansion, partial least squares, and accounting for correlations between the functional and scalar components.
	We propose a hybrid PLS regression framework that integrates functional and vector predictors in a principled and coherent manner. To extract predictive structure from these jointly observed and potentially correlated data types, we define a Hilbert space that treats the tuple of functional and vector components as a single hybrid object, equipped with a suitable inner product. The hybrid PLS direction is then obtained by iteratively maximizing the empirical covariance with the response, subject to a unit-norm constraint in this Hilbert space. Our framework is readily applicable to dense and irregular functional data and supports regularization techniques to prevent overfitting and reduce variance. We also provide the mathematical properties that justify our algorithm.
	
	
	\section{Background on partial least squares and its extension to hybrid predictors}  
	\jongmin{comment: I replaced the formal presentation of the naive algorithm with an   the scalar version and a brief outline of the pointwise hybrid  extension strategy. The main reason for this revision is that the hybrid inner product is introduced specifically to address the extension challenge, so it is more natural to present it after listing the challenges. Previously, the naive algorithm section relied on the hybrid inner product for normalization.}
	For intuition, let us return to the high-dimensional Euclidean predictor setting $
	Y_i =  \boldsymbol{\beta}^\top \mathbf{Z}_i + \epsilon_i$.
	A common way to address ill-posedness and correlation is to approximate the high-dimensional vector $\mathbf{Z}_i$ using a low-dimensional vector $
	\bigr(
	\widehat{\rho}_1^{[1]}, \ldots,   \widehat{\rho}_1^{[L]}  
	\bigr)^\top \in \mathbb{R}^L.
	$ To retain the regression relationship, the $l$-th PLS direction $\hat{\boldsymbol{\xi}}_l$ solves:
	\begin{equation}\label{scalar_PLS_sample}
		\max_{ \mathbf{ h } } 
		\widehat{\operatorname{Cov}}^2
		\bigl(
		\{
		\langle 
		\mathbf{ h }, \mathbf{Z}_i
		\rangle
		, Y_i
		\}_{i=1}^n
		\bigr)~\text{s.t.}~\| \mathbf{ h } \|_2 = 1,~\mathbf{ h }^\top  \widehat{\operatorname{Cov}}^2
		\bigl(
		\{
		\mathbf{Z}_i
		\}_{i=1}^n
		\bigr) \hat{\boldsymbol{\xi}}_j = 0, \quad j = 1, \ldots, l-1,
	\end{equation}
	where the two $\widehat{\operatorname{Cov}}^2$ denote sample cross-covariance and sample covariance, respectively. A standard algorithm for solving this optimization problem, called nonlinear iterative partial least squares (NIPALS) is presented in Algorithm \ref{alg:scalar_pls}.
	\begin{algorithm} 
		\caption{Scalar partial least squares regression}\label{alg:scalar_pls}
		\begin{algorithmic}[1]
			\State Standardize each $\mathbf{Z}_1, \ldots, \mathbf{Z}_n$ so that each feature have mean zero and variance one. Standardize $Y_1, \ldots, Y_n$.
			\For{$l = 1, 2, \ldots, L$}
			\\
			\textbf{PLS direction and score estimation:}
			\State $\widehat{\boldsymbol{\xi}}^{[l]} \leftarrow
			\arg \max_{  \boldsymbol{\alpha}   } 
			\widehat{\operatorname{Cov}}^2
			\bigl(
			\{
			\langle 
			\boldsymbol{\alpha}, \mathbf{Z}^{[l]}_i
			\rangle
			, Y^{[l]}_i
			\}_{i=1}^n
			\bigr)~\text{s.t.}~\| \boldsymbol{\alpha} \|_2 = 1
			$
			\Comment{PLS direction}\label{alg:step:scalar_pls_direction}
			\State 
			$
			\widehat{\rho}_i^{[l]} \leftarrow \langle \hat{\boldsymbol{\xi}}^{[l]},   \mathbf{Z}^{[l]}_i \rangle, 
			~i=1,  \ldots, n
			$ \Comment{PLS score}
			\\
			\textbf{Residualization:}
			\State $\nu^{[l]} 
			\leftarrow
			\frac{
				\sum_{i=1}^n Y_i^{[l]} \widehat{\rho}_{i}^{[l]}}{
				\sum_{i=1}^n \widehat{\rho}_{i}^{[l]2}
			}$ \Comment{Least squares estimate} 
			%
			\State $ Y_i^{[l+1]} \leftarrow Y_i^{[l]} - \nu^{[l]}\widehat{\rho}_i^{[l]}~i=1,  \ldots, n$
			%
			\State $ \widehat{\boldsymbol{\delta}}^{[l]}  \leftarrow \frac{1}{\sum_{i=1}^n \widehat{\rho}_{i}^{[l]2}}\sum_{i=1}^n  \widehat{\rho}_{i}^{[l]}\mathbf{Z}_i^{[l]} $ \Comment{Least squares estimate} 
			\label{alg:step:scalar_pls_residualization}
			\State $\mathbf{Z}_i^{[l+1]}  \leftarrow \mathbf{Z}_i^{[l]} -   \widehat{\rho}_{i}^{[l]}  \widehat{\boldsymbol{\delta}}^{[l]} $
			\EndFor
			\textbf{Regression coefficient estimation:}
			\State
			$\widehat{\boldsymbol{\iota}}^{[1]} \leftarrow \widehat{\boldsymbol{\xi}}^{[1]}$
			\For{$l =  2, \ldots, L$}
			\State $\widehat{\boldsymbol{\iota}}^{[l]} \leftarrow \widehat{ \boldsymbol{\xi} }^{[l]} - \sum_{u=1}^{l-1} \langle \widehat{\boldsymbol{\delta}}^{[u]}, \widehat{\boldsymbol{\xi}}^{[l]} \rangle 
			\widehat{\boldsymbol{\iota}}^{[u]}$  
			\EndFor
			$	\widehat{\boldsymbol{\beta}} \leftarrow \sum \limits_{l=1}^L\widehat{\nu}^{[l]} \widehat{\boldsymbol{\iota}}^{[l]}$ \Comment{regression coefficient estimate}
			\State \textbf{Output:} the  regression coefficient estimate
		\end{algorithmic}
	\end{algorithm}
	
	To extend Algorithm \ref{alg:scalar_pls} to the hybrid regression model \eqref{eq:hybrid_regression_model}, we must compute hybrid objects in both the PLS direction estimation and predictor residualization steps (lines \ref{alg:step:scalar_pls_direction} and \ref{alg:step:scalar_pls_residualization}). These computations cannot be separated into functional and scalar parts, as their correlation must be addressed.  
	A naive approach is to add loops over the observed evaluation points $t$ to lines \ref{alg:step:scalar_pls_direction} and \ref{alg:step:scalar_pls_residualization}, treating  
	\[
	\bigl( X_{i1}(t), \ldots, X_{iK}(t), Z_{i1}, \ldots, Z_{ip} \bigr) \in \mathbb{R}^{K+P}
	\]  
	as a Euclidean predictor, applying steps \ref{alg:step:scalar_pls_direction} and \ref{alg:step:scalar_pls_residualization}, and aggregating the results. This pointwise method, however, is computationally prohibitive for densely observed functional predictors, infeasible for irregular data, and prone to variability across the domain, resulting in overfitting and unstable predictions.  
	The main challenge of extension to the hybrid setting lies in the following points. First, 
	independent variables consist of multiple highly structured images and scalar predictors. 
	Second, our sample size is small compared to the dimension and number of functional and scalar predictors.
	Third,  existing partial least squares (PLS) methods can only accommodate (i) univariate or multivariate functional predictors without any scalar predictors \citep{predaPLSRegressionStochastic2005, delaigleMethodologyTheoryPartial2012, febrero-bandeFunctionalPrincipalComponent2017, Beyaztas2020}; or (ii) a univariate functional predictor with other scalar predictors \citep{Wang2018}.
	The next section proposes a new extension framework that addresses these issues.
	
	
	
	
	
	
	
	\section{Proposed PLS Algorithm} \label{section:main:our_algorithm}
	Our new framework builds on the fact that all computations in Algorithm \ref{alg:scalar_pls} are arithmetic operations in an inner product space. We therefore formally define the Hilbert space of hybrid random predictors along with its corresponding complete inner product, and leverage them to extend Algorithm \ref{alg:scalar_pls}.
	\jongmin{comment: I split the definition into three parts, adding completeness, separability, and the sigma-field.}
	\begin{definition}[Hybrid space]\label{def:hilbert_space}
		Let $\mathbb{H}$ be a product space defined as the Cartesian product of $K$ copies of the space of square-integrable functions on $[0,1]$ and the $p$-dimensional Euclidean space:
		$$
		\mathbb{H} := (\mathbb{L}^2[0,1])^K \times \mathbb{R}^p.
		$$
		An element $h \in \mathbb{H}$ is an ordered tuple $h = (f_1, \dots, f_K, \mathbf{u})$, where $f_k \in \mathbb{L}^2[0,1]$ for $k=1, \dots, K$ and $\mathbf{u} \in \mathbb{R}^p$. This space is equipped with element-wise vector addition and scalar multiplication.
		We define an inner product on $\mathbb{H}$ for any two elements $h_1 = (f_1, \dots, f_K, \mathbf{u})$ and $h_2 = (g_1, \dots, g_K, \mathbf{v})$ as:
		\begin{equation}\label{eq: hybrid inner product}
			\langle h_1, h_2 \rangle_{\mathbb{H}} := \sum_{k=1}^K \int_0^1 f_k(t) g_k(t) \, dt + \omega \mathbf{u}^\top \mathbf{v},
		\end{equation}
		where $\omega$ is a positive constant ($\omega > 0$).
		The inner product induces a norm $\Vert \cdot \Vert_{\mathbb{H}}$ on the space, defined as $\Vert h \Vert_{\mathbb{H}} := \langle h, h \rangle_{\mathbb{H}}^{1/2}$, and a corresponding metric $d(h_1, h_2) = \Vert h_1 - h_2 \Vert_{\mathbb{H}}$.
	\end{definition}
	In \eqref{eq: hybrid inner product}, 
	$\omega$ is a positive weight that needs to be pre-specified or estimated. It is mainly used to take into account heterogeneity between functional and scalar parts in terms of measurement scale and/or amount of variation (see Section \ref{subsec: Data Preprocessing}).
	Without loss of generality and for the clarity of illustration, all the following theoretical results will be derived for $\omega=1$. The results remain valid for any positive weights. 
	\begin{lemma}[Hybrid Hilbert space]\label{lemma:hybrid_hilbert_space}
		The hybrid space $\mathbb{H}$ is a separable Hilbert space.
	\end{lemma} 
	Proof of Lemma \ref{lemma:hybrid_hilbert_space} is provided in Appendix \ref{section:proof:lemma:hybrid_hilbert_space}.
	
	\begin{definition}[Hybrid Predictor]\label{def:hybrid_predictor}
		For the Hilbert space  $\mathbb{H}$ 
		defined in Definition \ref{def:hilbert_space},
		The Borel $\sigma$-field on $\mathbb{H}$, denoted $\mathfrak{B}(\mathbb{H})$, is the smallest $\sigma$-field containing the class $\mathfrak{M}$ of all sets of the form
		$
		\{q \in \mathbb{H} \mid \langle q, h \rangle \in O\},
		$
		for any $h \in \mathbb{H}$ and any open subset $O \subseteq \mathbb{R}$ (details can be found in Theorem 7.1.1 of \citealt{hsingTheoreticalFoundationsFunctional2015}).
		A hybrid predictor $ W_i = (X_{i1}(t), \ldots, X_{iK}(t), \mathbf{Z}_i)$ is a measurable mapping from a probability space $(\Omega, \mathfrak{F}, P)$ into   $
		\bigl(
		\mathbb{H}, \mathfrak{B}(\mathbb{H})
		\bigr)
		$. 
	\end{definition}
	Then the joint regression model \eqref{eq:hybrid_regression_model}  can be concisely written as
	\begin{equation}		\label{eq: Hybrid functional model}
		Y_i = \langle \beta, W_i \rangle_{\mathbb{H}} + \epsilon_i,~\text{where}~\beta := \bigl( \beta_1(t), \ldots, \beta_K(t), \boldsymbol{\beta} \bigr)\in \mathbb{H}.
	\end{equation}
	\begin{remark}
		Our method is applicable to settings where each functional predictor belongs to a different Hilbert space, possibly defined over a distinct compact domain in $\mathbb{R}^d$, for arbitrary $d$, and observed at different time points. However, for notational simplicity, our discussion assumes a common Hilbert space over domain $[0,1]$ for all functional predictors.
	\end{remark}  
	
	Our approach provides an efficient and robust means of producing PLS components and scores in the presence of multiple dense and/or irregular functional predictors and scalar predictors. It also incorporates a regularization scheme that enables the algorithm to borrow strength and exploit structural relationships within and between the functions to avoid overfitting of the PLS components and to improve the generalizability and interpretability of the predictive model.
	Each iteration of our approach consists of two subroutines: regularized estimation of smoothed PLS components and orthogonalization, detailed in Sections~\ref{section:sub:compute_PLS_component} and~\ref{section:sub:residualization}, respectively. After a suitable number of iterations, the hybrid regression coefficient is estimated, as described in Section~\ref{section:sub:regression_coeff}. For notational simplicity, we omit the iteration index $l$ in the following discussion, with the understanding that the subroutines apply to any iteration. The complete algorithm is summarized in Algorithm~\ref{alg:hybrid_pls}.
	
	\subsection{Preliminary step 1: finite-basis approximation}\label{sec: finite basis approximation}	
	Let ${b_m(t)}$ be a twice-differentiable basis of $\mathbb{L}^2([0,1])$ whose second derivatives are also linearly independent, for example, cubic B-splines, the Fourier basis, or an orthonormal polynomial basis of degree greater than three. Using this basis, 
	the $j$th functional predictor, 
	regression coefficient, PLS component direction, and orthogonalization regression coefficient (with iteration indices suppressed) are represented as follows:
	\begin{equation*}
		X_{ij}(t) = \sum_{m=1}^\infty \theta_{ijm} \, b_m(t),~
		\beta_j(t) = \sum_{m=1}^\infty \eta_{jm} \, b_m(t),~
		%%%%%%%%%%%%%%%%%%%%%%
		\xi_j(t)
		=
		\sum_{m=1}^\infty
		\gamma_{jm}
		b_m(t),
		~
		\delta_j(t)
		=
		\sum_{m=1}^\infty
		\pi_{jm}
		b_m(t)		
	\end{equation*}	
	In practice, the full set of coefficients can not be obtained  with finite sample size, as functional data are measured on a finite grid. Thus, we truncate the expansion at $M$ terms.
	We choose a moderately large $M$ (e.g., 15 or 20) to capture functional variation without fine-tuning, as smoothness is handled via penalization (see Section~\ref{sec: Regularization of PLS Components}).
	The truncated expansions of the predictor and coefficient are denoted as
	\begin{equation*}
		\widetilde{X}_{ij}(t) := \sum_{m=1}^M \theta_{ijm} \, b_m(t), \quad
		\tilde{\beta}_j(t) := \sum_{m=1}^M \eta_{jm} \, b_m(t),
	\end{equation*}
	and our suggest method restricts each PLS component direction and orthogonalization regression coefficient to admit the following expansion:
	\begin{equation}\label{def:truncated_zeta_and_delta}
		\xi_j(t) = \sum_{m=1}^M \gamma_{jm} \, b_m(t),
		\quad
		\delta_j(t)
		=
		\sum_{m=1}^M
		\pi_{jm}
		b_m(t)		
	\end{equation}
	This implies that all computations in this paper are carried out entirely within the subspace
	\begin{equation}\label{def:truncated_hilbert_space}
		\widetilde{ \mathbb{H} } := \operatorname{span}\bigl(b_1(t), \ldots, b_M(t)\bigr)^K \times \mathbb{R}^p \subset \mathbb{H}.
	\end{equation}
	The $i$th hybrid predictor, projected on $\widetilde{ \mathbb{H} }$, is represented by the tuple
	\begin{equation*}
		\widetilde{W}_i := 
		(
		\widetilde{X}_{i1}, \ldots, \widetilde{X}_{iK}, \boldsymbol{Z}_i
		).
	\end{equation*}
	Let $\boldsymbol{\theta}_{ij}$, $\boldsymbol{\eta}_j$,  $\boldsymbol{\gamma}_j$, and
	$\boldsymbol{\pi}_j$
	denote the $M$-dimensional vectors of coefficients:
	\begin{equation}\label{def:coef_vector_form}
		\boldsymbol{\theta}_{ij} := (\theta_{ij1}, \ldots, \theta_{ijM})^\top,
		~
		\boldsymbol{\eta}_{j} := (\eta_{j1}, \ldots, \eta_{jM})^\top ,~
		\boldsymbol{\gamma}_{j} := (\gamma_{j1}, \ldots, \gamma_{jM})^\top,~
		\boldsymbol{\pi}_{j} := (\pi_{j1}, \ldots, \pi_{jM})^\top
	\end{equation}
	For the predictors, we stack the coefficient vectors across observations into the matrix
	\begin{equation}\label{def:theta}
		\Theta_j := (\boldsymbol{\theta}_{1j}, \ldots, \boldsymbol{\theta}_{nj})^\top  \in \mathbb{R}^{n \times M},
	\end{equation}
	and construct the full design matrix 
	\begin{equation}\label{def:full_theta}
		\Theta := (\Theta_1, \ldots, \Theta_K, \mathbf{Z}) \in \mathbb{R}^{n \times (MK + p)}.
	\end{equation}
	Let us denote the response vector as $\mathbf{y} := (y_1, \ldots, y_n)^\top$.
	Let $B, B^{\prime\prime} \in \mathbb{R}^{M \times M}$ denote the Gram matrices of the basis functions and their second derivatives, with entries
	\begin{equation}\label{def:B_matrix}
		B_{m, m'} := \int_0^1 b_m(t)\, b_{m'}(t)\, dt, \quad
		B^{\prime\prime}_{m, m'} := \int_0^1 b_m''(t)\, b_{m'}''(t)\, dt,
	\end{equation}
	for $m, m' = 1, \ldots, M$. We then define the block-diagonal matrices
	\begin{equation}\label{def:gram_block}
		\mathbb{B} := \operatorname{blkdiag}(B, \ldots, B, I_p), \quad
		\mathbb{B}^{\prime\prime} := \operatorname{blkdiag}(B^{\prime\prime}, \ldots, B^{\prime\prime}, I_p),
	\end{equation}
	
	Then the full data for the hybrid PLS problem at the $l$-th iteration can be represented by the tuple
	\begin{equation}\label{def:problem_instance}
		(\mathbb{B}, \mathbb{B}^{\prime\prime}, \Theta, \mathbf{y}) \in 
		\mathbb{R}^{(MK+p) \times (MK+p)} \times 
		\mathbb{R}^{(MK+p) \times (MK+p)} \times 
		\mathbb{R}^{n \times (MK+p)} \times 
		\mathbb{R}^n,
	\end{equation}
	with the index $l$ omitted for brevity.
	
	
	\begin{remark}
		While different bases could be used for each functional predictor, we adopt a common basis for simplicity. The definitions of $\mathbb{B}$ and $\mathbb{B}^{\prime\prime}$ remain general enough to accommodate distinct bases if needed.
	\end{remark}
	
	%Additionally, it serves as a regularization mechanism, borrowing strength across components and helping to prevent overfitting. 	
	
	
	
	\subsection{Preliminary step 2 : data preprocessing} \label{subsec: Data Preprocessing}
	Functional and scalar elements of the hybrid predictors often have incompatible units and/or exhibit different amounts of variation. This can be problematic for our PLS framework which is not scale invariant as: i) each predictor has different chance of contributing to the predictor/response structure; and ii) a predictor with high correlation to $Y$ but relatively low variance may be overlooked. 
	
	To obtain PLS components that have a meaningful interpretation, we standardize the predictor data via the following steps. The first step is to account for discrepancies \textit{within} respective functional and scalar parts, if needed. If the functional parts $\widetilde{X}_{i1}(t), \ldots, \widetilde{X}_{iK}(t)$ are measured in different units or have quite different domains, one can standardize them to have mean zero and integrated variance of one. If multivariate scalar predictors $\mathbf{Z}_i=(Z_{i1}, \ldots, Z_{ip})^\top$ exhibit different amounts of variation, one can standardize them to have mean zero and unit variance. The second step is to eliminate the discrepancies \textit{between} functional and scalar parts. To accomplish this aim, we choose an appropriate weight $\omega$ in the hybrid inner product \eqref{eq: hybrid inner product} that ensures functional and vector parts have comparable variance. A sensible data-driven approach to choosing an appropriate weight is to set
	\begin{equation*}
		\omega = \frac{\sum_{i=1}^n \sum_{k=1}^K \Vert \widetilde{X}_i \Vert^2_{\mathbb{L}^2[0,1]}}{\sum_{i=1}^n \Vert \mathbf{Z}_i \Vert_2^2},
		\label{eq: weight}
	\end{equation*}
	In practice, 
	instead of using inner product weighted by $\omega^{1/2}$, one can implement 
	this weighting scheme   by formulating the hybrid object as $\widetilde{W}_i = \bigl(
	\widetilde{X}_{i1}(t), \ldots, \widetilde{X}_{iK}(t),  \omega^{1/2} \mathbf{Z}_i
	\bigr)$, whose vector part has been scaled by a factor of $\omega^{1/2}$.
	
	\subsection{Iterative steps}\label{section:sub:iterative}
	The iterative process presented here yields an orthonormal hybrid basis that effectively captures the predictor-response relationships. It proceeds through two intermediate steps: the estimation of the PLS component direction (Section \ref{section:sub:compute_PLS_component}) and residualization (Section \ref{section:sub:residualization}). 
	The proofs are deferred to Appendix \ref{section:proof:section:sub:iterative}.
	The properties of the resulting estimates are introduced in Section \ref{section:main:properties_of_our_method}.
	\subsubsection{Iterative step 1: regularized estimation of  PLS component direction}  \label{section:sub:compute_PLS_component} 
	We begin by formally introducing the core optimization problem pertinent to the PLS direction estimation, which is formulated as a generalized Rayleigh quotient (Proposition \ref{proposition:eigen_noregul}). Building upon this foundational concept, we present our regularized PLS component direction estimation step that promotes smoothness (Proposition \ref{proposition:eigen_regul}). Furthermore,  we detail an efficient computational scheme (Proposition \ref{proposition:linear_regul}).
	
	\medskip
	\noindent
	\textit{Core optimization problem.}\label{sec: pls component computation intermediate}~
	We   present the core optimization problem that directly estimates the hybrid PLS component direction. It fully leverages the continuous nature of the functional components and the function-scalar hybrid structure.
	We describe the strategy at the $l$-th iteration. %For simplicity, we omit the iteration index and assume the observations have been residualized from previous steps (see Section~\ref{section:sub:residualization} for details). 
	The  PLS component direction is estimated by the unit-norm direction $\xi^{[l]} \in \widetilde{\mathbb{H}}$ that maximizes the squared empirical covariance, which quantifies the linear dependence between the PLS scores $\langle \widetilde{W}_1, \xi \rangle_{\mathbb{H}}, \ldots, \langle \widetilde{W}_n, \xi \rangle_{\mathbb{H}}$ and the responses $y_1, \ldots, y_n$, defined as
	
	\begin{equation}\label{def:squared_empirical_cov}
		\widehat{\textnormal{Cov}} 
		( \langle 
		\widetilde{W},
		\xi \rangle_{\mathbb{H}}, Y )
		:=
		\frac{1}{n} 
		\sum_{i=1}^n
		y_i
		\langle \widetilde{W}_i, \xi \rangle_{\mathbb{H}}
		.
	\end{equation}
	We denote the estimated PLS component direction as
	\begin{equation}\label{def:maximizer_squared_empirical_cov}
		\hat{\xi} 
		:= \arg \max_{\xi \in \widetilde{\mathbb{H}}
		}~\widehat{\textnormal{Cov}}^2 
		\bigl(
		\langle \widetilde{W}, \xi \rangle_{\mathbb{H}}, Y 
		\bigr)~s.t.~\| \xi  \|_\mathbb{H} = 1.
	\end{equation}
	%
	Here, $\hat{\xi} \in \widetilde{\mathbb{H}}$ is an ordered pair  expanded as:
	\begin{equation*}
		\hat{\xi}	= 
		\bigl(
		\hat{\xi}_1(t), \ldots, \hat{\xi}_K(t), \hat{\boldsymbol{\zeta}}
		\bigr)
		=
		\biggl(
		\sum_{m=1}^M \hat{\gamma}_{1m} \, b_m(t),
		\ldots, 
		\sum_{m=1}^M \hat{\gamma}_{Km} \, b_m(t), \hat{\boldsymbol{\zeta}}
		\biggr)
		,
	\end{equation*}
	where  $\hat{\boldsymbol{\zeta}}$ is the scalar part. Obtaining these coefficients is equivalent to solving the maximization problem   \eqref{def:maximizer_squared_empirical_cov}. The following proposition formulates this coefficients obtaining procedure as a generalized Rayleigh quotient:
	%
	\begin{proposition}\label{proposition:eigen_noregul}
		Let
		$
		(\mathbb{B}, \mathbb{B}^{\prime \prime}, \Theta, \mathbf{y})
		$
		denote the observed data defined in \eqref{def:problem_instance}.
		At the $l$-th iteration of the PLS algorithm, 
		the coefficients of the squared covariance  maximizer defined in \eqref{def:maximizer_squared_empirical_cov}, 
		is obtained as
		\begin{equation}	\label{eq: Unregularized generalized rayleigh quotient equation}
			\left(
			\hat{\gamma}_{11}, \ldots, \hat{\gamma}_{1M}
			, \ldots,
			\hat{\gamma}_{K1}, \ldots, \hat{\gamma}_{KM}
			, \hat{\boldsymbol{\zeta}}^\top
			\right)^\top
			=
			\arg \max_{\boldsymbol{\xi} \in \mathbb{R}^{MK+p}}   \boldsymbol{\xi}^\top V \boldsymbol{\xi}
			\quad \text{subject to} \quad \boldsymbol{\xi}^\top \mathbb{B} \boldsymbol{\xi} = 1.
		\end{equation}
		where
		\begin{equation}\label{def:V_matrix}
			V := \frac{1}{n^2} (\mathbb{B} \Theta^\top \mathbf{y})(\mathbb{B} \Theta^\top \mathbf{y})^\top \in \mathbb{R}^{(MK+p) \times (MK+p)}.
		\end{equation}
	\end{proposition}
	The proof of Proposition \ref{proposition:eigen_noregul}
	is provided in Appendix \ref{section:proof:proposition:linear_regul_system_unregularized}.
	
	\medskip
	\noindent
	\textit{Proposed regularized estimation procedure.}~ \label{sec: Regularization of PLS Components}
	Although Proposition \ref{proposition:eigen_noregul} offers an efficient way to estimate the PLS component direction $\hat{\xi}$, its functional components, $\hat{\xi}_1, \ldots, \hat{\xi}_K$, may not be smooth. \jongmin{added: Since  the final regression coefficient $\hat{\beta}$ is constructed as a linear combination of these directions (Section \ref{section:sub:regression_coeff}),
		this can lead to a non-smooth $\hat{\beta}$} which complicates interpretation and can lead to overfitting and unstable predictions. 
	To address this, we propose a regularized extension that balances predictive performance with smoothness. Specifically, we penalize the roughness of each $\hat{\xi}_j$ using its integrated squared second derivative 
	\begin{equation}\label{def:roughness_penalty}
		\operatorname{PEN}(\hat{\xi}_j) := \int_0^1 \bigl\{ \hat{\xi}_j^{\prime\prime}(t) \bigr\}^2 dt
		.
	\end{equation}
	
	Instead of solely maximizing the squared empirical covariance
	$\widehat{\textnormal{Cov}}^2 
	\bigl(
	\langle \widetilde{W}, \xi \rangle_\mathbb{H}, Y 
	\bigr)
	$, 
	we incorporate this roughness penalty to simultaneously control the complexity of the estimated functional components.
	One possible approach is to extend the smoothed functional PCA framework of \citet{Rice1991} by modifying the objective in \eqref{def:maximizer_squared_empirical_cov} to
	\begin{equation*}
		\widehat{\textnormal{Cov}}^2 
		\bigl(
		\langle \widetilde{W}, \xi \rangle_{\mathbb{H}}, Y 
		\bigr)
		- \sum \limits_{j=1}^K \lambda_j 
		\operatorname{PEN}({\xi}_j)
		,
	\end{equation*}
	where the smoothing parameters $\{\lambda_j\}_{j=1}^K$ control the trade-off between maximizing covariance and penalizing roughness. 
	However, this approach  \citet{Rice1991} assumes that the functional predictors admit an orthogonal expansion in the $\mathbb{L}^2$ sense.
	
	
	To avoid the orthogonal basis assumption of  \citet{Rice1991}, we adopt the strategy of \citet{Silverman1996}, which replaces the standard orthonormality constraint with a weaker one based on a modified inner product that incorporates roughness. Accordingly, our estimation procedure at the $l$-th iteration, iteration index omitted and assuming the observations have been residualized in previous steps, solves the following optimization problem:
	\begin{equation}\label{def:maximizer_squared_empirical_cov_reg}
		\hat{\xi} 
		:= \arg \max_{\xi \in \widetilde{\mathbb{H}}
		}~\widehat{\textnormal{Cov}}^2 
		\bigl(
		\langle \widetilde{W}, \xi \rangle_{\mathbb{H}}, Y 
		\bigr)~s.t.~\| \xi  \|_\mathbb{H} +  \sum \limits_{j=1}^K \lambda_j 
		\operatorname{PEN}(\xi_K) = 1.
	\end{equation}
	Here, $\hat{\xi} \in \widetilde{\mathbb{H}}$ is an ordered pair  expanded as:
	\begin{equation*}
		\hat{\xi}	= 
		\bigl(
		\hat{\xi}_1(t), \ldots, \hat{\xi}_K(t), \hat{\boldsymbol{\zeta}}
		\bigr)
		=
		\biggl(
		\sum_{m=1}^M \hat{\gamma}_{1m} \, b_m(t),
		\ldots, 
		\sum_{m=1}^M \hat{\gamma}_{Km} \, b_m(t), \hat{\boldsymbol{\zeta}}
		\biggr)
		,
	\end{equation*}
	where  $\hat{\boldsymbol{\zeta}}$ is the scalar part.
	This formulation maximizes the squared covariance over a class of smooth functions.
	Obtaining these coefficients is equivalent to solving the maximization problem   \eqref{def:maximizer_squared_empirical_cov_reg}. The following proposition formulates this coefficients obtaining procedure as a generalized Rayleigh quotient:
	\begin{proposition}[Regularized estimation of PLS component direction]\label{proposition:eigen_regul}
		Let
		$
		(\mathbb{B}, \mathbb{B}^{\prime \prime}, \Theta, \mathbf{y})
		$
		denote the  data given at the $l$-th iteration, as defined in \eqref{def:problem_instance}.
		Recall from \eqref{def:V_matrix} that $
		V = n^{-2} (\mathbb{B} \Theta^\top \mathbf{y})(\mathbb{B} \Theta^\top \mathbf{y})^\top$ .
		Let 
		$\Lambda \in \mathbb{R}^{(MK+p) \times (MK+p)}$ be defined as:
		\begin{equation}\label{def:Lambda}
			\Lambda := \operatorname{blkdiag}(\lambda_1 I_M, \ldots, \lambda_K I_M, 0_{p \times p}),
			~\text{where}~\lambda_1, \ldots, \lambda_K \geq 0.
		\end{equation}
		Here, $0_{p \times p}$ denotes the $p \times p$ zero matrix.
		The coefficients of the squared covariance  maximizer defined in \eqref{def:maximizer_squared_empirical_cov_reg}, 
		are obtained as
		\begin{equation}	\label{eq: Regularized generalized rayleigh quotient equation}
			\left(
			\hat{\gamma}_{11}, \ldots, \hat{\gamma}_{1M}
			, \ldots,
			\hat{\gamma}_{K1}, \ldots, \hat{\gamma}_{KM}
			, \hat{\boldsymbol{\zeta}}^\top
			\right)^\top
			\hspace{-.7em}
			=
			\arg 
			\hspace{-.6em}
			\max_{\boldsymbol{\xi} \in \mathbb{R}^{MK+p}}   \boldsymbol{\xi}^\top V \boldsymbol{\xi}
			~~\text{s.t.}~~
			%
			% constraint
			\boldsymbol{\xi}^\top 
			\hspace{-.2em}
			(\mathbb{B} + \Lambda \mathbb{B}^{\prime \prime}) \boldsymbol{\xi} = 1.
		\end{equation}
		
	\end{proposition}
	The proof of Proposition \ref{proposition:eigen_regul} is provided in Appendix  \ref{section:proof:proposition:eigen_regul}.
	%orthonormality preview
	The constraint $	\boldsymbol{\xi}^\top (\mathbb{B} + \Lambda \mathbb{B}^{\prime \prime}) \boldsymbol{\xi} = 1$
	enforces the orthonormality of the estimated PLS component directions with respect to a modified inner product (see Section \ref{section:sub:geom} for details).
	The smoothing parameter $\lambda_k$  balances goodness of fit and smoothness in $\hat{\xi}_j$. Smaller $\lambda_k$ yields components that better fit the data but risks overfitting; setting $\lambda_k = 0$ recovers the unregularized solution in Proposition~\ref{proposition:eigen_noregul}. Larger $\lambda_k$ enforces greater smoothness, and in the limit $\lambda_k \to \infty$, $\hat{\xi}_j(t)$ approaches a linear form $a + bt$. In practice, both $\{\lambda_k\}$ and the number of components $L$ can be selected via cross-validation using a predictive criterion such as mean squared error.
	
	
	\medskip
	\noindent{\textit{Computation.}}~The generalized eigenproblem presented in Proposition~\ref{proposition:eigen_regul}  may be computationally unstable in practice. However, by leveraging the rank-one structure of the matrix 
	$V$
	Proposition \ref{proposition:linear_regul} derives a closed-form solution that requires only the solution of linear systems.
	\begin{proposition}[Closed-form solution]  
		\label{proposition:linear_regul}  
		Consider the optimization problem described in Proposition~\ref{proposition:eigen_regul}. Define the following quantities, which depend on the observed data but are not decision variables:  
		\[
		\mathbf{u}_j := B \Theta_j^\top \mathbf{y} \in \mathbb{R}^M \quad \text{for } j = 1, \ldots, K, \quad \text{and} \quad \mathbf{v} := \mathbf{Z}^\top \mathbf{y} \in \mathbb{R}^p.
		\]  
		Let
		\[
		q := \sum_{j=1}^K \mathbf{u}_j^\top (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j + \mathbf{v}^\top \mathbf{v}.
		\]  
		
		Then the unique (up to sign) solution to the regularized maximization problem is given in closed form by  
		\[
		\hat{\boldsymbol{\gamma}}_j = \frac{1}{\sqrt{q}} (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j \quad \text{for } j = 1, \ldots, K, \quad \text{and} \quad
		\hat{\boldsymbol{\zeta}} = \frac{1}{\sqrt{q}} \mathbf{v}.
		\]  
	\end{proposition}
	The proof of Proposition~\ref{proposition:linear_regul} is provided in Appendix~\ref{section:proof:proposition:linear_regul}. The expressions above involve solving linear systems for the functional and scalar components separately, followed by normalization by a common factor. Although the unnormalized coefficients are obtained independently, the normalization step couples the functional and scalar parts, allowing them to influence one another. This coupling enables the procedure to capture the correlation between the functional and scalar components of the PLS direction.
	
	
	
	
	
	\subsubsection{Iterative step 2: residualization via hybrid-on-scalar regression} \label{section:sub:residualization}
	The $l$-th iteration's second step involves residualization of both predictors and responses. We first compute the individual PLS score:  
	\begin{equation}\label{def:plsscore}
		\hat{\rho}^{[l]}_i := \langle \widetilde{W}_i^{[l]}, \, \hat{\xi}^{[l]} \rangle_{\mathbb{H}},
	\end{equation}
	using the estimated PLS component direction $\widehat{\xi}^{[l]}$ obtained from Propositions \ref{proposition:eigen_regul} and \ref{proposition:linear_regul}. 
	Since   $\widetilde{W}_i^{[l]}$ are assumed to have a sample mean of zero, these PLS scores will also have a sample mean of zero.
	To obtain the $(l+1)$-th  iteration's responses and hybrid predictors, we regress the $(l)$-th iteration's responses and hybrid predictors on  
	these PLS scores by least squares and then residualize. 
	Specifically,
	the $(l+1)$-th predictor is computed as a residual of hybrid-on-scalar linear regression model:
	\begin{equation*}
		\widetilde{W}_i^{[l]}  =
		\widehat{\rho}_i^{[l]}
		\hat{\delta}^{[l]} + \epsilon_i,
	\end{equation*}
	where $\hat{\delta}^{[l]} \in \widetilde{\mathbb{H}}$ is the regression coefficient.
	In the same spirit as the PLS component direction estimation step,
	rather than treating the hybrid object as a long vector of concatenated function evaluations at time points and scalar vectors,
	we employ a basis expansion approach to fit the entire hybrid object in one step.
	Therefore, our method is computationally efficient, and applicable for dense or irregular functional data.
	Consequently, $\delta^{[l]}$ is obtained by minimizing a  
	least squares criterion: \jongmin{comment: I removed the penalization because it makes proving the orthonormality of the PLS components in Proposition \ref{proposition: modified orthnormality of PLS components} impossible. Smoothness of $\beta$ seems to soley rely on the smoothness of $\xi$. See Section \ref{section:sub:regression_coeff}.}
	\begin{equation}\label{def:argmin_pensse}
		\hat{\delta}^{[l]} := \arg \min_{\delta \in \widetilde{\mathbb{H}}} \sum_{i=1}^n 
		\| \widetilde{W}_i^{[l]} - \hat{\rho}_i^{[l]}\, \delta\|_{\mathbb{H}}^2.
	\end{equation}
	On the other hand, the $(l+1)$-th response   is computed as a residual of a scalar-on-scalar linear regression model:
	\begin{equation*}
		Y_i^{[l]}  =
		\hat{\nu}^{[l]}   	\widehat{\rho}_i^{[l]} + \epsilon_i.
	\end{equation*}
	The following proposition demonstrates that this residualization step can be performed simply, analogous to scalar PLS.
	\begin{lemma}[Closed-form  solution]\label{proposition:closed_form_orthgonalization}\jongmin{Previously stated as "we can show".}
		Let us denote
		$
		\hat{\boldsymbol{\rho}}^{[l]} := (\hat{\rho}^{[l]}_1, \ldots, \hat{\rho}^{[l]}_n)^\top.
		%\quad\text{and} \quad		\bar{\rho}^{[l]} := (\hat{\boldsymbol{\rho}}^{[l]})^\top \hat{\boldsymbol{\rho}}^{[l]}.
		$
		The $(l+1)$-th iteration's  predictors and responses are computed as
		\begin{equation*}
			\widetilde{W}_i^{[l+1]} 
			:= 	\widetilde{W}_i^{[l]}  - 
			\widehat{\rho}_i^{[l]}
			\hat{\delta}^{[l]},~\text{where}~
			\delta^{[l]}
			:= 
			\frac{1}{\| 	\hat{\boldsymbol{\rho}}^{[l]}\|_2^2}
			\sum_{i=1}^n 
			\widehat{\rho}_i^{[l]}
			\widetilde{W}_i^{[l]},
			%%%%%%%%%%% 
			,
		\end{equation*} 	
		and
		\begin{equation}\label{algorithm_step:residualization_predictor}
			Y_i^{[l+1]} = Y_i^{[l]} - 
			\hat{\nu}^{[l]} 
			\widehat{\rho}_i^{[l]},~\text{where}~\hat{\nu}^{[l]} :=  
			\frac{
				\mathbf{y}^{[l] \top }  \hat{\boldsymbol{\rho}}^{[l]}
			}{
				\| \hat{\boldsymbol{\rho}}^{[l]} \|_2^2
			}.
		\end{equation}
	\end{lemma}
	Proof of Lemma \ref{proposition:closed_form_orthgonalization} is provided in Appendix \ref{section:proof:proposition:closed_form_orthgonalization}.
	\jongmin{proof needs to be revised}
	Since $\widetilde{W}_i^{[l]}$ and $Y_i^{[l]}$ are assumed to have a sample mean of zero, their respective residuals, $\widetilde{W}_i^{[l+1]}$ and $Y_i^{[l+1]}$, also maintain a zero sample mean.
	
	
	
	
	
	\subsection{Final step: estimating  the hybrid regression coefficient}
	\label{section:sub:regression_coeff}
	The hybrid regression coefficient $\beta$ in model \eqref{eq: Hybrid functional model} can be written 
	as a linear combination of PLS directions:
	\begin{lemma}\label{lemma:recursive}
		Let use define
		$
		\widehat{\iota}^{[1]} := \widehat{\xi}^{[1]}.
		$
		For $l \ge 2$, we recursively   define:
		\begin{equation*}
			\widehat{ \iota}^{[l]} = \widehat{\xi}^{[l]} - \sum_{u=1}^{l-1} \langle \widehat{ \delta}^{[u]}, \widehat{\xi}^{[l]} \rangle_{\mathbb{H}} \widehat{ \iota}^{[u]}. 
		\end{equation*}
		Then we have:
		\begin{equation*}
			\widehat{\rho}_i^{[l]}
			=
			\langle  W_i^{[l]}, \widehat{\xi}^{[l]} \rangle_\mathbb{H}
			=
			\langle  W_i, \widehat{ \iota}^{[l]} \rangle_\mathbb{H}.
		\end{equation*}
	\end{lemma}
	Proof of Lemma \ref{lemma:recursive} is provided in Appendix \ref{section:proof:lemma:recursive}.
	
	Next,  \eqref{algorithm_step:residualization_predictor} leads to the following model:
	\begin{equation*}
		Y_i = \sum \limits_{l=1}^L \widehat{\nu}^{[l]} \widehat{\rho}_i^{[l]} + \epsilon_i.
	\end{equation*}
	This model lets us to express $Y_i$ as:
	\begin{equation*}
		Y_i = \sum \limits_{l=1}^L \widehat{\nu}^{[l]} \langle W_i^{[l]}, \widehat{\xi}^{[l]} \rangle_\mathbb{H} +\epsilon_i 
		= 
		\bigl \langle W_i, \sum \limits_{l=1}^L \widehat{\nu}^{[l]} \widehat{ \iota }^{[l]}
		\bigr \rangle_\mathbb{H}+\epsilon_i,
	\end{equation*}
	which, given the uniqueness of $ \beta $, leads to
	\begin{equation*}
		\widehat{ \beta} = \sum \limits_{l=1}^L\widehat{\nu}^{[l]} \widehat{ \iota }^{[l]}
		=\jongmin{added:}
		\sum_{l=1}^L \left( \widehat{\nu}^{[l]} - \sum_{k=l+1}^L \widehat{\nu}^{[k]} \langle \widehat{\delta}^{[l]}, \widehat{\xi}^{[k]} \rangle_{\mathbb{H}} \right) \widehat{\xi}^{[l]}.
	\end{equation*}
	\jongmin{added: Note that if $L$ is too large, $\widehat{ \beta}$ can be wiggly even if $\widehat{\xi}^{[l]}$'s are smooth.}
	Thus the number of PLS components $L$ to be estimated can be chosen by cross-validation.
	
	\begin{algorithm} 
		\caption{Hybrid partial least squares regression}\label{alg:hybrid_pls}
		\begin{algorithmic}[1]
			\State 	\textbf{Initialize:} $(\mathbb{B}, \mathbb{B}^{\prime\prime}, \Theta^{[1]}, \mathbf{y}^{[1]})$ as the data objects after basis expansion, following Section \ref{sec: finite basis approximation}.
			
			\State   $\widetilde{W}_1^{[1]}, \ldots, \widetilde{W}_n^{[1]}, Y_1^{[1]}, \ldots, Y_n^{[1]} \leftarrow$ standardized versions of $W_1, \ldots, W_n, Y_1, \ldots, Y_n$, following Section \ref{subsec: Data Preprocessing}
			\For{$l = 1, 2, \ldots, L$}
			\\
			\textbf{PLS direction and score estimation  (Proposition \ref{proposition:linear_regul}): }
			\State $\mathbf{u}_j^{[l]} \leftarrow B \Theta_j^{[l]\top} \mathbf{y}^{[l]}, ~j = 1, \ldots, K$
			\State $\mathbf{v}^{[l]} \leftarrow \mathbf{Z}^{[l]\top} \mathbf{y}^{[l]} $
			\State $	q^{[l]}  \leftarrow \sum_{j=1}^K \mathbf{u}_j^{[l]\top} (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j^{[l]} + \mathbf{v}^{[l]\top} \mathbf{v}^{[l]}$
			\State $( \hat{\gamma}_{j1}^{[l]} , \ldots, \hat{\gamma}_{jM}^{[l]} )^\top  \leftarrow \frac{1}{\sqrt{q}} (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j^{[l]} , ~j = 1, \ldots, K$
			\State $\hat{\boldsymbol{\zeta}}^{[l]}  \leftarrow \frac{1}{\sqrt{q}} \mathbf{v}^{[l]} $
			\State $	\hat{\xi}^{[l]}	\leftarrow 
			\biggl(
			\sum_{m=1}^M \hat{\gamma}_{1m}^{[l]} \, b_m(t),
			\ldots, 
			\sum_{m=1}^M \hat{\gamma}_{Km}^{[l]} \, b_m(t), \hat{\boldsymbol{\zeta}}^{[l]}
			\biggr)
			$
			\Comment{PLS direction} 
			\State 
			$
			\widehat{\rho}_i^{[l]} \leftarrow \langle \hat{\xi}^{[l]},   \tilde{W}^{[l]}_i \rangle, 
			~i=1,  \ldots, n
			$ \Comment{PLS score}
			\\
			\textbf{Residualization (Proposition \ref{proposition:closed_form_orthgonalization}):}
			\State $\nu^{[l]} 
			\leftarrow
			\frac{
				\sum_{i=1}^n Y_i^{[l]} \widehat{\rho}_{i}^{[l]}}{
				\sum_{i=1}^n \widehat{\rho}_{i}^{[l]2}
			}$ \Comment{Least squares estimate} 
			%
			\State $ Y_i^{[l+1]} \leftarrow Y_i^{[l]} - \nu^{[l]}\widehat{\rho}_i^{[l]}~i=1,  \ldots, n$
			%
			\State $ \widehat{ \delta }^{[l]}  \leftarrow \frac{1}{\sum_{i=1}^n \widehat{\rho}_{i}^{[l]2}}\sum_{i=1}^n  \widehat{\rho}_{i}^{[l]}\widetilde{W}_i^{[l]} $ \Comment{Least squares estimate} 
			\label{alg:step:scalar_pls_residualization}
			\State $\widetilde{W}_i^{[l+1]}  \leftarrow \widetilde{W}_i^{[l]} -   \widehat{\rho}_{i}^{[l]}  \widehat{ \delta}^{[l]} $
			\EndFor
			\textbf{Regression coefficient estimation (Section \ref{section:sub:regression_coeff}):}
			\State
			$\widehat{ \iota }^{[1]} \leftarrow \widehat{ \xi }^{[1]}$
			\For{$l =  2, \ldots, L$}
			\State $\widehat{ \iota }^{[l]} \leftarrow \widehat{  \xi }^{[l]} - \sum_{u=1}^{l-1} \langle \widehat{ \delta }^{[u]}, \widehat{ \xi }^{[l]} \rangle  \widehat{ \iota }^{[u]}$  
			\EndFor
			$	\widehat{ \beta } \leftarrow \sum \limits_{l=1}^L\widehat{\nu}^{[l]} \widehat{ \iota}^{[l]}$ \Comment{regression coefficient estimate}
			\State \textbf{Output:} the  regression coefficient estimate $	\widehat{ \beta }$
		\end{algorithmic}
	\end{algorithm}
	
	
	\section{Properties of the hybrid PLS}
	\label{section:main:properties_of_our_method}
	This section provides the mathematical properties that support the algorithm suggested in Section \ref{section:main:our_algorithm}.
	Section \ref{section:sub:tucker} shows that the core optimization problem for the Partial Least Squares (PLS) direction estimation step, presented in Proposition \ref{proposition:eigen_noregul}, is well-defined under mild conditions.
	Section \ref{section:sub:geom} demonstrates that our algorithm preserves the core properties of PLS, namely the orthonormality of the derived directions and the orthogonality of the scores.
	
	\subsection{ Tucker's Criterion  }\label{section:sub:tucker} 
	\jongmin{comment:maybe we can move this section to precede the algorithm section. then the new structure will introduce population version first and introduce finite-sample estimation procedure.}
	We now derive the population analogue of Algorithm~\ref{alg:hybrid_pls} and establish its key mathematical properties, namely the orthogonality of the PLS directions and scores, as well as convergence to the true response. At the population level, where the random objects are fully observable, we may drop the sample index $i$ and simply work with $Y$
	and
	$W = (X_1, \ldots, X_K, Z_1, \ldots, Z_p)$. Throughout this section we assume 
	$\mathbb{E}[Y] = 0 \in \mathbb{R}$ and $\mathbb{E}[W] = 0 \in \mathbb{H}$.
	
	The core optimization problem in PLS is formulated in terms of maximizing the squared cross-covariance. At the population level, this quantity is naturally characterized by cross-covariance operators, which we define next. For notational simplicity, we suppress the iteration index $l$, with the understanding that for $l=1$ the predictors correspond to the original variables, while for $l \geq 2$ they represent the residualized versions.
	\begin{definition}\label{def:cross_cov_terms}
		The covariance operator of the hybrid predictor $W$ is denoted as $\Sigma_{W} := \mathbb{E}[W \otimes W]$, so that for $u,v \in \mathbb{H}$, we have
		\[
		\Sigma_{W} u = \mathbb{E}[
		\langle W, u\rangle_{\mathbb{H}}W
		],
		\quad
		\langle \Sigma_{W} u, v \rangle_{\mathbb{H}}
		= \mathbb{E}[\langle W,u \rangle_{\mathbb{H}} \, \langle W,v \rangle_{\mathbb{H}}].
		\]
		
		The cross-covariances between the response $Y$ and the predictors are
		\[
		\sigma_{YX}
		:=
		\bigl(\mathbb{E}[Y X_{1}],\ldots,\mathbb{E}[Y X_{K}]\bigr) \in \mathcal{F}^{K},
		\quad
		\sigma_{YZ} 
		:=
		\bigl(\mathbb{E}[Y Z_{1}],\ldots,\mathbb{E}[Y Z_{p}]\bigr)^\top \in \mathbb{R}^p.
		\]
		
		The cross-covariance between $Y$ and the hybrid predictor $W$ is then
		\[
		\Sigma_{YW} := \mathbb{E}[ Y W ] = \bigl( \sigma_{YX}, \, \sigma_{YZ} \bigr) \in \mathbb{H}.
		\]
	\end{definition}
	At the population level, Algorithm \ref{alg:hybrid_pls} without regularizaiton can be expressed as follows.
	Let $W^{[1]} := W$ and $Y^{[1]} := Y$.
	For any   integer $l \geq 2$,
	to be the residuals of the 
	
	where 
	
	and
	
	which is attained by the leading eigenfunction of the operator 
	$\mathcal{U}^{[l-1]}$, defined analogously to Lemma~\ref{lemma:composite_cross_cov} 
	using $W^{[l-1]}$ and $Y^{[l-1]}$, provided the conditions in 
	Lemma~\ref{lemma:cross_cov_functional} are satisfied.
	\begin{definition}[Population PLS]\label{def:population_pls}
		The population analogue of the PLS optimization problem \eqref{scalar_PLS_sample} in the hybrid space is formulated as the constrained generalized eigenproblem:
		\begin{equation}\label{const_eigen_pop}
			\xi^{[l]} = \arg\max_{h \in \mathbb{H}}
			\operatorname{Cov}^2(\langle W, h \rangle_{\mathbb{H}}, Y),~s.t.~\|h\|_{\mathbb{H}}=1, \langle h, \Sigma_W \, \xi^{[j]} \rangle_{\mathbb{H}} = 0,\, j < l.
		\end{equation}
	\end{definition}
	As in scalar case, this problem can be solved by an iterative algorithm, and yields orthogonal scores:
	\begin{proposition}[Properties of population PLS]
		\label{prop:residualization_equiv_eigen}
		The solution of optimization problem \eqref{const_eigen_pop} of Definition \ref{def:population_pls} is equivalent to that of population NIPALS (Algorithm \ref{alg:population_pls}).  For every \(s\ge 1\) and every \(j\le s-1\), we have
		$
		\mathbb{E}[\rho^{[j]}\rho^{[s]}]=0
		$
		and
		$
		\mathbb{E}[Y^{[s]}\rho^{[j]}]=0.
		$
		
	\end{proposition}
	Proof of Proposition \ref{prop:residualization_equiv_eigen} is provided in Appendix \ref{section:proof:prop:residualization_equiv_eigen}.
	\begin{algorithm} 
		\caption{Population NIPALS}\label{alg:population_pls}
		\begin{algorithmic}[1]
			\State
			$(W^{[1]}, Y^{[1]} \leftarrow (W, Y)$
			\For{$l = 1, 2, \ldots, L$}
			\State
			$
			\xi^{[l]}
			\leftarrow
			\arg\max_{h \in \mathbb{H}} \operatorname{Cov}^2(\langle W^{[l]}, h \rangle_{\mathbb{H}}, Y^{[l]})~s.t.~ \|h\|_{\mathbb{H}} = 1,
			$\label{alg:line:pls_direction} \Comment{PLS direction}
			%%%%%%%%%
			\State 
			$
			\rho^{[l]} \leftarrow \langle \xi^{[l]}, W^{[l]} \rangle 
			$ \Comment{PLS score}
			%%%%%%%%%%
			\State
			$
			\delta^{[l]}
			\leftarrow
			\frac{1}{\mathbb{E}[ (\rho^{[l]})^2 ]}  \mathbb{E}[W^{[l]} \rho^{[l]}]
			$\label{population_delta}
			\Comment{Linear regression of 
				$W^{[l]}$
				on $\rho^{[l]}$}
			%%%%%%%%%%%
			\State
			$
			\nu^{[l]}
			\leftarrow
			\frac{1}{\mathbb{E}[ (\rho^{[l]})^2 ]}
			\mathbb{E}[Y^{[l]} \rho^{[l]}]
			$
			\Comment{
				Linear regression of 
				$Y^{[l]} $
				on $\rho^{[l]}$
			}
			%%%%%%%%%%%%%%%%%%%
			\State 
			$ W^{[l+1]} \leftarrow W^{[l]} - \rho^{[l]} \, \delta^{[l]}$
			\label{population_predictor_residualize}
			\Comment{
				Residualized predictor
			}
			\State
			$ Y^{[l]} \leftarrow Y^{[l-1]} - \nu^{[l-1]} \, \rho^{[l-1]}$
			\Comment{
				Residualized response
			}
			\EndFor
			\State \textbf{Output:}
			PLS directions $\xi^{[1]}, \ldots, \xi^{[L]}$
		\end{algorithmic}
	\end{algorithm}
	
	We show that problem \eqref{const_eigen_pop} and Algorithm \ref{alg:population_pls} are well-defined under mild conditions by  establishing that line \ref{alg:line:pls_direction} of Algorithm \ref{alg:population_pls} is well-defined under mild conditions. For clarity, we define the relevant operators while omitting the iteration index for the residualized predictors and responses.
	\begin{lemma}\label{lemma:cross_cov_functional}
		Define the operator $
		\mathcal{C}_{YW}$ such that for   any $h=(f_1, \ldots, f_K, \mathbf{v}) \in \mathbb{H}$,
		\begin{equation*} \mathcal{C}_{YW} h := \mathbb{E} \left[ \langle W, h \rangle_\mathbb{H} Y \right] = \langle \Sigma_{YW}, h \rangle_\mathbb{H} \in \mathbb{R}.
		\end{equation*} 
		\jongmin{instead of showing $\mathcal{U}$ is compact, let's show $\mathcal{C}_{YW}$ is compact. This way, we don't need uniform continuiity of $\sigma_{YX}$.}
		If 
		$\sigma_{YX}$ and $\sigma_{YZ}$ are bounded in the sense that
		there exist finite constants $Q_1$ and $Q_2$ such that
		\begin{align}\label{condition_for_compact}
			\max \limits_{k=1, \ldots, K} \sup \limits_{t \in [0,1]} \mathbb{E}
			[Y X_{k}](t)^2 < Q_1 \quad \textnormal{and} \quad  \max_{r=1, \ldots, p} \mathbb{E}
			[Y Z_{r}]^2 < Q_2,
		\end{align}
		the operator $\mathcal{C}_{YW}$ is a compact operator.
	\end{lemma}
	Proof of Lemma \ref{lemma:cross_cov_functional}
	is provided in Appendix \ref{section:proof:lemma:cross_cov_functional}.
	\begin{lemma}\label{lemma:cross_cov_operator}
		Define $\mathcal{C}_{WY} = \mathbb{E}[Y \otimes W]$,
		such that for any  $d \in \mathbb{R}$,
		%%%%
		\begin{align*}		
			\mathcal{C}_{WY} d 
			:= 
			\mathbb{E} 
			\bigl[
			\langle Y, d \rangle W 
			\bigr] 
			= 
			\mathbb{E}
			[ Y W d ] =  d \, \Sigma_{Y W} \in \mathbb{H}.
		\end{align*}
		Then	$\mathcal{C}_{WY}$ is an adjoint operator of $\mathcal{C}_{YW}$. That is, $\mathcal{C}_{WY} = \mathcal{C}_{YW}^\ast$.
	\end{lemma}	
	The proof of Lemma \ref{lemma:cross_cov_operator} is provided in Appendix \ref{section:proof:lemma:cross_cov_operator}.
	
	\begin{lemma}[Composite cross-covariance operator]\label{lemma:composite_cross_cov}
		Define
		$\mathcal{U} := \mathcal{C}_{WY} \circ \mathcal{C}_{YW}: \mathbb{H} \rightarrow \mathbb{H}$ as an operator which performs the following mapping:
		\begin{equation*}
			\mathcal{U} h 
			= 
			\mathcal{C}_{WY} ( \mathcal{C}_{YW} h) 
			=
			\mathcal{C}_{YW} (\langle \Sigma_{YW}, h \rangle_\mathbb{H}) 
			=  
			\Sigma_{YW} \, \langle \Sigma_{YW}, h \rangle_\mathbb{H} 
			= 
			(\Sigma_{YW} \otimes \Sigma_{YW}) h.
		\end{equation*}
		In other words, $\mathcal{U} = \Sigma_{YW} \otimes \Sigma_{YW}$.
		Then $\mathcal{U}$
		is a self-adjoint and positive-semidefinite operator.
		Under the conditions of Lemma \ref{lemma:cross_cov_functional}, $\mathcal{U}$ is a compact operator.
	\end{lemma}
	The proof of Lemma \ref{lemma:composite_cross_cov} is provided in Appendix \ref{section:proof:lemma:composite_cross_cov}.
	By the Hilbert-Schmidt theorem (e.g., Theorem 4.2.4 in \citealp{hsingTheoreticalFoundationsFunctional2015}), Lemma \ref{lemma:composite_cross_cov} guarantees the existence of a complete orthonormal system of eigenfunctions $\{\xi_{(u)}\}_{u \in \mathbb{N}}$ of $\mathcal{U}$ in $\mathbb{H}$ such that $\mathcal{U} \xi_{(u)} = \kappa_{(u)}  \xi_{(u)}$, where $\{\kappa_{(u)}\}_{u \in \mathbb{N}}$ are the corresponding sequence of eigenvalues that goes to zero as $u \rightarrow \infty$, that is, $\kappa_{(1)} \ge \kappa_{(2)} \ge \cdots \ge 0$.
	
	The following theorem presents population-level PLS and Tucker's Criterion for our scalar-on-hybrid regression model and ensures that line \ref{alg:line:pls_direction} of Algorithm \ref{alg:population_pls} is well-defined under mild conditions.
	\begin{theorem}[Tucker's criterion]\label{theorem:population_PLS} Under the conditions of Lemma \ref{lemma:cross_cov_functional}, the constrained maximum
		\begin{equation*} 
			\max \limits_{\substack{ \Vert \xi \Vert_\mathbb{H}=1}} \operatorname{Cov}^2 \left(\langle W , \, \xi \rangle_\mathbb{H}, Y \right)
		\end{equation*} 
		is attained by the eigenfunction associated with the largest eigenvalue of the operator $\mathcal{U}$.
	\end{theorem}
	The proof of Theorem \ref{theorem:population_PLS} is provided in Appendix \ref{section:proof:theorem:population_PLS}.
	
	Finally, the following theorem states that the fitted values from hybrid PLS converge to those of ordinary linear regression in the mean-squared sense. This shows that hybrid PLS, as a dimension-reduction technique, can effectively capture the predictive power of a full linear model when a sufficient number of components is used.
	\begin{theorem}[$L^2$ Convergence of Hybrid PLS at Population Level]\label{thm:hybrid_pls_convergence}
		Let $Y_{\mathrm{PLS}, L} = \sum_{l=1}^L \nu^{[l]} \rho^{[l]}$ be the Hybrid PLS fitted value with $L$ components. Then, the Hybrid PLS fitted value converges to $Y$ in the mean-squared sense:
		\begin{equation*}
			\lim_{L \to \infty} \mathbb{E}\left[ \| Y_{\mathrm{PLS},L } - Y  \|_2^2 \right] = 0
		\end{equation*}
	\end{theorem}
	The proof of Theorem \ref{thm:hybrid_pls_convergence} is provided in Appendix \ref{section:proof:thm:hybrid_pls_convergence}.
	
	\subsection{Geometric properties}\label{section:sub:geom}
	A fundamental property of partial least squares,
	as seem in Definition \ref{def:population_pls} and Proposition \ref{prop:residualization_equiv_eigen},
	is that between iterations, its derived directions are orthonormal and PLS scores are orthogonal. Our regularized estimates preserve this property, with respect to a modified inner product that incorporates the roughness penalty, defines as follows:
	\begin{definition}[Roughness-sensitive inner product]\label{def:hybrid_inner_product_roughness}
		Given two hybrid predictors $W_1 = (X_{11}, \ldots, X_{1K}, \mathbf{Z}_1)$ and $W_2 = (X_{21}, \ldots, X_{2K}, \mathbf{Z}_2)$, both elements of $\mathbb{H}$ as defined in Definition \ref{def:hybrid_predictor}, and a roughness penalty matrix $\Lambda = \operatorname{blkdiag}(\lambda_1 I_M, \ldots, \lambda_K I_M, 0_{p \times p})$, the roughness-sensitive inner product between $W_1$ and $W_2$ is defined as:
		\begin{equation}		\label{eq: hybrid inner product_roughness}
			\langle W_1, W_2\rangle_{\mathbb{H}, \Lambda}
			:=
			\sum \limits_{k=1}^K \int_0^1 X_{1k}(t) X_{2k}(t) \, dt
			+
			\sum \limits_{k=1}^K \lambda_k \int_0^1 X^{\prime \prime}_{1k}(t) X^{\prime \prime}_{2k}(t) \, dt
			+
			\mathbf{Z}_1^\top \mathbf{Z}_2.
		\end{equation}
	\end{definition}
	Based on this inner product, the following proposition states that the PLS component directions estimated from Proposition  \ref{proposition:eigen_regul} are orthonormal.
	\begin{proposition}[Orthonormality of estimated PLS component directions]\label{proposition: modified orthnormality of PLS components}
		The PLS component directions $
		\widehat{\xi}^{[1]}, \widehat{\xi}^{[2]}, \ldots, \widehat{\xi}^{[L]}
		$, estimated via Proposition \ref{proposition:eigen_regul} with a roughness penalty matrix $\Lambda = \operatorname{blkdiag}(\lambda_1 I_M, \ldots, \lambda_K I_M, 0_{p \times p})$, are mutually orthonormal with respect to the inner product $\langle \cdot, \cdot \rangle_{\mathbb{H}, \Lambda}$. That is,
		\begin{equation*}
			\langle \widehat{\xi}^{[l_1]}, \widehat{\xi}^{[l_2]} \rangle_{\mathbb{H}, \Lambda}
			= \mathbbm{1}(l_1 = l_2), \quad l_1, l_2 = 1, \ldots, L.
		\end{equation*}
	\end{proposition}
	The proof of Proposition \ref{proposition: modified orthnormality of PLS components} is provided in Appendix \ref{section:proof:proposition: modified orthnormality of PLS components}.
	
	The next proposition states that the vectors of estimated PLS scores for different iteration numbers are mutually orthogonal.
	\begin{proposition}	\label{proposition: orthnormality of PLS scores}
		Recall from Lemma \ref{proposition:closed_form_orthgonalization} that   $\widehat{\boldsymbol{\rho}}^{[l]}$ denote the $n$-dimensional vector whose elements consist of the $l$-th estimated PLS scores ($l=1, \ldots, L$) of $n$ observations.
		The vectors $\widehat{\boldsymbol{\rho}}^{[1]}, \widehat{\boldsymbol{\rho}}^{[2]}, \ldots, \widehat{\boldsymbol{\rho}}^{[L]}$  are mutually orthogonal in the sense that
		\begin{equation*}
			\widehat{\boldsymbol{\rho}}^{[l_1] \top}  \widehat{\boldsymbol{\rho}}^{[l_2] }= 0 \quad \textnormal{for} \quad \; l_1,l_2 \in \{1, \ldots, L\}, \; l_1 \ne l_2. 
		\end{equation*}
	\end{proposition}
	The proof of Proposition \ref{proposition: orthnormality of PLS scores} is provided in Appendix \ref{section:proof:proposition: orthnormality of PLS scores}.
	
	\section{Simulation studies}\jongmin{08/16/2025: re-starting. the writing here is not completed.}
	To evaluate the superiority of our method under complex dependency structures
	{\textemdash}
	specifically, dependencies among functional predictors,
	among scalar predictors,
	and between scalar and functional predictors
	{\textemdash}
	
	\medskip
	\noindent
	\textit{Matrix-normal setting.}
	We begin by constructing predictors from a matrix-normal distribution, a framework that offers convenient and flexible control over the dependence structure across both rows and columns. Matrix-normal (MN) models, also known as Kronecker-separable covariance models, provide a principled approach to modeling multivariate data with structured covariance. Specifically, the matrix-normal distribution is defined as
	\begin{equation*}
		\mathbf{X} \sim \mathcal{MN}_{m \times n}(\mathbf{M}; \mathbf{R}, \mathbf{C}),
	\end{equation*}
	and its log-density is given by
	\[
	\log p(\mathbf{X} \mid \mathbf{M}, \mathbf{R}, \mathbf{C}) = -\frac{mn}{2} \log(2\pi) - \frac{m}{2} \log |\mathbf{C}| - \frac{n}{2} \log |\mathbf{R}| - \frac{1}{2} \mathrm{Tr}\left[ \mathbf{C}^{-1} (\mathbf{X} - \mathbf{M})^\top \mathbf{R}^{-1} (\mathbf{X} - \mathbf{M}) \right].
	\]
	The key insight behind Kronecker separability is that if
	$
	\mathbf{Y} \sim \mathcal{MN}(\mathbf{M}, \mathbf{R}, \mathbf{C}),
	$
	then its vectorized form follows a multivariate normal distribution:
	$
	\mathrm{vec}(\mathbf{Y}) \sim \mathcal{N}(\mathrm{vec}(\mathbf{M}), \mathbf{C} \otimes \mathbf{R}),
	$
	where $\otimes$ denotes the Kronecker product and $\mathrm{vec}$ is the vectorization operator.
	
	
	\begin{figure}[ht!]
		\centering
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image/cor_strong.pdf}
			\caption{Correlation Matrix - Strong Dependency}
			\label{fig:cor_strong}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image/cor_weak.pdf}
			\caption{Correlation Matrix - Weak Dependency}
			\label{fig:cor_weak}
		\end{subfigure}
		
		\vspace{0.5cm}  % Adjust vertical spacing
		
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image/graph_strong.pdf}
			\caption{Graph Structure - Strong Dependency}
			\label{fig:graph_strong}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image/graph_weak.pdf}
			\caption{Graph Structure - Weak Dependency}
			\label{fig:graph_weak}
		\end{subfigure}
		
		\caption{Comparison of Correlation Matrices and Graph Structures under Strong and Weak Dependencies.}
		\label{fig:correlation_graph_comparison}
	\end{figure}
	
	\begin{comment}
		The goal of this simulation is to:
		\begin{enumerate}
			\item Utilize a graphical model to introduce a complex dependence structure.
			\item Demonstrate that our method outperforms the penalized functional regression approach by \citet{goldsmithPenalizedFunctionalRegression2011}.
			\item Show that our method exhibits superior performance as correlation strength increases.
		\end{enumerate}
		
		Our previous setup, presented in Section \ref{simul:meeting_feb}, appeared to achieve these objectives. However, we identified several limitations:
		\begin{enumerate}
			\item The resulting precision matrix was singular, requiring us to scale up the diagonal elements to compute its inverse. This adjustment disrupted the original AR(1) structure of the functional component, making it significantly less smooth.
			\item Increasing conditional correlation did not consistently enhance the superiority of our method.
		\end{enumerate}
		To address these issues, I will explore alternative functional graphical model settings that exhibit a strong dependence structureone substantial enough to be detected by statistical methodologies and validated by top statistical journal papers. I will evaluate the performance of penalized functional regression on these data-generating models.
		Among them, I will select three to four settings for comparison against other methods.
	\end{comment}
	Building on the functional graphical model simulation of \citet{zhuBayesianGraphicalModels2016}, we generate a mixed graphical model with five nodes, described as follows:  
	\begin{itemize}
		\item Nodes F1 and F2: Two functional predictors modeled as Gaussian processes using a truncated Karhunen-Love expansion, where the eigenbasis consists of Fourier basis functions with a fixed number of basis functions, \(M = 9\).  
		\item  Nodes S1, S2, and S3: Three scalar predictors, each following an \(s\)-dimensional multivariate normal distribution. Unlike the functional predictors, these scalar predictors are modeled directly without basis expansion.  
	\end{itemize}
	To capture dependencies among predictors, we introduce a graph structure that governs their conditional correlations. We consider two types of graph structures: a weakly connected graph and a strongly connected graph. In the Gaussian process framework, the precision matrix \(\mathbf{R}_0^{-1}\) encodes conditional independence relationships, while its inverse, \(\mathbf{R}_0\), represents marginal covariances. This structure extends to a blockwise correlation matrix \(\mathbf{R} \in \mathbb{R}^{(2M + 3s) \times (2M + 3s)}\), where off-diagonal blocks represent correlations between FPC scores and scalar predictor values.  
	Each block \((i, j)\) of \(\mathbf{R}\) is given by \((R_0)_{ij} \mathbf{I}_{M_i, M_j}\), where \(\mathbf{I}_{M_i, M_j}\) is a rectangular identity matrix. Here, \(M_i = 9\) if node \(i\) corresponds to a functional predictor and \(M_j = s\) if node \(j\) corresponds to a scalar predictor. 
	
	For each functional predictor, we assgin  \(M\) reference eigenvalues (or FPC score) drawn independently from gamma distributions with decreasing means.
	These reference eigenvalues will later be multiplies by randomly  multiplier drawn from correlated multivaraite Normal distribution. These reference eigenvalues are fixed over all samples and all independent repetition of the experiment. We draw it one randomly just to ensure the differentiation between the two functional predictors.
	we independently draw for each functional predictor from . We then sample zero-mean multivariate normal data from the covariance matrix \(\mathbf{R}\). The last \(3s\) components are assigned as scalar predictors, while the first \(2M\) components, scaled by their corresponding eigenvalues, serve as FPC scores. These scores are then expanded into functional data, evaluated over 100 equally spaced points on \([0,1]\) using a Fourier basis.  
	
	Finally, we introduce structured variability by adding a common mean function, defined as a scaled and shifted sine function. To visualize the generated data, Figure \ref{fig:correlation_graph_comparison}  plots functional predictor realizations and heatmaps of sample correlation matrices for both graph structures, illustrating the distinct dependency patterns induced by the precision matrices.  
	
	
	
	\subsection{Simulation shown in february 2025 meeting}\label{simul:meeting_feb}
	To effectively manage the dependencies among functional predictors, scalar covariates, and between functional and scalar predictors, we utilize a Gaussian Markov random field (GMRF) within the framework of Gaussian undirected graphical models. In a GMRF, the off-diagonal elements of the precision matrix capture the conditional correlations between the corresponding components. Given that our setting involves both functional and scalar covariates, we adopt the simulation setup proposed by \cite{kolar_graph_2014}, which focuses on mixed attribute Gaussian graphical models.
	
	We construct a graph consisting of two functional predictor nodes and three vector predictor nodes. Below, we sequentially describe the graph structure and the generation process for each node.
	
	\medskip
	\noindent
	\textit{Functional Predictors.}
	We consider two functional predictors that share the same precision matrix. Denoted as \( \boldsymbol{\Theta} := (\theta_{t \tilde{t}}) \in \mathbb{R}^{p \times p} \), this precision matrix follows an AR(1) structure with a white noise variance of \( 0.1^2 \) and an autoregressive coefficient of \( = 0.95 \), ensuring a smooth functional trajectory.
	
	More formally, each functional predictor is evaluated at \( p \) equally spaced points on \( [0,1] \), with values defined recursively as:
	\begin{equation*}
		X^{(k)}_i(0) = 0, \quad 
		X^{(k)}_i(t)= \rho X^{(k)}_i(t-1) + \varepsilon_{itk}, \quad \varepsilon_{itk} \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2), \quad t = 1, \dots, p,~i=1, \ldots, n,~k = 1,2.
	\end{equation*}
	In this setting, the precision matrix \( \boldsymbol{\Theta} \) exhibits a tridiagonal Toeplitz structure, where the diagonal entries are given by:
	\begin{equation*}
		\theta_{tt} = 
		\begin{cases}
			\dfrac{1}{\sigma^2(1-0.95^2)}, & t = 1, p, \\
			\dfrac{1+0.95^2}{\sigma^2(1-0.95^2)}, & 2 \leq t \leq p-1.
		\end{cases}
	\end{equation*}
	The off-diagonal entries are:
	\begin{equation*}
		\theta_{t, t+1} = \theta_{t+1, t} = -\dfrac{\rho}{\sigma^2(1-\rho^2)}, \quad 1 \leq t \leq p-1.
	\end{equation*}
	Finally, the functional predictors are smoothed using a B-spline basis with 15 basis functions, as described in Section \ref{section:sub:compute_PLS_component}.
	
	\medskip
	\noindent
	\textit{Vector Predictors.}
	We consider \( s \) vector predictors, each following a \( d \)-dimensional zero-mean Gaussian distribution with a shared precision matrix. This precision matrix, denoted as \( \Gamma := (\gamma_{ij}) \in \mathbb{R}^{d \times d} \), follows a Toeplitz structure with exponentially decaying entries, given by:
	\begin{equation*}  
		\gamma_{ij} := 0.5^{|i-j|}.
	\end{equation*}
	
	\medskip
	\noindent
	\textit{Graph Structure.}
	We arrange five nodes   in a chain structure, where each node follows a sequential order, and the last node connects back to the first, as illustrated in Figure~\ref{fig:graph}. An edge in the graph indicates that the connected nodes remain correlated when the values of all other nodes are fixed. 
	The resulting marginal dependence structure is significantly more complex than the chain structure itself. We designate nodes \(F_1\) and \(F_2\) as functional predictors and nodes \(V_1\), \(V_2\), and \(V_3\) as vector predictors.
	%
	\begin{figure}
		\centering
		\includegraphics[width=0.4\linewidth]{image/graph}
		\caption{}
		\label{fig:graph}
	\end{figure}
	%
	To introduce conditional dependencies between the components, we set the off-diagonal blocks of the precision matrix to be $0.5 \mathbf{1}$ if the corresponding components are connected in the graph, and zero otherwise. Here, $\mathbf{1}$ denotes a matrix of appropriate dimensions where all elements are equal to 1. Overall, we generate a $2p + 3d$-dimensional multivariate Gaussian distribution with mean zero and a precision matrix $\Omega$, structured as follows:
	
	\[
	\Omega =
	\begin{pmatrix}
		\Omega_{F} & 0.5 \mathbf{1} & 0 & 0 & 0.5 \mathbf{1} \\
		0.5 \mathbf{1} & \Omega_{F} & 0.5 \mathbf{1} & 0 & 0 \\
		0 & 0.5 \mathbf{1} & \Omega_{V} & 0.5 \mathbf{1} & 0 \\
		0 & 0 & 0.5 \mathbf{1} & \Omega_{V} & 0.5 \mathbf{1} \\
		0.5 \mathbf{1} & 0 & 0 & 0.5 \mathbf{1} & \Omega_{V} \\
	\end{pmatrix}
	\]
	
	For each observation drawn from this multivariate Gaussian distribution, we process 
	
	The regression coefficients for the first functional predictor, $\beta_{F_1}$, are drawn from a multivariate normal distribution $N(0, 5 \mathbf{I}_p)$, with a fixed random seed. The coefficients for the second functional predictor, $\beta_{F_2}$, are drawn independently from the same distribution and are also smoothed using a B-spline basis with 10 basis functions.
	For the vector covariates, the regression coefficients are sampled from $N(0, I_d)$. After calculating the inner product of the covariates and their corresponding coefficients, independent Gaussian noise from $N(0, 0.1^2)$ is added to the generated responses to simulate measurement noise.
	
	The baseline methods are:
	\begin{itemize}
		\item Penalized functional regression (pfr)~\cite
		{goldsmithPenalizedFunctionalRegression2011}\item Principal component regression (fpcr): run both of PCA for multple functional predictors~\cite{happ_multivariate_2018} and scalar PCA and run OLS on the PC scores.
	\end{itemize}
	% TODO: \usepackage{graphicx} required
	
	We compare our method with these baseline methods using $p=100$. We consider scenarios with $d=1, 2, 3, 4, 5$ and $n = 100, 200, 300, 400$. For each scenario, we use 70\% of the data for training and evaluate prediction performance on the remaining 30\% test set, using the root prediction mean squared error as the evaluation metric. For our method and PCR, the maximum number of components are set as 20. The number of components is chosen by 5-fold cross validation. The results, summarized in Table~\ref{table
	}, demonstrate that our method consistently outperforms the baseline methods across all scenarios.
	\begin{figure}
		\centering
		\includegraphics[width=0.9\linewidth]{Screenshot 2025-02-25 125839.png}
		\caption{Enter Caption}
		\label{fig:enter-label}
	\end{figure}
	
	
	\section{Data Application} \jongmin{08/16/2025: re-starting. the writing here is not completed.}
	
	\medskip
	\noindent
	\textit{Renal study data.}~
	We applied our proposed hybrid functional PLS regression, along with other regression methods, to the Emory renal study data. The study collected data on 226 kidneys (left and right) from 113 subjects, including: (i) baseline renogram curves; (ii) post-furosemide renogram curves; (iii) ordinal ratings of kidney obstruction status (non-obstructed, equivocal, or obstructed) independently assessed by three nuclear medicine experts; (iv) eight kidney-level pharmacokinetic variables derived from radionuclide imaging; and (v) two subject-level variables (age and gender). The subjects had a mean age of 57.8 years (SD = 15.5; range = 1883), with 54 males (48\%) and 59 females (52\%). The three experts unanimously classified 153 kidneys as non-obstructed, 5 as equivocal, and 40 as obstructed, while 28 kidneys had discrepant ratings.
	
	The two renogram curves, (i) and (ii), were treated as functional predictors and smoothed using a B-spline basis of order 15. The remaining variables, excluding the diagnosis, were treated as scalar predictors. Given the nature of these variables, we assume they are correlated with the renogram curves but not entirely redundant, as they may contain additional useful information. Finally, the diagnoses provided by the three experts were averaged and transformed using a min-max logit transformation. We splitted the data into 70\% of training data and 30\% of testing data, and evalauted the prediction perforamnec by root mean squared error on the test data, normalized by the range of the test data response.
	
	
	\subsection{}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% Support information, if any,             %%
	%% should be provided in the                %%
	%% Acknowledgements section.                %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{acks}[Acknowledgments]
		The authors would like to thank the anonymous referees, an Associate
		Editor and the Editor for their constructive comments that improved the
		quality of this paper.
	\end{acks}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% Funding information, if any,             %%
	%% should be provided in the                %%
	%% funding section.                         %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{funding}
		The first author was supported by NSF Grant DMS-??-??????.
		
		The second author was supported in part by NIH Grant ???????????.
	\end{funding}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%% Supplementary Material, including data   %%
	%% sets and code, should be provided in     %%
	%% {supplement} environment with title      %%
	%% and short description. It cannot be      %%
	%% available exclusively as external link.  %%
	%% All Supplementary Material must be       %%
	%% available to the reader on Project       %%
	%% Euclid with the published article.       %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{supplement}
		\stitle{Title of Supplement A}
		\sdescription{Short description of Supplement A.}
	\end{supplement}
	\begin{supplement}
		\stitle{Title of Supplement B}
		\sdescription{Short description of Supplement B.}
	\end{supplement}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%                  The Bibliography                       %%
	%%                                                         %%
	%%  imsart-nameyear.bst  will be used to                   %%
	%%  create a .BBL file for submission.                     %%
	%%                                                         %%
	%%  Note that the displayed Bibliography will not          %%
	%%  necessarily be rendered by Latex exactly as specified  %%
	%%  in the online Instructions for Authors.                %%
	%%                                                         %%
	%%  MR numbers will be added by VTeX.                      %%
	%%                                                         %%
	%%  Use \cite{...} to cite references in text.             %%
	%%                                                         %%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	%% if your bibliography is in bibtex format, uncomment commands:
	\bibliographystyle{imsart-nameyear} % Style BST file
	\bibliography{bibliography.bib}       % Bibliography file (usually '*.bib')
	
	
	
	\appendix
	
	\section{Overview of Appendix}
	\begin{itemize}
		\item Appendix \ref{section:proof:lemma:hybrid_hilbert_space} proves that our defined product space is a separable Hilbert space.
		\item Appendix \ref{section:proof:section:sub:iterative}  provides the proofs for the core problem formulations and computational schemes of the iterative steps for estimating the PLS direction and performing residualization.
		\item Appendix \ref{section:proof:section:sub:tucker} provides proof of Theorem \ref{theorem:population_PLS}, 
		demonstrating that the population-level version of the optimization problem solved at each step is well-defined.
	\end{itemize}
	\section{Proof of Lemma \ref{lemma:hybrid_hilbert_space}}\label{section:proof:lemma:hybrid_hilbert_space}
	\begin{proof}
		Both $\mathbb{L}^2[0,1]$ and $\mathbb{R}^p$ are Hilbert spaces, with their inner products, norms, and metrics corresponding to the terms defined in equation \eqref{eq: hybrid inner product}. A finite Cartesian product of Hilbert spaces, equipped with an $\ell_2$ norm of the component metrics (which is our metric), is also a Hilbert space.
		Furthermore, both of 	 $\mathbb{L}^2[0,1]$ and $\mathbb{R}^p$ are separable.
		For $\mathbb{L}^2[0,1]$, the set of all polynomials with rational coefficients is a countable and dense subset.
		For $\mathbb{R}^p$,  the set of all vectors with rational components forms a countable and dense subset.
		A finite Cartesian product of separable spaces is also separable.
		Therefore $\mathbb{H} = \bigl( \mathbb{L}^2[0,1] \bigr)^K \times \mathbb{R}^p$ is separable.
	\end{proof}
	\section{Proof of Section \ref{section:sub:iterative}}\label{section:proof:section:sub:iterative}
	
	
	
	This section provides the proofs for the core problem formulations and computational schemes of the iterative steps for estimating the PLS direction and performing residualization.
	Appendices \ref{section:proof:proposition:linear_regul_system_unregularized} and \ref{section:proof:proposition:eigen_regul} derive the eigenproblem formulations for unregularized and regularized PLS direction estimation, respectively.
	Appendices \ref{section:proof:proposition:linear_regul} and \ref{section:proof:proposition:closed_form_orthgonalization} provide proofs for the simple computation schemes for regularized PLS direction estimation and residualization, respectively.	
	\subsection{Proof of Proposition \ref{proposition:eigen_noregul}  }\label{section:proof:proposition:linear_regul_system_unregularized}
	\begin{proof}
		Let $\xi = (\xi_1, \ldots, \xi_K, \boldsymbol{\zeta}) \in \mathbb{H}$. Assume that each functional component $\xi_j \in \mathbb{L}^2([0,1])$ lies in the span of the basis functions ${b_1(t), \ldots, b_M(t)}$ and can be written as
		\begin{equation*}
			\xi_j(t) := 
			\sum_{m=1}^M d_{jm} \, b_m(t), \quad j = 1, \ldots, K,
		\end{equation*}
		with coefficient vectors $\boldsymbol{\gamma}_j := (d_{j1}, \ldots, d_{jM})^\top \in \mathbb{R}^M$.
		Then, 
		the empirical covariance is
		computed as follows:
		\begin{align*}
			\widehat{\textnormal{Cov}} (
			\langle \widetilde{W}, \xi  \rangle_{\mathbb{H}}, Y 
			) 
			&=  %1
			\frac{1}{n} 
			\sum 
			\limits_{i=1}^n 
			y_i
			\langle \widetilde{W}_i, \xi \rangle_\mathbb{H} \\
			&= %2
			\frac{1}{n} 
			\sum \limits_{i=1}^n
			y_i
			\biggl(
			\sum_{j=1}^K
			\langle \widetilde{X}_{ij}, \xi_j \rangle  
			+
			\mathbf{Z}_i^\top 
			\boldsymbol{\zeta} 
			\biggr)
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&= %3
			\frac{1}{n}
			\sum
			\limits_{i=1}^n 
			y_i
			\biggl( \sum \limits_{k=1}^K \int_0^1 \widetilde{X}_{ij}(t) \, \xi_j(t) \, dt \biggr)
			+
			\frac{1}{n}
			\sum
			\limits_{i=1}^n
			y_i
			( \mathbf{Z}_i^\top \boldsymbol{\zeta} )
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=  
			\frac{1}{n} \sum \limits_{i=1}^n 
			y_i
			\biggl\{ 
			\sum \limits_{k=1}^K \int_0^1
			\biggl(
			\sum_{m=1}^M \theta_{ijm} b_{m}(t) 
			\biggr) 
			\biggl(
			\sum_{m'=1}^M d_{jm'} b_{m'}(t) 
			\biggr) 
			dt
			\biggr\}  
			+
			\frac{1}{n}
			\mathbf{y}^\top
			Z \boldsymbol{\zeta} 
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=  
			\frac{1}{n} \sum \limits_{i=1}^n 
			y_i
			\biggl\{ 
			\sum \limits_{k=1}^K 
			\sum_{m=1}^M
			\sum_{m'=1}^M  
			\theta_{ijm}
			d_{jm'}
			\int_0^1
			b_{m}(t) 
			b_{m'}(t) 
			dt
			\biggr\}  
			+
			\frac{1}{n}
			\mathbf{y}^\top
			Z \boldsymbol{\zeta} 
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=  
			\sum \limits_{k=1}^K 
			\biggl\{
			\frac{1}{n} \sum \limits_{i=1}^n 
			y_i
			(
			\boldsymbol{\theta}_{ij}^\top
			B
			\boldsymbol{\gamma}_j
			)  
			\biggr\}
			+
			\frac{1}{n}
			\mathbf{y}^\top
			Z \boldsymbol{\zeta}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=  
			\frac{1}{n}
			\biggl(
			\sum \limits_{k=1}^K 
			\mathbf{y}^\top
			\Theta_{j}
			B
			\boldsymbol{\gamma}_j
			+
			\mathbf{y}^\top
			Z \boldsymbol{\zeta} 
			\biggr)
		\end{align*}	
		Building on this computation, the squared empirical covariance is
		expressed as the quadratic form involving the matrix $V$ defined in \eqref{def:V_matrix}:
		%	The objective function matches that of \eqref{eq: Regularized generalized rayleigh quotient equation}, takin
		\begin{align*}
			\widehat{\textnormal{Cov}}^2 (
			\langle \widetilde{W}, \xi  \rangle_{\mathbb{H}}, Y 
			)  
			&= 
			\frac{1}{n^2}
			\biggl(
			\sum \limits_{k=1}^K 
			\mathbf{y}^\top
			\Theta_{j}
			B
			\boldsymbol{\gamma}_j
			+
			\mathbf{y}^\top
			Z \boldsymbol{\zeta} 
			\biggr)^2
			%%%%%%%%%%%%%%%%%%%%
			\\	&=
			\frac{1}{n^2} 
			(
			\mathbf{y}^\top
			\Theta
			\mathbb{B}  
			\boldsymbol{\xi}
			)^2
			\\
			&	= 
			\frac{1}{n^2} 
			\boldsymbol{\xi}^\top
			(\mathbb{B} \Theta^\top \mathbf{y})(\mathbb{B} \Theta^\top \mathbf{y})^\top 
			\boldsymbol{\xi}
			\\&=
			\boldsymbol{\xi}^\top
			V 
			\boldsymbol{\xi}.
			\numberthis \label{covariance_squares_as_quadratic_form}
		\end{align*}
		The squared norm of $\xi$ in the hybrid Hilbert space $\mathbb{H}$ 
		%can be expressed as the inner product on $(\mathbb{R}^M)^K \times \mathbb{R}^p$ defined in Lemma~\ref{lemma:valid_inner_product_tuple} as follows:
		is computed as follows:
		\begin{align*}
			\langle {\xi},{\xi} \rangle_{\mathbb{H}}
			&=
			\sum_{j=1}^K
			\int_0^1
			\xi_j(t) \, \xi_j(t)
			\, dt
			+
			\boldsymbol{\zeta}^\top \boldsymbol{\zeta}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\sum \limits_{j=1}^K \int_0^1
			\biggl(
			\sum_{m=1}^M d_{jm} b_{m}(t) 
			\biggr) 
			\biggl(
			\sum_{m'=1}^M d_{jm'} b_{m'}(t) 
			\biggr) 
			dt
			+
			\boldsymbol{\zeta}^\top \boldsymbol{\zeta}  
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\sum \limits_{j=1}^K 
			\sum_{m=1}^M
			\sum_{m'=1}^M
			d_{jm}
			d_{jm'}
			\int_0^1
			b_{m}(t) 
			b_{m'}(t) 
			dt
			+
			\boldsymbol{\zeta}^\top \boldsymbol{\zeta}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\sum \limits_{j=1}^K 
			\boldsymbol{\gamma}_j^\top
			B
			\boldsymbol{\gamma}_j
			+
			\boldsymbol{\zeta}^\top \boldsymbol{\zeta},
			%%%%%%%%%%%%%
			\\&=
			\boldsymbol{\xi}^\top \mathbb{B} \boldsymbol{\xi}
			\numberthis \label{unit_norm_as_quadform}
		\end{align*}
		Therefore, the covariance maximization problem \eqref{def:maximizer_squared_empirical_cov} is equivalent to the generalized Raleigh quotient \eqref{eq: Unregularized generalized rayleigh quotient equation}. This completes  the proof of Proposition~\ref{proposition:eigen_noregul}.
		
		\subsection{Proof of Proposition \ref{proposition:eigen_regul} }\label{section:proof:proposition:eigen_regul}
		The penalization term 
		$\sum \limits_{j=1}^K \lambda_j 
		\operatorname{PEN}(\xi_K)$
		can be written in matrix form as:
		\begin{align*}
			\sum \limits_{j=1}^K 
			\lambda_j
			\operatorname{PEN}({\xi}_j)
			&=
			\sum \limits_{j=1}^K 
			\lambda_j
			\int_0^1 \bigl\{ \hat{\xi}_j^{\prime\prime}(t) \bigr\}^2 dt
			\\&=
			\sum \limits_{j=1}^K 
			\lambda_j
			\int_0^1 \bigl\{ \sum_{l=1}^M \theta_{ijl} \, b_l^{\prime \prime}(t) \bigr\}^2 dt
			\\&	 =
			%%%%%%%%%%%%%%%%%%%
			\sum \limits_{j=1}^K 
			\lambda_j
			\sum_{l=1}^M
			\sum_{m=1}^M
			\theta_{ijl} \theta_{ijm}
			\int_0^1
			\, b_l^{\prime \prime}(t) 
			b_m^{\prime \prime}(t)
			dt
			\\&	 =
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\sum \limits_{j=1}^K
			\lambda_j
			\boldsymbol{\gamma}_j^\top
			B^{\prime \prime} 
			\boldsymbol{\gamma}_j
			%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&= 
			\boldsymbol{\xi}^{\top} \Lambda \mathbb{B}^{\prime \prime} \boldsymbol{\xi},
			\numberthis \label{pen_as_matrix}
		\end{align*}
		where $\boldsymbol{\xi}$ is the concatenated coefficient vector form of $\xi$ as defined in \eqref{def:maximizer_euclidean_form}, $\mathbb{B}^{\prime\prime}$ is defined in \eqref{def:gram_block}, and $\Lambda$ is defined in \eqref{def:Lambda}. 
		
		Combining this with \eqref{unit_norm_as_quadform}, 
		we can compute
		\begin{equation*}
			\| \xi  \|_\mathbb{H} +  \sum \limits_{j=1}^K \lambda_j 
			\operatorname{PEN}(\xi_K) 
			= 
			\boldsymbol{\xi}^\top \mathbb{B} \boldsymbol{\xi}
			+ 
			\boldsymbol{\xi}^{\top} \Lambda \mathbb{B}^{\prime \prime} \boldsymbol{\xi}
			=
			\boldsymbol{\xi}^\top 
			(
			\mathbb{B}
			+
			\Lambda \mathbb{B}^{\prime \prime})
			\boldsymbol{\xi}.
		\end{equation*}
		This computation along with \eqref{covariance_squares_as_quadratic_form} implies that the covariance maximization problem  
		\eqref{def:maximizer_squared_empirical_cov_reg} is equivalent to the generalized Raleigh quotient \eqref{eq: Regularized generalized rayleigh quotient equation}.  This completes  the proof of Proposition \ref{proposition:eigen_regul}.
		
		
		
		
		
	\end{proof}
	
	\subsection{Proof of Proposition  \ref{proposition:linear_regul}}\label{section:proof:proposition:linear_regul}
	\begin{proof}
		The optimization problem 
		\eqref{eq: Regularized generalized rayleigh quotient equation} can be written as
		\begin{align*}
			\max_{\boldsymbol{\gamma}_1, \ldots, \boldsymbol{\gamma}_K \in \mathbb{R}^M,\;
				\boldsymbol{\zeta} \in \mathbb{R}^p}~&
			\frac{1}{n^2}
			\left(
			\sum_{j=1}^K 
			\mathbf{y}^\top
			\Theta_j
			B  
			\boldsymbol{\gamma}_j
			+
			\mathbf{y}^\top
			Z \boldsymbol{\zeta} 
			\right)^2,
			\\
			\text{subject to}\quad&
			\sum_{j=1}^K 
			\boldsymbol{\gamma}_j^\top
			(B + \lambda_j B^{\prime\prime})
			\boldsymbol{\gamma}_j
			+
			\boldsymbol{\zeta}^\top \boldsymbol{\zeta} = 1.
		\end{align*}
		
		Let $\mathbf{u}_j := B \Theta_j^\top \mathbf{y} \in \mathbb{R}^M$ and $\mathbf{v} := Z^\top \mathbf{y} \in \mathbb{R}^p$. These are fixed problem data. Then the objective becomes
		\[
		\frac{1}{n^2}
		\left(
		\sum_{j=1}^K 
		\mathbf{u}_j^\top \boldsymbol{\gamma}_j
		+
		\mathbf{v}^\top \boldsymbol{\zeta}
		\right)^2.
		\]
		The objective function is continuous. The constraint defines the boundary of an ellipsoid in a finite-dimensional Euclidean space, the feasible set is compact. By the Weierstrass Extreme Value Theorem (see, e.g., Theorem 2.3.1 in \citealt{bazaraaNonlinearProgrammingTheory2006}), a global maximizer exists. The constraint function is continuously differentiable and its gradient vanishes only at the origin, which is not feasible. 
		Thus the gradient  is nonzero at all feasible points.
		Hence, the Linear Independence Constraint Qualification (LICQ) holds, and the Karush-Kuhn-Tucker (KKT) conditions are necessary for local optimality (see, e.g., Theorem 5.3.1 in \citealt{bazaraaNonlinearProgrammingTheory2006}).
		
		Define the Lagrangian:
		\[
		\mathcal{L}(\{\boldsymbol{\gamma}_j\}, \boldsymbol{\zeta}, \mu) 
		:= 
		\left( \sum_{j=1}^K \mathbf{u}_j^\top  \boldsymbol{\gamma}_j + \mathbf{v}^\top \boldsymbol{\zeta} \right)^2
		- 
		\mu 
		\left[
		\sum_{j=1}^K \boldsymbol{\gamma}_j^\top (B + \lambda_j B^{\prime\prime}) \boldsymbol{\gamma}_j + \boldsymbol{\zeta}^\top \boldsymbol{\zeta} - 1
		\right].
		\]
		
		Let $s := \dfrac{2}{n^2} \left( \sum_{j=1}^K \mathbf{u}_j^\top \boldsymbol{\gamma}_j + \mathbf{v}^\top \boldsymbol{\zeta} \right)$. The KKT conditions require that:
		\begin{align}
			\nabla_{\boldsymbol{\gamma}_j} \mathcal{L} &= 
			s \mathbf{u}_j - 2\mu (B + \lambda_j B^{\prime\prime}) \boldsymbol{\gamma}_j = 0, \quad \text{for } j = 1, \ldots, K, \label{eq:lag_grad_gamma} \\
			\nabla_{\boldsymbol{\zeta}} \mathcal{L} &= 
			s \mathbf{v} - 2\mu \boldsymbol{\zeta} = 0. \label{eq:lag_grad_zeta}
		\end{align}
		
		From \eqref{eq:lag_grad_gamma} and \eqref{eq:lag_grad_zeta}, we have
		\[
		(B + \lambda_j B^{\prime\prime}) \boldsymbol{\gamma}_j = \frac{s}{2\mu} \mathbf{u}_j, \quad
		\boldsymbol{\zeta} = \frac{s}{2\mu} \mathbf{v}, \quad \text{for } j = 1, \ldots, K.
		\]
		This implies that for any local maximizer, there exists $c \ne 0$ such that 
		\[
		\boldsymbol{\gamma}_j = c (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j, \quad
		\boldsymbol{\zeta} = c \mathbf{v}, \quad \text{for } j = 1, \ldots, K.
		\]
		Since we assumed in Section~\ref{sec: finite basis approximation} that the functions $\{b_m(t)\}$ and their second derivatives are linearly independent, both $B$ and $B^{\prime\prime}$ are positive definite (see, for example, Theorem 273 of \citealp{gockenbachFiniteDimensionalLinearAlgebra2010}). As positive definiteness is preserved under conic combinations, $B + \lambda_j B^{\prime\prime}$ is also positive definite. 
		
		A local maximizer must also be primal feasible. Substituting the conditions above in to the primal constraint:
		\begin{align*}
			\sum_{j=1}^K 
			\left( c (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j \right)^\top 
			(B + \lambda_j B^{\prime\prime}) 
			\left( c (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j \right)
			+ c^2 \mathbf{v}^\top \mathbf{v} &= 1 \\
			\Rightarrow 
			c^2 \left( \sum_{j=1}^K \mathbf{u}_j^\top (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j + \mathbf{v}^\top \mathbf{v} \right) &= 1.
		\end{align*}
		
		Solving for $c^2$ gives:
		\[
		c^2 = \left( \sum_{j=1}^K \mathbf{u}_j^\top (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j + \mathbf{v}^\top \mathbf{v} \right)^{-1}.
		\]
		
		Hence, the unique maximizer (up to sign) is:
		\[
		\boldsymbol{\gamma}_j^\ast = \frac{1}{\sqrt{q}} (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j, \quad
		\boldsymbol{\zeta}^\ast = \frac{1}{\sqrt{q}} \mathbf{v},
		\]
		where
		\[
		q := \sum_{j=1}^K \mathbf{u}_j^\top (B + \lambda_j B^{\prime\prime})^{-1} \mathbf{u}_j + \mathbf{v}^\top \mathbf{v}.
		\]
		
		This completes the proof of Proposition \ref{section:proof:proposition:linear_regul}.
		
		
	\end{proof}
	
	
	\subsection{Proof of Lemma \ref{proposition:closed_form_orthgonalization} }\label{section:proof:proposition:closed_form_orthgonalization}
	For notational simplicity, we omit the iteration superscript $[l]$.
	Recall from  \eqref{def:plsscore} that
	$\hat{\rho}_i
	=
	\langle \widetilde{W}_i, \, \hat{\xi} \rangle_{\mathbb{H}}$.
	Using the basis expansion coefficient notation from \eqref{def:truncated_zeta_and_delta}, \eqref{def:coef_vector_form}, \eqref{def:theta}, and \eqref{def:B_matrix}, the full vector of PLS scores $\hat{\boldsymbol{\rho}}  = (\hat{\rho}_1, \ldots, \hat{\rho}_n)^\top$ 
	is computed through the following matrix multiplication:
	\begin{equation}\label{score_inner_product_all_n}
		\hat{\boldsymbol{\rho}}^\top = \sum_{k=1}^K ( \hat{\boldsymbol{\gamma}}_j )^\top B \, \Theta_j^\top + \boldsymbol{\zeta}^\top \mathbf{Z}^\top.
	\end{equation}
	The least square criterion   can be decomposed as follows:
	\begin{align*}
		\operatorname{SSE}( \delta) &= \sum_{i=1}^n \langle \widetilde{W}_i^{[l]} - \hat{\rho}_i^{[l]}\, \delta, \, \widetilde{W}_i^{[l]} - \hat{\rho}_i^{[l]}\, \delta \rangle_{\mathbb{H}} \\
		&= 
		\underbrace{
			\sum_{i=1}^n \langle \widetilde{W}_i^{[l]}, \, \widetilde{W}_i^{[l]} \rangle_{\mathbb{H}}}_{A} 
		\underbrace{
			-2 \sum_{i=1}^n \hat{\rho}_i^{[l]} \langle \delta, \, \widetilde{W}_i^{[l]} \rangle_{\mathbb{H}} 
		}_{B_1(\boldsymbol{\delta})}
		\underbrace{
			+ 
			(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}})
			\langle \delta, \, \delta \rangle_{\mathbb{H}}
		}_{B_2(\boldsymbol{\delta})}
	\end{align*}
	\begin{comment}
		\begin{align*}
			\operatorname{PENSSE}( \delta) &= \sum_{i=1}^n \langle \widetilde{W}_i^{[l]} - \hat{\rho}_i^{[l]}\, \delta, \, \widetilde{W}_i^{[l]} - \hat{\rho}_i^{[l]}\, \delta \rangle_{\mathbb{H}} + \tau \sum_{j=1}^K\operatorname{PEN}(\delta_j) \\
			&= 
			\underbrace{
				\sum_{i=1}^n \langle \widetilde{W}_i^{[l]}, \, \widetilde{W}_i^{[l]} \rangle_{\mathbb{H}}}_{A} 
			\underbrace{
				-2 \sum_{i=1}^n \hat{\rho}_i^{[l]} \langle \delta, \, \widetilde{W}_i^{[l]} \rangle_{\mathbb{H}} 
			}_{B_1(\boldsymbol{\delta})}
			\underbrace{
				+ 
				(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}})
				\langle \delta, \, \delta \rangle_{\mathbb{H}}
			}_{B_2(\boldsymbol{\delta})}
			\underbrace{
				+ 
				\tau \sum_{j=1}^K\operatorname{PEN}(\delta_j)
			}_{B_3(\boldsymbol{\delta})}
		\end{align*}
	\end{comment}
	Here, part $A$ does not contain $\delta$, so it does not contribute to the minimization problem.
	
	Let $\boldsymbol{\delta}:= (\pi_1^\top, \ldots, \pi_K^\top, \boldsymbol{\chi}^\top )^\top$ be the $MK+p$-dimensional concatenated vector of basis coefficients and the scalar part of $\boldsymbol{\delta}$. 
	%Abusing notation, we treat $\operatorname{PENSSE}$ as a function of $\boldsymbol{\delta}$.
	Abusing notation, we treat $\operatorname{SSE}$ as a function of $\boldsymbol{\delta}$.
	Next, We will now demonstrate that the combined term $
	%B_1(\boldsymbol{\delta}) + B_2(\boldsymbol{\delta}) + B_3(\boldsymbol{\delta})
	B_1(\boldsymbol{\delta}) + B_2(\boldsymbol{\delta})
	$ is a quadratic function of $\boldsymbol{\delta}$, by expanding its component functions:
	\begin{align*}
		B_1(\boldsymbol{\delta}) &= 
		- 2\sum_{i=1}^n \hat{\rho}_i  \langle \delta, \, \widetilde{W}_i  \rangle_{\mathbb{H}} 
		=
		- 2\hat{\boldsymbol{\rho}}^\top 
		\biggl(
		\sum_{j=1}^K \Theta_j B \boldsymbol{\pi}_j
		+\mathbf{Z} \boldsymbol{\chi}
		\biggr),
		%%%%%%%%%%%%%%%%%%%%%%%
		\\ 
		B_2(\boldsymbol{\delta})
		&=
		(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}})
		\langle \delta, \, \delta \rangle_{\mathbb{H}}
		=
		(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}}) 
		\biggl(
		\sum_{j=1}^K \pi_j^\top B \pi_j
		+\boldsymbol{\chi}^\top \boldsymbol{\chi}
		\biggr),
		\\
		%	B_3(\boldsymbol{\delta}) &= \tau \sum_{j=1}^K\operatorname{PEN}(\delta_j) = \tau \boldsymbol{\delta} B^{''} \boldsymbol{\delta},
	\end{align*}
	%where the expansion of $B_3(\boldsymbol{\delta})$ leverages the computation in \eqref{pen_as_matrix}.
	Now we compute the gradients:
	\begin{align*}
		\nabla_{\boldsymbol{\pi}_j}
		B_1(\boldsymbol{\delta}) 
		&= -2
		B
		\Theta_j^\top
		\hat{\boldsymbol{\rho}}  ,~
		j = 1, \ldots, K,
		\quad
		\nabla_{\boldsymbol{\chi}}
		B_1(\boldsymbol{\delta}) 	  
		= -2\mathbf{Z}^\top  \hat{\boldsymbol{\rho}},
		%
		\\
		\nabla_{\boldsymbol{\pi}_j}
		B_2(\boldsymbol{\delta}) 
		&= 
		2
		(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}}) 
		B 	\boldsymbol{\pi}_j
		,~
		j = 1, \ldots, K,
		\quad
		\nabla_{\boldsymbol{\chi}}
		B_2(\boldsymbol{\delta}) 
		=
		2(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}})\boldsymbol{\chi} ,
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		%\\ \nabla_{\boldsymbol{\pi}_j} B_3(\boldsymbol{\delta})  &=  2 \tau B^{\prime \prime }	\boldsymbol{\pi}_j ,~ j = 1, \ldots, K, \quad \nabla_{\boldsymbol{\chi}} B_3(\boldsymbol{\delta})  =0.
	\end{align*}
	The Hessian of 
	$\operatorname{SSE}( \boldsymbol{\delta})$ is then given by 
	$
	2 	(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}})  B.
	$
	\begin{comment}
		The Hessian of 
		$\operatorname{PENSSE}( \boldsymbol{\delta})$ is then given by 
		$
		2 	(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}})  B
		+
		2
		\tau
		B^{\prime \prime}.
		$
	\end{comment}
	%Since we assumed in Section \ref{sec: finite basis approximation} that both $B'$ and $B''$ are positive definite, $\operatorname{PENSSE}( \delta)$ is convex. Consequently, the gradient vanishes at its unique minimizer:
	Since   $B$ and is positive definite, $\operatorname{SSE}( \delta)$ is convex. Consequently, the gradient vanishes at its unique minimizer:
	\begin{align*}
		\nabla_{\boldsymbol{\pi}_j}	\operatorname{SSE}( \hat{\boldsymbol{\delta}} )
		&	=
		-2
		B
		\Theta_j^\top
		\hat{\boldsymbol{\rho}}
		+
		2	(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}}) 
		B 	\hat{\boldsymbol{\pi}}_j
		=
		0,~
		j = 1, \ldots, K,
		\\
		\nabla_{\boldsymbol{\chi} }	\operatorname{SSE}( \hat{\boldsymbol{\delta}} )
		&=
		-2\mathbf{Z}^\top  \hat{\boldsymbol{\rho}}
		+
		2 (\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}}) \hat{\boldsymbol{\chi} }
		=
		0,
	\end{align*}
	\begin{comment}
		\begin{align*}
			\nabla_{\boldsymbol{\pi}_j}	\operatorname{PENSSE}( \hat{\boldsymbol{\delta}} )
			&	=
			-2
			B
			\Theta_j^\top
			\hat{\boldsymbol{\rho}}
			+
			2	(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}}) 
			B 	\hat{\boldsymbol{\pi}}_j
			+
			2
			B^{\prime \prime }	\hat{\boldsymbol{\pi}}_j
			=
			0,~
			j = 1, \ldots, K,
			\\
			\nabla_{\boldsymbol{\chi} }	\operatorname{PENSSE}( \hat{\boldsymbol{\delta}} )
			&=
			-2\mathbf{Z}^\top  \hat{\boldsymbol{\rho}}
			+
			2 (\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}}) \hat{\boldsymbol{\chi} }
			=
			0,
		\end{align*}
	\end{comment}
	providing the following closed-form solution:
	\begin{comment}
		\begin{equation*}
			\hat{\boldsymbol{\pi}}_j\\
			=
			\bigl\{
			(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}})  B
			+
			\tau
			B^{\prime \prime}
			\bigr\}^{-1}
			B
			\Theta_j^\top
			\hat{\boldsymbol{\rho}}
			~
			j = 1, \ldots, K,
			%%%%%%%%%%%%%%%%%%%%%%%%
			\quad
			\hat{\boldsymbol{\chi} }
			=
			\frac{1}{(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}})}
			\mathbf{Z}^\top  \hat{\boldsymbol{\rho}}.
		\end{equation*}
	\end{comment}
	\begin{equation*}
		\hat{\boldsymbol{\pi}}_j\\
		=
		\bigl\{
		(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}})  B
		\bigr\}^{-1}
		B
		\Theta_j^\top
		\hat{\boldsymbol{\rho}}
		=
		\frac{1}{\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}}} \Theta_j^\top
		\hat{\boldsymbol{\rho}}
		~
		j = 1, \ldots, K,
		%%%%%%%%%%%%%%%%%%%%%%%%
		\quad
		\hat{\boldsymbol{\chi} }
		=
		\frac{1}{(\hat{\boldsymbol{\rho}}^\top \hat{\boldsymbol{\rho}})}
		\mathbf{Z}^\top  \hat{\boldsymbol{\rho}}.
	\end{equation*}
	\begin{comment}
		The solution to the optimization problem \eqref{def:argmin_pensse}, denoted
		$\hat{\delta}^{[l]}   \in \widetilde{\mathbb{H}}$, has basis coefficients
		\begin{equation*}
			\hat{\boldsymbol{\pi}}_j^{[l]}
			=
			\frac{1}{\| 	\hat{\boldsymbol{\rho}}^{[l]}\|_2^2}
			\Theta_j^{[l] \top}
			\hat{\boldsymbol{\rho}}^{[l]}
			,
			~
			j = 1, \ldots, K,
		\end{equation*}
		and scalar parts given by:
		\begin{equation*}
			\hat{\boldsymbol{\chi} }^{[l]}
			=
			\frac{1}{\| 	\hat{\boldsymbol{\rho}}^{[l]}\|_2^2}
			\mathbf{Z}^{[l] \top}  \hat{\boldsymbol{\rho}}^{[l]}.
		\end{equation*}
	\end{comment}
	Expanding with respect to these coefficients, we obtain
	\begin{equation*}
		\delta^{[l]}
		= 
		\frac{1}{\| 	\hat{\boldsymbol{\rho}}^{[l]}\|_2^2}
		\sum_{i=1}^n 
		\widehat{\rho}_i^{[l]}
		\widetilde{W}_i^{[l]}.
	\end{equation*}
	On the other hand since $\mathbf{y}^{[l]}$ and  
	$\hat{\boldsymbol{\rho}}^{[l]}$ are zero-mean,
	it is well known that the least square estimate of linear regression coefficient is compuated as $\hat{\nu}^{[l]} =  
	\frac{
		\mathbf{y}^{[l] \top }  \hat{\boldsymbol{\rho}}^{[l]}
	}{
		\| \hat{\boldsymbol{\rho}}^{[l]} \|_2^2}$.
	This completes the proof of Lemma \ref{proposition:closed_form_orthgonalization}.
	
	\section{Proof of Lemma \ref{lemma:recursive}}\label{section:proof:lemma:recursive}
	\begin{proof}We prove the lemma using mathematical induction.
		
		\medskip
		\noindent
		\textit{Base case ($l=1$).}~
		For $l=1$, the lemma states $\widehat{\rho}_i^{[1]} = \langle W_i, \widehat{\iota}^{[1]} \rangle_{\mathbb{H}}$. Since  $\widehat{\iota}^{[1]} := \widehat{\xi}^{[1]}$ and $W^{[1]}_i = W_i$, the base case holds.
		
		\medskip
		\noindent
		\textit{Inductive steps.}~		
		Assume   $\widehat{\rho}_i^{[u]} = \langle W_i, \widehat{\iota}^{[u]} \rangle_{\mathbb{H}}$ for $u =1 , \ldots, l$. We want to show that $\widehat{\rho}_i^{[l+1]} = \langle W_i, \widehat{\iota}^{[l+1]} \rangle_{\mathbb{H}}$.
		Recall from  \eqref{def:plsscore} and Lemma \ref{proposition:closed_form_orthgonalization} that we have
		\begin{equation}
			\widehat{\rho}_i^{[l+1]} = \langle \widetilde{W}_i^{[l+1]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}},
			\quad
			\widetilde{W}_i^{[l+1]} = \widetilde{W}_i^{[l]} - \widehat{\rho}_i^{[l]} \widehat{\delta}^{[l]}
		\end{equation}
		Combining these two, we have
		\begin{equation}\label{rholp1}
			\widehat{\rho}_i^{[l+1]} = \langle \widetilde{W}_i^{[l]} - \widehat{\rho}_i^{[l]} \widehat{\delta}^{[l]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}} 
			=
			\langle \widetilde{W}_i^{[l]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}} - \widehat{\rho}_i^{[l]} \langle \widehat{\delta}^{[l]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}}.
		\end{equation}
		Next,
		we can express $\widetilde{W}_i^{[l]}$ in terms of the original predictor $W_i$ by recursively applying  Lemma \ref{proposition:closed_form_orthgonalization}:
		$$ \widetilde{W}_i^{[l]} = \widetilde{W}_i^{[l-1]} - \widehat{\rho}_i^{[l-1]} \widehat{\delta}^{[l-1]} = \ldots = W_i - \sum_{u=1}^{l-1} \widehat{\rho}_i^{[u]} \widehat{\delta}^{[u]} $$
		Substituting this into our equation \eqref{rholp1}, we have:
		\begin{align*}
			\widehat{\rho}_i^{[l+1]} 
			&= \left\langle W_i - \sum_{u=1}^{l-1} \widehat{\rho}_i^{[u]} \widehat{\delta}^{[u]}, \widehat{\xi}^{[l+1]} \right\rangle_{\mathbb{H}} - \widehat{\rho}_i^{[l]} \langle \widehat{\delta}^{[l]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}} 
			\\&=
			\langle W_i, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}} - \sum_{u=1}^{l-1} \widehat{\rho}_i^{[u]} \langle \widehat{\delta}^{[u]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}} - \widehat{\rho}_i^{[l]} \langle \widehat{\delta}^{[l]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}}
			\\&=
			\langle W_i, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}} - \sum_{u=1}^{l} \widehat{\rho}_i^{[u]} \langle \widehat{\delta}^{[u]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}}
			\\& \overset{(i)}{=}
			\langle W_i, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}} - \sum_{u=1}^{l} \langle \widehat{\delta}^{[u]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}} \langle W_i, \widehat{\iota}^{[u]} \rangle_{\mathbb{H}}
			%%%%%%%%%%%%%%%%%%%%%%%%
			\\& = 
			\langle W_i, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}}
			-
			\left\langle W_i,   \sum_{u=1}^{l} \langle \widehat{\delta}^{[u]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}} \widehat{\iota}^{[u]} \right\rangle_{\mathbb{H}} 
			%%%%%%%%%%%%%%%%%%%%%%%%
			\\& = 
			\left\langle W_i, \widehat{\xi}^{[l+1]} - \sum_{u=1}^{l} \langle \widehat{\delta}^{[u]}, \widehat{\xi}^{[l+1]} \rangle_{\mathbb{H}} \widehat{\iota}^{[u]} \right\rangle_{\mathbb{H}} 
			\\&= \widehat{\rho}_i^{[l+1]}.
		\end{align*}
		where step $(i)$ uses the inductive hypothesis $\langle W_i, \widehat{\iota}^{[u]} \rangle_{\mathbb{H}} = \widehat{\rho}_i^{[u]}$ for all $u \leq l$.
		By the principle of mathematical induction, the lemma holds for all $l \ge 1$. This completes the proof of Lemma \ref{lemma:recursive}.
	\end{proof}
	
	\section{Proof of Section \ref{section:sub:tucker}}\label{section:proof:section:sub:tucker}
	This section provides proof of Theorem \ref{theorem:population_PLS}, 
	demonstrating that the population-level version of the optimization problem solved at each step is well-defined, and 
	proofs of the intermediate lemmas that support the main theorem.
	\subsection{Proof of  Lemma \ref{lemma:cross_cov_functional} }\label{section:proof:lemma:cross_cov_functional}
	\jongmin{An alternative proof to the compactness of $\mathcal{U}$; rather than directly showing $\mathcal{U}$ is compact, we show $\mathcal{C}_{YW}$ is compact}
	\begin{proof} 
		Any bounded linear functional from a Hilbert space to  $\mathbb{R}$  is a compact operator.
		This is because the image of any bounded set under such a functional is a bounded set in $\mathbb{R}$,
		and by Bolzano-Weierstrass Theorem, every bounded sequence of real numbers has a convergent subsequence.
		Therefore,  to show that $\mathcal{C}_{YW}$ is a compact operator, it suffices to show that 
		$\mathcal{C}_{YW}$ is a bounded linear functional.
		
		\medskip
		\noindent
		\textit{Linearity.}~	  
		The operator $\mathcal{C}_{YW}: \mathbb{H} \rightarrow \mathbb{R}$ is defined as 
		$\mathcal{C}_{YW} h = \langle \Sigma_{YW}, h \rangle_\mathbb{H}$.
		For any $h_1, h_2 \in \mathbb{H}$ and scalar $c \in \mathbb{R}$, the linearity of the inner product implies: \begin{align*} \mathcal{C}_{YW}(h_1 + h_2) &= \langle \Sigma_{YW}, h_1 + h_2 \rangle_\mathbb{H} = \langle \Sigma_{YW}, h_1 \rangle_\mathbb{H} + \langle \Sigma_{YW}, h_2 \rangle_\mathbb{H} = \mathcal{C}_{YW} h_1 + \mathcal{C}_{YW} h_2,
			\\ 
			\mathcal{C}_{YW}(c h) &= \langle \Sigma_{YW}, c h \rangle_\mathbb{H} = c \langle \Sigma_{YW}, h \rangle_\mathbb{H} = c \, \mathcal{C}_{YW} h. \end{align*}
		Thus, $\mathcal{C}_{YW}$ is a linear functional. 
		
		\medskip
		\noindent
		\textit{Boundedness.}~ 
		To show that $\mathcal{C}_{YW}$ is bounded, we need to find a finite constant $M$ such that $|\mathcal{C}_{YW} h| \leq M \|h\|_\mathbb{H}$ for all $h \in \mathbb{H}$. By the Cauchy-Schwarz inequality, we have: \begin{equation*} 
			|\mathcal{C}_{YW} h| = |\langle \Sigma_{YW}, h \rangle_\mathbb{H}| \leq \|\Sigma_{YW}\|_\mathbb{H} \|h\|_\mathbb{H}. 
		\end{equation*}
		%%%%%%%%%%%%%%%%
		Now, we must verify that $\|\Sigma_{YW}\|_\mathbb{H}$ is finite. 
		Let $\mu([0,1])$ denote a Lebesgue measure of $[0,1]$, and let $T = \max_{k=1, \ldots, K} \, \mu([0,1])$.
		The squared norm of $\Sigma_{YW}$   is given by:
		\begin{equation*}
			\|\Sigma_{YW}\|_\mathbb{H}^2 
			= 
			\langle \Sigma_{YW}, \Sigma_{YW} \rangle_\mathbb{H}
			=
			\sum_{k=1}^K
			\int_0^1 \mathbb{E} \left[   Y X_k\right](t)^2 \, dt
			+ \sum_{r=1}^p \mathbb{E} \left[   Y Z_r\right]^2
			<
			K T Q_1 + p  Q_2 < \infty,
		\end{equation*} 
		where the last inequality uses the condition \eqref{condition_for_compact}.
		Let $M = \sqrt{K T Q_1 + p  Q_2 }$. Thus, we have shown that $|\mathcal{C}_{YW} h| \leq M \|h\|_\mathbb{H}$ for a finite constant $M$.
		This completes the proof of Lemma \ref{lemma:cross_cov_functional}.
		
	\end{proof}
	\subsection{Proof of Lemma \ref{lemma:cross_cov_operator}}\label{section:proof:lemma:cross_cov_operator}
	\begin{proof}
		For any $h \in \mathbb{H}$ and $d \in \mathbb{R}$, we have:
		\begin{equation*}
			\langle
			\mathcal{C}_{YW} h, d \rangle 
			= 
			\mathbb{E}
			[
			\langle W_1, h \rangle_\mathbb{H} Y_1  
			] 	 \,  d
			%%%%%%%%%%%%%%%%%%
			=
			\mathbb{E} \langle   Y_1 W_1 d , \, h\rangle_{\mathbb{H}} 
			%%%%%%%%%%%%%%%%%%%%%%%%
			= \langle h, \, \mathbb{E}[Y_1 W_1 d   ]\rangle_{\mathbb{H}}
			=
			\langle h, \mathcal{C}_{WY} d \rangle_{\mathbb{H}}
		\end{equation*}
		Thus, we have $\langle \mathcal{C}_{YW} h, d \rangle =  \langle h, \mathcal{C}_{WY} d \rangle_{\mathbb{H}}$, which implies $\mathcal{C}_{WY} = \mathcal{C}_{YW}^\ast$.
		This completes the proof of Lemma \ref{lemma:cross_cov_operator}.
	\end{proof}
	
	\subsection{Proof of Lemma \ref{lemma:composite_cross_cov}}\label{section:proof:lemma:composite_cross_cov}
	\begin{proof}We show that $\mathcal{U}$ is self-adjoint, positive-semidefinite, and compact, in turn.
		
		\medskip
		\noindent
		\textit{Self-adjoint.}~
		For any $h_1, h_2 \in \mathbb{H}$, we have
		\begin{align*}
			\langle 
			\mathcal{U} h_1, \, h_2 
			\rangle_\mathbb{H}  
			= 
			\bigl \langle 
			\langle h_1, \Sigma_{YW} \rangle_\mathbb{H} \Sigma_{YW},
			\,
			h_2
			\bigr \rangle_\mathbb{H}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			&= \langle h_1, \Sigma_{YW} \rangle_\mathbb{H} \langle \Sigma_{YW}, h_2 \rangle_\mathbb{H}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&= 
			\bigl \langle 
			h_1,
			\,
			\langle \Sigma_{YW}, h_2 \rangle_\mathbb{H} \Sigma_{YW} 
			\bigr \rangle_\mathbb{H}
			\\ 	&= \langle h_1, \mathcal{U} h_2 \rangle_\mathbb{H}.
		\end{align*}
		
		\medskip
		\noindent
		\textit{Positive-semidefinite.}~
		For every $h \in \mathbb{H}$, we have
		\begin{equation*}
			\langle 
			\mathcal{U} h,
			\,
			h
			\rangle_\mathbb{H}
			=
			\bigl \langle 
			\langle 
			\Sigma_{YW},\, h \rangle_{\mathbb{H}}  \Sigma_{YW}, \, h
			\bigr \rangle_\mathbb{H}
			= \langle \Sigma_{YW}, \, h \rangle_\mathbb{H}^2 \ge 0.
		\end{equation*}
		
		\medskip
		\noindent
		\textit{Compact.}~
		By Lemma \ref{lemma:cross_cov_functional}, $\mathcal{C}_{YW}$ is a compact operator.
		By  Theorem 4.1.3 of \citet{hsingTheoreticalFoundationsFunctional2015}, 
		the composition of two operators is compact if either operator is compact. Therefore
		$\mathcal{U} := \mathcal{C}_{WY} \circ \mathcal{C}_{YW}$ is a compact operator.
		This completes the proof of Lemma \ref{lemma:composite_cross_cov}.
	\end{proof}
	
	
	\subsection{Proof of Theorem \ref{theorem:population_PLS}}\label{section:proof:theorem:population_PLS}
	\begin{proof}
		by Theorem 4.3.1 of \citet{hsingTheoreticalFoundationsFunctional2015}, 
		since $\mathcal{C}_{YW}$ is compact by Lemma \ref{lemma:cross_cov_functional}, it 
		has the singular value decomposition of $\mathcal{C}_{YW}$, given by:
		\begin{equation*}
			\mathcal{C}_{YW} = \sum \limits_{j=1}^\infty \iota_j (f_{1j} \otimes f_{2j}).
		\end{equation*}
		Let $\Vert \cdot \Vert_{\mathrm{op}}$ denote an operator norm. By  4.3.4 in \citet{hsingTheoreticalFoundationsFunctional2015}, we have
		\begin{equation*}
			\Vert \mathcal{C}_{YW} \Vert_{\mathrm{op}} = \sup_{\substack{h \in \mathbb{H} \\ \Vert h \Vert_\mathbb{H}=1}} | \mathcal{C}_{YW} h |^2 
			= \sup_{\substack{h \in \mathbb{H} \\ \Vert h \Vert_\mathbb{H}=1}} | \mathbb{E}(\langle W, h \rangle_\mathbb{H} Y)|^2 
			= \sup_{\substack{h \in \mathbb{H} \\ \Vert h \Vert_\mathbb{H}=1}} \textnormal{Cov}^2(\langle W, h \rangle_\mathbb{H}, Y) 
			= \kappa_1^2,
		\end{equation*}
		with maximum attained at $h=f_{11}$, which is an eigenfunction of  $\mathcal{C}_{YW}^* \, \circ \, \mathcal{C}_{YW} = \mathcal{C}_{WY} \, \circ \, \mathcal{C}_{YW} = \mathcal{U}$ corresponding to the largest eigenvalue $\kappa_1^2.$
		This completes the proof of Theorem \ref{theorem:population_PLS}.
	\end{proof}
	
	
	
	
	
	
	
	
	
	\begin{comment}
		\section{Proof of Proposition \ref{proposition:compact_operator}}\label{section:proof:proposition:compact_operator}
		\begin{proof}
			The brief strategy of the proof is as follows.
			We   show that an image of a bounded family in $\mathbb{H}$ under $\mathcal{U}$ is uniformly bounded and equicontinuous.
			Then we apply the Arzel\'{a}-Ascoli theorem to show that $\mathcal{U}$ is \textit{compact}.
			
			Let $\mathcal{B}=\{h \in \mathbb{H}: \Vert h \Vert_\mathbb{H}^2 \le M\}$ denote a bounded family in $\mathbb{H}$ for some constant $0 < M < \infty$.
			Clearly, $h=(f, \mathbf{v}) \in \mathcal{B}$ also implies 
			\begin{equation}\label{individual_norm_bound}
				\Vert f_k \Vert_\mathcal{F}^2 \le M,
				~k=1,\ldots,K, \quad\text{and}\quad
				\Vert \mathbf{v} \Vert^2 \le M.
			\end{equation}
			Define $\mathcal{I}  = \{\mathcal{U}h: h \in \mathcal{B}\}$ as the image of  $\mathcal{B}$ under $\mathcal{U}$.
			
			\medskip
			\noindent
			\textit{Uniform boundedness of $\mathcal{I}$.}~Given a hybrid object $h = (f_1, \ldots, f_K, \mathbf{v}) \in \mathbb{H}$ and an evaluation point $\mathbf{t}= (t_1, \ldots, t_k) \in [0,1]^K$, we denote the $\ell_2$ norm of the evaluated object $\bigl( f_1(t_1), \ldots, f_k(t_k), \mathbf{v} \bigr)$ as follows:
			$\bigl(
			f_1(t_1), \ldots, f_k(t_k), \mathbf{v}
			\bigr)$
			as follows:
			\begin{equation*}
				\| h (\mathbf{t}) \|_2^2 := \sum_{k=1}^K f^2_k(t_k) + \| \mathbf{v} \|_2^2.
			\end{equation*}
			%We first show that $\mathcal{I}$ is a family of uniformly bounded functions with respect to arguments on $\mathcal{T}$.
			Let $\mu([0,1])$ denote a Lebesgue measure of $[0,1]$, and let $T = \max_{k=1, \ldots, K} \, \mu([0,1])$. Then for any $g \in \mathcal{I}$ and $\mathbf{t} \in [0,1]^K$, 
			by Lemma \ref{lemma:composite_cross_cov}, we have:
			\begin{equation}\label{gtsq_expansion}
				\Vert g (\mathbf{t}) \Vert^2_2
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				= \Vert (\mathcal{U}h)(\mathbf{t}) \Vert^2_2 
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				\overset{(i)}{=} 
				\Vert \langle \Sigma_{YW}, h \rangle_\mathbb{H} \, \Sigma_{YW}(\mathbf{t}) \Vert^2_2
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				=
				\langle \Sigma_{YW}, h \rangle_\mathbb{H}^2
				\Vert   \Sigma_{YW}(\mathbf{t}) \Vert^2_2.
			\end{equation}
			We will show that $\Vert g (\mathbf{t}) \Vert^2_2 < \infty$ by  showing $\langle \Sigma_{YW}, h \rangle_\mathbb{H}^2 < \infty$ and $\Vert   \Sigma_{YW}(\mathbf{t}) \Vert^2_2 < \infty$ in turn. First, to bound 
			$\langle \Sigma_{YW}, h \rangle_\mathbb{H}^2$, it suffices to bound $\langle \Sigma_{YW}, h \rangle_\mathbb{H}$:
			\begin{align*}
				\langle \Sigma_{YW}, h \rangle_\mathbb{H}
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				&=
				\sum_{k=1}^K \langle \sigma_{YX, k}, \, f_k \rangle_\mathcal{F}  
				+ 
				\langle \sigma_{YZ}, \, \mathbf{v} \rangle
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				\\&\overset{(i)}{\le}
				\sum_{k=1}^K 
				\langle \sigma_{YX, k}, \sigma_{YX, k} \rangle_\mathcal{F}^{1/2} 
				\langle f_k, f_k \rangle_\mathcal{F}^{1/2} + \langle \sigma_{YZ}, \sigma_{YZ} \rangle^{1/2} \langle \mathbf{v}, \mathbf{v} \rangle^{1/2}
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				\\&=
				\sum \limits_{k=1}^K
				\biggl(
				\int_0^1
				\sigma_{YX, k}^{2}(t)\, dt
				\biggr)^{1/2} 
				\Vert f_k \Vert_\mathcal{F} 
				+
				\biggl(
				\sum \limits_{r=1}^p \sigma_{YZ, r}^2
				\biggr)^{1/2}
				\Vert \mathbf{v} \Vert_2
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				\\&\overset{(ii)}{<}
				\sqrt{M}
				\biggl\{
				\sum \limits_{k=1}^K 
				\biggl(
				\int_0^1 Q_1 dt
				\biggr)^{1/2}
				+
				\biggl(
				\sum \limits_{r=1}^p Q_2 
				\biggr)^{\frac{1}{2}} 
				\biggr\}
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				\\ & = K\sqrt{M TQ_1}   + \sqrt{MpQ_2} < \infty.
				\numberthis \label{squared_ip_bound}
			\end{align*}
			where
			step $(i)$ uses Cauchy-Schwarz inequality,
			and
			step $(ii)$ uses the conditions \eqref{condition_for_compact} 
			and \eqref{individual_norm_bound}.
			
			Next, we bound $\Vert   \Sigma_{YW}(\mathbf{t}) \Vert^2_2  $:
			\begin{align*}
				\Vert   \Sigma_{YW}(\mathbf{t}) \Vert^2_2  
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				= %%%%%%%%%%%%%%%%%
				\sum \limits_{k=1}^K
				\sigma_{YX, k}^{2}(t_k)
				+
				\sum \limits_{r=1}^p \sigma^2_{YZ,r}
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				<
				\sum \limits_{k=1}^K Q_1 + \sum \limits_{r=1}^p Q_2 
				=  KQ_1 +pQ_2 < \infty.
			\end{align*}
			Therefore, we have
			\begin{equation*}
				\Vert g (\mathbf{t}) \Vert^2_2 < 
				2K^2 M TQ_1   + 2MpQ_2 + KQ_1 +pQ_2< \infty.
			\end{equation*}
			Since this bound does not depend on the evaluation point, we conclude that across all $g = (s_1(t), \ldots, s_K(t), \mathbf{w}) \in \mathcal{I}$, their functional components $s_1(t), \ldots, s_K(t)$ are uniformly bounded.
			
			\medskip
			\noindent
			\textit{Equicontinuity of $\mathcal{I}$.}~Given a hybrid object $h = (f_1, \ldots, f_K, \mathbf{v}) \in \mathbb{H}$ and 
			two evaluation points
			$\mathbf{t}=(t_1, \ldots, t_K) \in [0,1]^K$
			and
			$\mathbf{t}^\ast=(t_1^\ast, \ldots, t_K^\ast) \in [0,1]^K$,
			we denote the $\ell_2$ distance between   two evaluations
			$\bigl( f_1(t_1), \ldots, f_k(t_k), \mathbf{v} \bigr)$ 
			and
			$\bigl( f_1(t^\ast_1), \ldots, f_k(t^\ast_k), \mathbf{v} \bigr)$
			as follows:
			\begin{equation*}
				\| h (\mathbf{t}) - h (\mathbf{t}^\ast)\|_2^2 
				:= 
				\sum_{k=1}^K 
				\bigl(
				f_k(t_k) - f_k(t_k^\ast)
				\bigr)^2 .
			\end{equation*}
			For any $g \in \mathcal{I}$, via similar computation as in \eqref{gtsq_expansion}, we have
			\begin{align*}
				\Vert g(\mathbf{t}) - g ( \mathbf{t}^\ast ) \Vert_2^2 
				%%%%%%%%%%%%%%%%%
				= %%%%%%%%%%%%%%%%%%
				\Vert 
				(\mathcal{U}h)(\mathbf{t}) - (\mathcal{U}h)( \mathbf{t}^\ast ) 
				\Vert_2^2
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				&= %%%%%%%%%%%%%%%
				\Vert \langle \Sigma_{YW}, h \rangle_\mathbb{H} 
				\,
				\Sigma_{YW}(\mathbf{t}) - \langle \Sigma_{YW}, h \rangle_\mathbb{H} \Sigma_{YW}( \mathbf{t}^\ast ) \Vert^2_2
				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
				\\&=
				\langle \Sigma_{YW}, h \rangle_\mathbb{H}^2 \Vert \Sigma_{YW}(\mathbf{t})- \Sigma_{YW}( \mathbf{t}^\ast ) \Vert^2.
			\end{align*}
			We have already shown in \eqref{squared_ip_bound} that  $\langle \Sigma_{YW}, h \rangle_\mathbb{H}^2 \leq  2K^2 M TQ_1   + 2MpQ_2$.
			Next we bound $\Vert \Sigma_{YW}(\mathbf{t})- \Sigma_{YW}( \mathbf{t}^\ast ) \Vert^2$, which do not depend on $h$.
			\begin{align*}
				\Vert \Sigma_{YW}(\mathbf{t})- \Sigma_{YW}( \mathbf{t}^\ast ) \Vert^2_2	
				=%%%%%%%%%%%%%
				\sum \limits_{k=1}^K  
				\bigl\{
				\sigma_{YX, k }(t_k) -\sigma_{YX, k}(t_k^\ast ) 
				\bigr\}^2 
				& \leq%%%%%%%%%%%%%
				\sum \limits_{k=1}^K  
				\bigl\{
				2\sigma^2_{YX, k }(t_k) + 2\sigma^2_{YX, k}(t_k^\ast ) 
				\bigr\}
				%%%%%%%%%%%%%%%%%%%%%%
				\\& \le 
				4KQ_1.
			\end{align*}
			Therefore we have
			\begin{equation}\label{bound_elltwo_dist}
				\Vert g(\mathbf{t}) - g ( \mathbf{t}^\ast ) \Vert_2^2 \leq 2K^2 M TQ_1   + 2MpQ_2 + 4KQ_1.
			\end{equation}
			Given $\epsilon > 0$, define 
			\begin{equation*}
				\tilde{\epsilon}: = \frac{\epsilon}{  ( 2K^2 M TQ_1   + 2MpQ_2 + 4KQ_1 )^{1/2}}
			\end{equation*}
			By the uniform continuity assumption of $\sigma_{YX, k}$'s, there exists $\delta_k > 0$ such that
			\begin{equation*}
				|t_k - t_k^\ast| < \delta_k \implies |\sigma_{YX, k}(t_k) - \sigma_{YX, k}(t_k^\ast)|< \tilde{\epsilon},~\text{for all}~k=1, \ldots, K.
			\end{equation*}
			Set $\delta := \min_{k=1, \ldots, K} \delta_k$.Then  by \eqref{bound_elltwo_dist},  
			across  $g = (s_1(t), \ldots, s_K(t), \mathbf{w}) \in \mathcal{I},$ for all $k=1, \ldots, K$, 
			$|t_k - t^*_k| < \delta$ implies $|s_k(t_k) - s_k(t_k^\ast)| < \epsilon$.
			
			\medskip
			\noindent
			\textit{Conclusion.}~ For any bounded sequence $\{h_n\}_{n \in \mathbb{N}} \in \mathcal{B}$, the sequence $\{\mathbf{g}_n = \mathcal{U}h_n\}_{n \in \mathbb{N}} \in \mathcal{I}$ contains a subsequence that converges in its first functional component, by the Arzel\'{a}-Ascoli theorem. We then apply the theorem again to that subsequence to extract a sub-subsequence that also converges in the second functional component. By repeating this process, we ultimately obtain a subsequence of $\{\mathbf{g}_n\}$ that converges in all its components. Therefore, $\mathcal{U}$ is a compact operator.
		\end{proof}
	\end{comment}
	
	\section{Proof of consistency}
	
	
	\subsection{Proof of Proposition \ref{prop:residualization_equiv_eigen}}\label{section:proof:prop:residualization_equiv_eigen}
	The proof procedes in the following steps.
	\begin{enumerate}
		\item Show that for every \(s\ge 1\) and every \(j\le s-1\), we have $
		\mathbb{E}[\rho^{[j]}\rho^{[s]}]=0
		$ for Algorithm \ref{alg:population_pls} (Appendix \ref{section:proof:orthogonal_score_pop}),
		%%%%
		\item Show that for every \(s\ge 1\) and every \(j\le s-1\), we have
		$
		\mathbb{E}[Y^{[s]}\rho^{[j]}]=0
		$
		for Algorithm \ref{alg:population_pls} (Appendix \ref{section:proof:orthogonal_score_residual_pop}),
		%
		\item Show the equivalence between Algorithm \ref{alg:population_pls} and optimization problem \eqref{const_eigen_pop} (Appendix \ref{equiv_problem}).
	\end{enumerate}
	
	\subsubsection{Orthogonality of the scores in Algorithm \ref{alg:population_pls}}\label{section:proof:orthogonal_score_pop}
	\begin{proof}
		Let
		$
		\Sigma_{W^{[l]}} := \mathbb{E}[ W^{[l]} \otimes W^{[l]} ]
		$ as in Definition \ref{def:cross_cov_terms}.
		Since 
		$
		\rho^{[l]} = \langle W^{[l]}, \xi^{[l]} \rangle_{\mathbb{H}},
		$
		we can rewrite the numerator and denominator of $\delta^{[l]}$ as
		\[
		\mathbb{E}[ \rho^{[l]}  \,  W^{[l]} ] 
		= 
		\mathbb{E}[
		\langle W^{[l]}, \xi^{[l]} \rangle_{\mathbb{H}} W^{[l]} ]
		= \Sigma_{W^{[l]}} \, \xi^{[l]},
		\] 
		and
		\[
		\mathbb{E}[ \rho^{[l]}  \,  \rho^{[l]} ] 
		= 
		\mathbb{E}[
		\langle W^{[l]}, \xi^{[l]} \rangle_{\mathbb{H}} \langle W^{[l]}, \xi^{[l]} \rangle_{\mathbb{H}} ]
		= 
		\langle \Sigma_{W^{[l]}} \, \xi^{[l]}, \xi^{[l]} \rangle_{\mathbb{H}},
		\]
		respectively.
		Hence, at the population level, the residualization step can be written as
		\[
		W^{[l+1]} = W^{[l]} - 
		\langle W^{[l]}, \xi^{[l]} \rangle_{\mathbb{H}}
		\frac{ 
			1
		}
		{ \langle \xi^{[l]}, \Sigma_{W^{[l]}} \, \xi^{[l]} \rangle_{\mathbb{H}} }
		\Sigma_{W^{[l]}} \, \xi^{[l]} .
		\]
		
		For $l_1 < l_2$:
		\[
		\mathbb{E}[\rho^{[l_1]} \, \rho^{[l_2]}]
		=
		\mathbb{E}\big[ \rho^{[l_1]} \,
		\langle W^{[l_2]}, \xi^{[l_2]} \rangle_{\mathbb{H}} \big]
		=
		\mathbb{E}\big[ 
		\langle \rho^{[l_1]} \, W^{[l_2]}, \xi^{[l_2]} \rangle_{\mathbb{H}} \big]
		=
		\langle 
		\mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_2]}] \,
		, \,
		\xi^{[l_2]} \rangle_{\mathbb{H}}
		\]
		Therefore,
		to show that $\mathbb{E}[\rho^{[l_1]} \, \rho^{[l_2]}]$,
		it suffices to show $\mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_2]}] = 0 \in \mathbb{H} $.
		
		We can express
		$\mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_2]}]$ as
		\begin{align*}
			\mathbb{E}&\big[ \rho^{[l_1]} \,W^{[l_2]}]
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_2-1]}]
			-
			\mathbb{E}[
			\rho^{[l_1]}
			\frac{ 
				\langle W^{[l_2-1]}, \xi^{[l_2-1]} \rangle_{\mathbb{H}}
			}
			{ \langle \xi^{[l_2-1]}, \Sigma_{W^{[l_2-1]}} \, \xi^{[l_2-1]} \rangle_{\mathbb{H}} }
			\Sigma_{W^{[l_2-1]}} \, \xi^{[l_2-1]}
			]
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_2-1]}]
			-
			\mathbb{E}[
			\rho^{[l_1]}
			\langle W^{[l_2-1]}, \xi^{[l_2-1]} \rangle_{\mathbb{H}}
			]
			\frac{ 
				1
			}{ 
				\langle
				\xi^{[l_2-1]}, 
				\Sigma_{W^{[l_2-1]}} \, \xi^{[l_2-1]}
				\rangle_{\mathbb{H}} }
			\Sigma_{W^{[l_2-1]}} \, \xi^{[l_2-1]}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_2-1]}]
			-
			\langle 
			\mathbb{E}[
			\rho^{[l_1]} W^{[l_2-1]}], \xi^{[l_2-1]} \rangle_{\mathbb{H}}
			\frac{ 
				1
			}{ 
				\langle
				\xi^{[l_2-1]}, 
				\Sigma_{W^{[l_2-1]}} \, \xi^{[l_2-1]}
				\rangle_{\mathbb{H}} }
			\Sigma_{W^{[l_2-1]}} \, \xi^{[l_2-1]}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\	&=	H^{[l_2-1]} \bigl( 
			\mathbb{E}[\rho^{[l_1]  }
			W^{[l_2-1]}] \bigr)~(\text{say}).
		\end{align*}
		Here, the deterministic operator $H^{[l_2-1]}: \mathbb{H} \mapsto \mathbb{H}$ maps the zero element of $\mathbb{H}$ to itself. Using this relationship, we have
		\begin{align*}
			\mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_2]}]  
			%%%%%%%%%%%%%%%%
			&=
			H^{[l_2-1]} \bigl( \mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_2-1]}] \bigr)
			%%%%%%%%%%%%%%%%
			\\&=
			H^{[l_2-1]} 
			\circ 
			H^{[l_2-2]}
			\bigl(
			\mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_2-2]}]
			\bigr)
			%%%%%%%%%%%%%%%%
			\\&=
			H^{[l_2-1]} 
			\circ 
			H^{[l_2-2]}
			\circ 
			\ldots
			\circ 
			H^{[l_1]}
			\bigl( \mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_1]}] \bigr).
		\end{align*}
		Therefore,
		to  show that $\mathbb{E}[\rho^{[l_1]} \, \rho^{[l_2]}]$,
		it suffices to show $H^{[l_1]}
		\bigl( \mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_1]}] \bigr) = 0 \in \mathbb{H}$. Note that
		we have
		\begin{equation*}
			\mathbb{E}[\rho^{[l_1]} W^{[l_1]}] = \mathbb{E}[\langle W^{[l_1]}, \xi^{[l_1]} \rangle_{\mathbb{H}} \, W^{[l_1]}] = \Sigma_{W^{[l_1]}} \, \xi^{[l_1]}.
		\end{equation*}
		Leveraging this, we have
		\begin{align*}
			H^{[l_1]}&
			\bigl( \mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_1]}] \bigr)
			\\           &=
			\mathbb{E}\big[ \rho^{[l_1]} \,W^{[l_1]}]
			-
			\langle 
			\mathbb{E}[
			\rho^{[l_1]} W^{[l_1]}], \xi^{[l_1]} \rangle_{\mathbb{H}}
			\frac{ 
				1
			}{ 
				\langle
				\xi^{[l_1]}, 
				\Sigma_{W^{[l_1]}} \, \xi^{[l_1]}
				\rangle_{\mathbb{H}} }
			\Sigma_{W^{[l_1]}} \, \xi^{[l_1]}
			\\           &=
			\Sigma_{W^{[l_1]}} \, \xi^{[l_1]}
			-
			\langle 
			\Sigma_{W^{[l_1]}} \, \xi^{[l_1]}, \xi^{[l_1]} \rangle_{\mathbb{H}}
			\frac{ 
				1
			}{ 
				\langle
				\xi^{[l_1]}, 
				\Sigma_{W^{[l_1]}} \, \xi^{[l_1]}
				\rangle_{\mathbb{H}} }
			\Sigma_{W^{[l_1]}} \, \xi^{[l_1]}
			\\           &=
			\Sigma_{W^{[l_1]}} \, \xi^{[l_1]}
			-
			\Sigma_{W^{[l_1]}} \, \xi^{[l_1]}
			=0 \in \mathbb{H}.
		\end{align*}
		This completes the proof of
		orthogonality of the scores in Algorithm \ref{alg:population_pls}.
		
		
	\end{proof}
	
	\subsubsection{Orthogonality of the scores and response residuals in Algorithm \ref{alg:population_pls}}\label{section:proof:orthogonal_score_residual_pop}
	\begin{proof}
		Fix \(s\ge 2\) and \(j\le s-1\). Recall the residualization for the response:
		\[
		Y^{[s]} \;=\; Y - \sum_{k=1}^{s-1} \nu^{[k]}\,\rho^{[k]},
		\qquad
		\nu^{[k]} \;=\; \frac{\mathbb{E}[Y^{[k]}\rho^{[k]}]}{\mathbb{E}[(\rho^{[k]})^2]}.
		\]
		Compute the covariance of interest by expanding the residual:
		\begin{align*}
			\mathbb{E}\big[ Y^{[s]}\,\rho^{[j]} \big]
			= \mathbb{E}\Big[ \Big( Y - \sum_{k=1}^{s-1} \nu^{[k]}\rho^{[k]}\Big)\, \rho^{[j]} \Big] 
			= \mathbb{E}[Y\,\rho^{[j]}] \;-\; \sum_{k=1}^{s-1} \nu^{[k]}\,\mathbb{E}[\rho^{[k]}\rho^{[j]}].
		\end{align*}
		
		By the score orthogonality already established in Appendix \ref{section:proof:orthogonal_score_pop},
		we have \(\mathbb{E}[\rho^{[k]}\rho^{[j]}]=0\) for every \(k\neq j\). Thus only the \(k=j\) term survives in the sum and we obtain
		\[
		\mathbb{E}\big[ Y^{[s]}\,\rho^{[j]} \big]
		= \mathbb{E}[Y\,\rho^{[j]}] - \nu^{[j]}\,\mathbb{E}[(\rho^{[j]})^2].
		\]
		
		It remains to identify \(\mathbb{E}[Y\,\rho^{[j]}]\). Using the definition of the \((j)\)-th residual response,
		\[
		Y = Y^{[j]} + \sum_{m=1}^{j-1} \nu^{[m]}\rho^{[m]},
		\]
		and again using score orthogonality \(\mathbb{E}[\rho^{[m]}\rho^{[j]}]=0\) for \(m<j\), we find
		\[
		\mathbb{E}[Y\,\rho^{[j]}] = \mathbb{E}[Y^{[j]}\,\rho^{[j]}].
		\]
		Substitute this into the previous display and use the definition of \(\nu^{[j]}\):
		\[
		\mathbb{E}\big[ Y^{[s]}\,\rho^{[j]} \big]
		= \mathbb{E}[Y^{[j]}\,\rho^{[j]}] - 
		\frac{\mathbb{E}[Y^{[j]}\rho^{[j]}]}{\mathbb{E}[(\rho^{[j]})^2]}\,\mathbb{E}[(\rho^{[j]})^2]
		= 0.
		\]
		This completes the proof of orthogonality between scores and residualized responses.
	\end{proof}
	
	\subsubsection{Equivalence of the problems}\label{equiv_problem}
	\begin{proof}
		Recall the residualization definitions (for fixed \(l\ge 2\)):
		\[
		W^{[l]} \;=\; W - \sum_{j=1}^{\,l-1} \rho^{[j]}\,\delta^{[j]},
		\qquad
		Y^{[l]} \;=\; Y - \sum_{k=1}^{\,l-1} \nu^{[k]}\,\rho^{[k]},
		\]
		where
		\[
		\rho^{[j]}=\langle W^{[j]},\xi^{[j]}\rangle,\quad
		\delta^{[j]}=\frac{\mathbb{E}[\rho^{[j]} W^{[j]}]}{\mathbb{E}[(\rho^{[j]})^2]},
		\quad
		\nu^{[j]}=\frac{\mathbb{E}[\rho^{[j]} Y^{[j]}]}{\mathbb{E}[(\rho^{[j]})^2]}.
		\]
		Since \(W,Y\) are centered, 
		so are $W^{l}, Y^{l}$.
		Thus covariances are expectations of products.
		Fix any \(w\in\mathbb{H}\).  Using linearity,
		\begin{align*}
			\operatorname{Cov}\!\big(\langle W^{[l]},w\rangle,\;Y^{[l]}\big)
			&= \mathbb{E}\Big[\big(\langle W,w\rangle - \sum_{j=1}^{l-1} \rho^{[j]}\langle\delta^{[j]},w\rangle\big)
			\big(Y - \sum_{k=1}^{l-1} \nu^{[k]}\rho^{[k]}\big)\Big] \\
			\\&= \mathbb{E}[\langle W,w\rangle Y] 
			- \sum_{k=1}^{l-1} \nu^{[k]}\,\mathbb{E}[\langle W,w\rangle \rho^{[k]}]
			- \underbrace{
				\sum_{j=1}^{l-1} \langle\delta^{[j]},w\rangle\,\mathbb{E}[\rho^{[j]} Y]}_{:=S_1} \\
			&\qquad
			\underbrace{
				+ 
				\sum_{j=1}^{l-1}\sum_{k=1}^{l-1} \langle\delta^{[j]},w\rangle\,\nu^{[k]}\,\mathbb{E}[\rho^{[j]}\rho^{[k]}]
			}_{:=S_2}.
		\end{align*}
		We claim that $S_1  = S_2$.
		For $S_1$, first note that, since
		\(Y = Y^{[l]} + \sum_{k=1}^{l-1}\nu^{[k]}\rho^{[k]}\), for each \(j\le l-1\) we have
		\[
		\mathbb{E}[\rho^{[j]} Y] = \mathbb{E}[\rho^{[j]} Y^{[l]}] + \sum_{k=1}^{l-1} \nu^{[k]}\mathbb{E}[\rho^{[j]}\rho^{[k]}]=
		\nu^{[j]}\,\mathbb{E}[(\rho^{[j]})^2],
		\]
		where the last equality uses 
		\(\mathbb{E}[\rho^{[j]} Y^{[l]}]=0\) (Appendix \ref{section:proof:orthogonal_score_residual_pop}) and
		\(\mathbb{E}[\rho^{[j]}\rho^{[k]}]=0\) for \(j\ne k\) (Appendix \ref{section:proof:orthogonal_score_pop}).
		Therefore we have
		\[
		S_1 = \sum_{j=1}^{l-1} \langle\delta^{[j]},w\rangle \nu^{[j]}\,\mathbb{E}[(\rho^{[j]})^2].
		\]
		For $S_2$, 
		since  \(\mathbb{E}[\rho^{[j]}\rho^{[k]}]=0\) for \(j\ne k\) (Appendix \ref{section:proof:orthogonal_score_pop}), we have:
		\begin{equation*}
			S_2
			=
			\sum_{j=1}^{l-1}\sum_{k=1}^{l-1} \langle\delta^{[j]},w\rangle\,\nu^{[k]}\,\mathbb{E}[\rho^{[j]}\rho^{[k]}]
			= \sum_{j=1}^{l-1} \langle\delta^{[j]},w\rangle\,\nu^{[j]}\,\mathbb{E}[(\rho^{[j]})^2].   
		\end{equation*}
		Since $S_1 = S_2$, 
		and
		\(\mathbb{E}[\langle W,w\rangle Y]=\operatorname{Cov}(\langle W,w\rangle,Y)\), we have
		\[
		\operatorname{Cov}\big(\langle W^{[l]},w\rangle,\;Y^{[l]}\big)
		= \operatorname{Cov}(\langle W,w\rangle,Y)
		- \sum_{k=1}^{l-1} \nu^{[k]}\,\operatorname{Cov}(\langle W,w\rangle,\rho^{[k]}).
		\]
		Therefore we are solving
		\begin{align*}
			\max_{\|w\|_{\mathbb{H}}=1}~&
			\Big\{ 
			\operatorname{Cov}(\langle W,w\rangle,Y)
			- \sum_{k=1}^{l-1} \nu^{[k]}\,\operatorname{Cov}(\langle W,w\rangle,\rho^{[k]})
			\Big\}^2
			\\
			s.t.~&
			\nu^{[k]}
			=
			\frac{1}{\mathbb{E}[ (\rho^{[k]})^2 ]}
			\mathbb{E}[Y^{[l]} \rho^{[k]}],~k=1, \ldots, l-1
			\\~&
			\rho^{[k]} = \langle \xi^{[k]}, W^{[k]} \rangle,~k=1, \ldots, l-1 
			\\~&
			\delta^{[k]}
			=
			\frac{1}{\mathbb{E}[ (\rho^{[k]})^2 ]}  \mathbb{E}[W^{[k]} \rho^{[k]}],~k=1, \ldots, l-1
			\\~&
			W^{[k+1]} = W^{[k]} - \rho^{[k]} \, \delta^{[k]},~k=1, \ldots, l-1
			\\~&
			Y^{[k]} =Y^{[k-1]} - \nu^{[k-1]} \, \rho^{[k-1]},~k=1, \ldots, l-1
		\end{align*}
		Since the constraints imply 
		that for every \(s\ge 1\) and every \(j\le s-1\), we have $
		\mathbb{E}[\rho^{[j]}\rho^{[s]}]=\langle w, \Sigma_W \, \xi^{[k]} \rangle_{\mathbb{H}} = \operatorname{Cov}(\langle W,w\rangle,\rho^{[k]}) =0
		$ (Appendix \ref{section:proof:orthogonal_score_pop}),
		the above problem is equivalent to 
		\begin{align*}
			\max_{\|w\|_{\mathbb{H}}=1}~&
			\Big\{ 
			\operatorname{Cov}(\langle W,w\rangle,Y)
			\Big\}^2
			\\
			s.t.~&
			\nu^{[k]}
			=
			\frac{1}{\mathbb{E}[ (\rho^{[k]})^2 ]}
			\mathbb{E}[Y^{[l]} \rho^{[k]}],~k=1, \ldots, l-1
			\\~&
			\rho^{[k]} = \langle \xi^{[k]}, W^{[k]} \rangle,~k=1, \ldots, l-1 
			\\~&
			\delta^{[k]}
			=
			\frac{1}{\mathbb{E}[ (\rho^{[k]})^2 ]}  \mathbb{E}[W^{[k]} \rho^{[k]}],~k=1, \ldots, l-1
			\\~&
			W^{[k+1]} = W^{[k]} - \rho^{[k]} \, \delta^{[k]},~k=1, \ldots, l-1
			\\~&
			Y^{[k]} =Y^{[k-1]} - \nu^{[k-1]} \, \rho^{[k-1]},~k=1, \ldots, l-1
			\\~& \langle w, \Sigma_W \, \xi^{[k]} \rangle_{\mathbb{H}}=0,~k=1, \ldots, l-1.
		\end{align*}
		Removing constraints that does not affect the objective function, the above optimization problem is equivalent to
		\begin{align*}
			\max_{\|w\|_{\mathbb{H}}=1}~&
			\Big\{ 
			\operatorname{Cov}(\langle W,w\rangle,Y)
			\Big\}^2
			\\
			s.t.~&
			\langle w, \Sigma_W \, \xi^{[k]} \rangle_{\mathbb{H}}=0,~k=1, \ldots, l-1.
		\end{align*}
		This completes
		the proof of Proposition \ref{prop:residualization_equiv_eigen}.
	\end{proof}
	
	
	
	
	
	
	
	
	
	\subsection{Proof of Theorem \ref{thm:hybrid_pls_convergence}}\label{section:proof:thm:hybrid_pls_convergence}
	\begin{proof}
		Iterating the residualization $Y^{[l]} = Y^{[l-1]} - \nu^{[l-1]} \rho^{[l-1]}$ for $l=1,\dots,L$, we obtain
		\begin{equation}\label{eq:Y_sum_residuals}
			Y = \sum_{j=0}^{L-1} \nu^{[j]} \rho^{[j]} + Y^{[L]} = Y_{\mathrm{PLS},L-1} + Y^{[L]}.
		\end{equation}
		Consider the $L^2$ norm of the remaining residual $Y^{[L]}$:
		\[
		\mathbb{E}[(Y^{[L]})^2] = \mathbb{E}[(Y^{[L-1]} - \nu^{[L-1]} \rho^{[L-1]})^2] = \mathbb{E}[(Y^{[L-1]})^2] - (\nu^{[L-1]})^2 \mathbb{E}[(\rho^{[L-1]})^2].
		\]
		Thus, the residual variance is nonnegative and monotonically decreasing with $L$:
		\[
		0 \le \mathbb{E}[(Y^{[L]})^2] \le \mathbb{E}[(Y^{[L-1]})^2] \le \cdots \le \mathbb{E}[Y^2] < \infty.
		\]
		Therefore, the series $\sum_{l=1}^{\infty} (\nu^{[l]})^2 \mathbb{E}[(\rho^{[l]})^2]$ converges and the residual variance $\mathbb{E}[(Y^{[L]})^2] \to 0$ as $L \to \infty$.
		From \eqref{eq:Y_sum_residuals}, the mean squared error of the PLS approximation is
		\[
		\mathbb{E}\bigl[ \| Y_{\mathrm{PLS},L-1} - Y \|^2 \bigr] = \mathbb{E}[(Y^{[L]})^2] \to 0 \quad \text{as } L \to \infty.
		\]
		This completes the proof of Theorem \ref{thm:hybrid_pls_convergence}.
	\end{proof}
	
	
	\section{Proof of geometric properties}
	
	
	
	\subsection{Proof of Proposition \ref{proposition: modified orthnormality of PLS components}}
	\label{section:proof:proposition: modified orthnormality of PLS components}
	
	\begin{proof}
		The unit norm condition is trivially met by the constraint 
		$1 =  	\hat{\boldsymbol{\xi}}_l^\top 
		\hspace{-.2em}
		(\mathbb{B} + \Lambda \mathbb{B}^{\prime \prime}) \hat{\boldsymbol{\xi}}_l$
		enforced in \eqref{eq: Regularized generalized rayleigh quotient equation}, because we have:
		\begin{align*}
			\hat{\boldsymbol{\xi}}_l^\top 
			\hspace{-.2em}
			(\mathbb{B} + \Lambda \mathbb{B}^{\prime \prime}) \hat{\boldsymbol{\xi}}_l
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			&= \hat{\boldsymbol{\xi}}^\top_l 
			\hspace{-.2em}
			\mathbb{B}
			\hat{ \boldsymbol{\xi} }_l
			+
			\hat{ \boldsymbol{\xi} }_l^\top 
			\hspace{-.2em}
			\Lambda \mathbb{B}^{\prime \prime} \hat{ \boldsymbol{\xi} }_l
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&\overset{(i)}{=}
			\sum_{j=1}^K
			\int_0^1
			\hat{\xi}_l(t) \, \hat{\xi}_l(t)
			\, dt
			+
			\hat{\boldsymbol{\zeta}}^\top  \hat{\boldsymbol{\zeta}}
			+
			\hat{\boldsymbol{\xi}}^\top 
			\hspace{-.2em}
			\Lambda \mathbb{B}^{\prime \prime} \hat{\boldsymbol{\xi}}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&\overset{(ii)}{=}
			\sum_{j=1}^K
			\int_0^1
			\hat{\xi}_l(t) \, \hat{\xi}_l(t)
			\, dt
			+
			\hat{\boldsymbol{\zeta}}^\top \hat{\boldsymbol{\zeta}}
			+
			\sum \limits_{j=1}^K 
			\lambda_l
			\int_0^1 \bigl\{ \hat{\xi}_l^{\prime\prime}(t) \bigr\}^2 dt
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\langle \widehat{\xi}_{l}, \widehat{\xi}_{l} \rangle_{\mathbb{H}, \Lambda},
			\numberthis \label{rough_norm_matrix_form}
		\end{align*}
		where step $(i)$ uses \eqref{unit_norm_as_quadform}
		and
		step $(ii)$ uses \eqref{pen_as_matrix}.
		
		Now to switch our gears toward the the orthogonality. For  $l_1 > l_2 $, we have
		\begin{align*}
			\langle \widehat{\xi}_{l_1}, \widehat{\xi}_{l_2} \rangle_{\mathbb{H}, \Lambda}
			& \overset{(i)}{=}
			\hat{\boldsymbol{\xi}}_{l_1}^\top 
			\hspace{-.2em}
			(\mathbb{B} + \Lambda \mathbb{B}^{\prime \prime}) \hat{\boldsymbol{\xi}}_{l_2}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\	& \overset{(ii)}{=} \frac{1}{\kappa_{l_1}} (V^{[l_1]} \widehat{\boldsymbol{\xi}}_{l_1})^\top \hat{\boldsymbol{\xi}}_{l_2}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\	& = \frac{1}{\kappa_{l_1}}
			\widehat{\boldsymbol{\xi}}_{l_1}^\top
			V^{[l_1]} \hat{\boldsymbol{\xi}}_{l_2} 
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\	& \overset{(iii)}{=} \frac{1}{ n^2 \kappa_{l_1}}
			\widehat{\boldsymbol{\xi}}_{l_1}^\top
			(\mathbb{B} \Theta^{[l_1]\top} \mathbf{y})(\mathbb{B} \Theta^{[l_1]\top} \mathbf{y})^\top
			\hat{\boldsymbol{\xi}}_{l_2}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\	& = \frac{1}{ n^2 \kappa_{l_1}}
			\widehat{\boldsymbol{\xi}}_{l_1}^\top
			(\mathbb{B} \Theta^{[l_1]\top} \mathbf{y})
			\mathbf{y}^\top
			\Theta^{[l_1] } 
			\mathbb{B}
			\hat{\boldsymbol{\xi}}_{l_2}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\	& = c
			\mathbf{y}^\top
			(
			\Theta^{[l_1] }
			\mathbb{B}
			\hat{\boldsymbol{\xi}}_{l_2}
			),
		\end{align*}
		where
		step $(i)$ uses \eqref{rough_norm_matrix_form},
		step $(ii)$ uses the fact that $\hat{\boldsymbol{\xi}}_{l_1}$ is a generalized eigenvector ($\kappa_{l_1}$ denotes the corresponding generalized eigenvalue), as presented in Proposition \ref{proposition:eigen_regul},
		and step $(iii)$ uses the definition of $V$ matrix given in \eqref{def:V_matrix}.
		Here, $c$ represents a scalar that condenses all multiplicative terms preceding $\mathbf{y}^\top$.
		
		Orthogonality can be demonstrated by showing that $\Theta^{[l_1] } \mathbb{B} \hat{\boldsymbol{\xi}}_{l_2} = \mathbf{0} \in \mathbb{R}^n$.
		From the construction in \eqref{def:full_theta}, \eqref{def:theta} and \eqref{def:gram_block}, the
		$i$th entry of $\Theta^{[l_1] }
		\mathbb{B}
		\hat{\boldsymbol{\xi}}_{l_2}$ 
		is
		\begin{align*}
			(\Theta^{[l_1] }
			\mathbb{B}
			\hat{\boldsymbol{\xi}}_{l_2})_i
			&=
			(\theta_{i11}^{[l_1]}, \ldots, \theta_{i1M}^{[l_1]}, \ldots, \theta_{iK1}^{[l_1]}, \ldots, \theta_{iKM}^{[l_1]}, Z_{i1}^{[l_1]}, \ldots, Z_{ip}^{[l_1]})
			\mathbb{B}
			\hat{\boldsymbol{\xi}}_{l_2}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\sum_{k=1}^K
			\boldsymbol{\theta}_{ik}^{[l_1]} B
			\hat{\boldsymbol{\gamma}}_k^{[l_2]}
			+
			\mathbf{Z}_{i}^{[l_1] \top } \hat{\boldsymbol{\zeta}}^{[l_2]}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\	&=
			\langle
			\widetilde{W}_i^{[l_1]}, 
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}},
			\numberthis \label{thetaBzetal2i}
		\end{align*}
		where the last equality uses the computation of the $i$th entry in \eqref{score_inner_product_all_n}. To expand the last quantity, we use a recursive relationship derived as follows. Recall from Lemma  \ref{proposition:closed_form_orthgonalization} that we have
		\begin{equation}\label{algorithm_step:residualization_predictor:recall}
			\widetilde{W}_i^{[l_1]} 
			:= 
			\widetilde{W}_i^{[l_1-1]}  - 
			\frac{\widehat{\rho}_{i\,l_1-1}}{\| 	\hat{\boldsymbol{\rho}}^{[l_1-1]}\|_2^2}
			\sum_{i=1}^n 
			\widehat{\rho}_{i \,l_1-1}
			\widetilde{W}_i^{[l_1-1]}.
		\end{equation} 	
		Let us denote, with some abuse of notation:
		\begin{equation}\label{notation:n_predictors}
			\widetilde{\boldsymbol{W}}^{[l_1]}  := 
			\bigl(
			\widetilde{W}_1^{[l_1-1]}, \ldots, \widetilde{W}_n^{[l_1-1]}
			\bigr)^\top \in \widetilde{\mathbb{H}}^{\otimes n},
		\end{equation}
		and
		\begin{equation}\label{notation:n_inner_products}
			\langle 	\widetilde{\boldsymbol{W}}^{[l_1]}  ,
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}}
			:= 
			\biggl(
			\langle 
			\widetilde{W}_1^{[l_1-1]}, 
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}},
			\ldots, 
			\langle 
			\widetilde{W}_n^{[l_1-1]},  
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}}
			\biggr)^\top 
			\in \mathbb{R}^n.
		\end{equation}
		Recalling \eqref{thetaBzetal2i},
		to achieve our goal of showing $
		\Theta^{[l_1] }
		\mathbb{B}
		\hat{\boldsymbol{\xi}}_{l_2} = \mathbf{0} \in \mathbb{R}^n,
		$
		it suffices to show
		$	\langle 	\widetilde{\boldsymbol{W}}^{[l_1]}  ,
		\hat{\xi}^{[l_2]}
		\rangle_{\mathbb{H}} = \mathbf{0}$.
		Using these notations and  \eqref{algorithm_step:residualization_predictor:recall}, we have, with some abuse of notation,
		\begin{equation}\label{regression_relation_with_notation_abuse}
			\widetilde{\boldsymbol{W}}^{[l_1]}  = 
			\widetilde{\boldsymbol{W}}^{[l_1-1]} - 
			\biggl(
			\frac{1}{\| 	\hat{\boldsymbol{\rho}}^{[l_1-1]}\|_2^2}
			\hat{\boldsymbol{\rho}}^{[l_1-1] }
			\hat{\boldsymbol{\rho}}^{[l_1-1] \top
			} 
			\biggr)
			\widetilde{\boldsymbol{W}}^{[l_1-1]},
		\end{equation}
		and thus
		\begin{equation*}
			\langle 	\widetilde{\boldsymbol{W}}^{[l_1]}  ,
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}}
			= 
			\langle 	\widetilde{\boldsymbol{W}}^{[l_1-1]}  ,
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}} 
			- 
			\biggl(
			\frac{1
			}{
				\| 	\hat{\boldsymbol{\rho}}^{[l_1-1]}\|_2^2
			}
			\hat{\boldsymbol{\rho}}^{[l_1-1]}
			\hat{\boldsymbol{\rho}}^{[l_1-1] \top}
			\biggr)
			\bigl \langle 
			\widetilde{\boldsymbol{W}}^{[l_1-1]}  ,
			\hat{\xi}^{[l_2]}
			\bigr \rangle_{\mathbb{H}}.
		\end{equation*}
		Clearly, the right-hand side represents a linear operator from $\mathbb{R}^n$ to $\mathbb{R}^n$, which we denote as $P^{[l_1-1]}$.
		Thus, 
		we have $\langle 	\widetilde{\boldsymbol{W}}^{[l_1]}  ,
		\hat{\xi}^{[l_2]}
		\rangle_{\mathbb{H}}  = 
		P^{[l_1-1]} \langle 	\widetilde{\boldsymbol{W}}^{[l_1-1]}  ,
		\hat{\xi}^{[l_2]}
		\rangle_{\mathbb{H}}$.
		Repeatedly using this relationship, we have
		\begin{align*}
			\langle 	\widetilde{\boldsymbol{W}}^{[l_1]}  ,
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}} 
			&=
			P^{[l_1-1]} \langle 	\widetilde{\boldsymbol{W}}^{[l_1-1]}  ,
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}} 
			\\
			&=
			P^{[l_1-1]} P^{[l_1-2]}   \langle 	\widetilde{\boldsymbol{W}}^{[l_1-2]}  ,
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}}
			\\
			&= \underbrace{ P^{[l_1-1]} P^{[l_1-2]} \ldots P^{[l_2 + 1]}}_{:=P}  P^{[l_2]} \langle 	\widetilde{\boldsymbol{W}}^{[l_2]}  ,
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}}
			\\
			&= P \,\biggl\{
			\langle 	\widetilde{\boldsymbol{W}}^{[l_2]}  ,
			\hat{\xi}^{[l_2]}
			\rangle_{\mathbb{H}} 
			- 
			\biggl(
			\frac{1
			}{
				\| 	\hat{\boldsymbol{\rho}}^{[l_2]}\|_2^2
			}
			\hat{\boldsymbol{\rho}}^{[l_2]}
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\biggr)
			\bigl \langle 
			\widetilde{\boldsymbol{W}}^{[l_2]}  ,
			\hat{\xi}^{[l_2]}
			\bigr \rangle_{\mathbb{H}}
			\biggr\}
			\\
			&= P \,\biggl\{
			\hat{\boldsymbol{\rho}}^{[l_2]}
			- 
			\biggl(
			\frac{1
			}{
				\| 	\hat{\boldsymbol{\rho}}^{[l_1-1]}\|_2^2
			}
			\hat{\boldsymbol{\rho}}^{[l_2]}
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\biggr)
			\hat{\boldsymbol{\rho}}^{[l_2]}
			\biggr\}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&= P \,\biggl\{
			\hat{\boldsymbol{\rho}}^{[l_2]}
			- 
			\biggl(
			\frac{\hat{\boldsymbol{\rho}}^{[l_2] \top} \hat{\boldsymbol{\rho}}^{[l_2]}
			}{
				\| 	\hat{\boldsymbol{\rho}}^{[l_1-1]}\|_2^2
			}
			\biggr)
			\hat{\boldsymbol{\rho}}^{[l_2]}
			\biggr\}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&= P \,\biggl\{
			\hat{\boldsymbol{\rho}}^{[l_2]}
			- 
			\hat{\boldsymbol{\rho}}^{[l_2]}
			\biggr\}
			=
			P \mathbf{0} = \mathbf{0}.
		\end{align*}
		This completes the proof of Proposition 
		\ref{proposition: modified orthnormality of PLS components}.
	\end{proof}
	\subsection{Proof of Proposition \ref{proposition: orthnormality of PLS scores}}\label{section:proof:proposition: orthnormality of PLS scores}
	\begin{proof}
		For $l_1 < l_2$, we have
		\begin{align*}
			\widehat{\boldsymbol{\rho}}^{[l_1 ] \top}
			\widehat{\boldsymbol{\rho}}^{[l_2]}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			&=  
			\sum_{i=1}^n
			\widehat{ \rho }^{[l_1 ]}_i \,
			\widehat{ \rho }^{[l_2]}_i
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			=  
			\sum_{i=1}^n
			\widehat{ \rho }^{[l_1 ]}_i \,
			\bigl \langle 	\widetilde{W}^{[l_2]}_i  ,
			\hat{\xi}^{[l_2]}
			\bigr \rangle_{\mathbb{H}}
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			=  
			\big \langle
			\sum_{i=1}^n
			\widehat{ \rho }^{[l_1 ]}_i \,
			\widetilde{W}^{[l_2]}_i  ,
			\hat{\xi}^{[l_2]}
			\big \rangle_{\mathbb{H}}.
		\end{align*}
		Therefore,
		to show that $\widehat{\boldsymbol{\rho}}^{[l_1 ] \top}
		\widehat{\boldsymbol{\rho}}^{[l_2]} = 0$,
		it suffices to show that
		$\sum_{i=1}^n
		\widehat{ \rho }^{[l_1 ]}_i \,
		\widetilde{W}^{[l_2]}_i  = 0 \in   \mathbb{H}  $,
		where this   zero element represents an ordered pair of $K$ zero functions and a zero matrix.
		Using Lemma    \ref{proposition:closed_form_orthgonalization}, notations from 
		\eqref{notation:n_predictors} and
		\eqref{notation:n_inner_products},
		and equation \eqref{regression_relation_with_notation_abuse}, we have:
		\begin{align*}
			\widetilde{\boldsymbol{W}}^{[l_1]}  &= 
			\widetilde{\boldsymbol{W}}^{[l_1-1]} - 
			\biggl(
			\frac{1}{\| 	\hat{\boldsymbol{\rho}}^{[l_1-1]}\|_2^2}
			\hat{\boldsymbol{\rho}}^{[l_1-1] }
			\hat{\boldsymbol{\rho}}^{[l_1-1] \top
			} 
			\biggr)
			\widetilde{\boldsymbol{W}}^{[l_1-1]}
			%%%%%%%%%%%%%%%%%%%%%%
			\\&=	\widetilde{\boldsymbol{W}}^{[l_1-1]} - 
			\biggl(
			\frac{1}{\| 	\hat{\boldsymbol{\rho}}^{[l_1-1]}\|_2^2}
			\langle 	
			\widetilde{\boldsymbol{W}}^{[l_1-1]}  ,
			\hat{\xi}^{[l_1-1]}
			\rangle_{\mathbb{H}}
			\hat{\boldsymbol{\rho}}^{[l_1-1] \top
			} 
			\biggr)
			\widetilde{\boldsymbol{W}}^{[l_1-1]}.
		\end{align*}
		Using this relationship and
		with some abuse of notation, we have:
		\begin{align*}
			\sum_{i=1}^n
			\widehat{ \rho }^{[l_1 ]}_i \,
			\widetilde{W}^{[l_2]}_i 
			&=
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_1]}
			\\  &=	
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_1-1]} - 
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\biggl(
			\frac{1}{\| 	\hat{\boldsymbol{\rho}}^{[l_1-1]}\|_2^2}
			\langle 	
			\widetilde{\boldsymbol{W}}^{[l_1-1]}  ,
			\hat{\xi}^{[l_1-1]}
			\rangle_{\mathbb{H}}
			\hat{\boldsymbol{\rho}}^{[l_1-1] \top
			} 
			\biggr)
			\widetilde{\boldsymbol{W}}^{[l_1-1]}
			\\	&=	
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_1-1]} - 
			\frac{
				\langle 	 	\hat{\boldsymbol{\rho}}^{[l_2] \top}
				\widetilde{\boldsymbol{W}}^{[l_1-1]}  ,
				\hat{\xi}^{[l_1-1]}
				\rangle_{\mathbb{H}}
			}{\| 	\hat{\boldsymbol{\rho}}^{[l_1-1]}\|_2^2}
			%%%%%%%%%%%%%%%%%%%%%%%%%
			\bigl(
			\hat{\boldsymbol{\rho}}^{[l_1-1] \top
			} 
			\widetilde{\boldsymbol{W}}^{[l_1-1]}
			\bigr)
			\\	&=	h^{[l_1-1]} \bigl( \hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_1-1]} \bigr)~(\text{say}).
		\end{align*}
		Here, the function $h^{[l_1-1]}: \tilde{ \mathbb{H} } \mapsto \widetilde{\mathbb{H}}$ maps the zero element of $\widetilde{\mathbb{H}}$ to itself, where the zero element represents an ordered pair of $K$ zero functions and a zero matrix. Using this relationship, we have
		\begin{align*}
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_1]}  
			%%%%%%%%%%%%%%%%
			&=	h^{[l_1-1]} \bigl( \hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_1-1]} \bigr)
			%%%%%%%%%%%%%%%%
			\\&=
			h^{[l_1-1]} 
			\circ 
			h^{[l_1-2]}
			\bigl( \hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_1-2]} \bigr)
			%%%%%%%%%%%%%%%%
			\\&=
			h^{[l_1-1]} 
			\circ 
			h^{[l_1-2]}
			\circ 
			\ldots
			\circ 
			h^{[l_2]}
			\bigl( \hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_2]} \bigr).
		\end{align*}
		Therefore,
		to show that $\widehat{\boldsymbol{\rho}}^{[l_1 ] \top}
		\widehat{\boldsymbol{\rho}}^{[l_2]} = 0$,
		it suffices to show $h^{[l_2]}
		\bigl( \hat{\boldsymbol{\rho}}^{[l_2] \top}
		\widetilde{\boldsymbol{W}}^{[l_2]} \bigr) = 0$:
		%
		\begin{align*}
			h^{[l_2]} \bigl( \hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_2]} \bigr)
			&=
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_2]} - 
			\frac{
				\langle 	 	\hat{\boldsymbol{\rho}}^{[l_2] \top}
				\widetilde{\boldsymbol{W}}^{[l_2]}  ,
				\hat{\xi}^{[l_2]}
				\rangle_{\mathbb{H}}
			}{\| 	\hat{\boldsymbol{\rho}}^{[l_2]}\|_2^2}
			%%%%%%%%%%%%%%%%%%%%%%%%%
			\bigl(
			\hat{\boldsymbol{\rho}}^{[l_2] \top
			} 
			\widetilde{\boldsymbol{W}}^{[l_2]}
			\bigr)
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_2]} - 
			\frac{
				\hat{\boldsymbol{\rho}}^{[l_2] \top}
				\langle 	 
				\widetilde{\boldsymbol{W}}^{[l_2]}  ,
				\hat{\xi}^{[l_2]}
				\rangle_{\mathbb{H}}
			}{\| 	\hat{\boldsymbol{\rho}}^{[l_2]}\|_2^2}
			% 
			\bigl(
			\hat{\boldsymbol{\rho}}^{[l_2] \top
			} 
			\widetilde{\boldsymbol{W}}^{[l_2]}
			\bigr)
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_2]} - 
			\frac{
				\hat{\boldsymbol{\rho}}^{[l_2] \top}
				\hat{\boldsymbol{\rho}}^{[l_2]  } 
			}{\| 	\hat{\boldsymbol{\rho}}^{[l_2]}\|_2^2}
			% 
			\bigl(
			\hat{\boldsymbol{\rho}}^{[l_2] \top
			} 
			\widetilde{\boldsymbol{W}}^{[l_2]}
			\bigr)
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			\\&=
			\hat{\boldsymbol{\rho}}^{[l_2] \top}
			\widetilde{\boldsymbol{W}}^{[l_2]} 
			- 
			\hat{\boldsymbol{\rho}}^{[l_2] \top
			} 
			\widetilde{\boldsymbol{W}}^{[l_2]}
			\\&=0.
		\end{align*}
		This completes the proof of Proposition \ref{proposition: orthnormality of PLS scores}.
	\end{proof}
	
	
	
	
\end{document}
