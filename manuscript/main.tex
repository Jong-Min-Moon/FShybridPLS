\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[comma,authoryear]{natbib} 
\usepackage[margin=1in]{geometry}

\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{verbatim}

\newtheorem{proposition}{Proposition}
\newtheorem{alg}{Algorithm} 
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{note}[theorem]{Note}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{define}[theorem]{Definition}
% \newtheorem{denote}[theorem]{denote}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}{Lemma}
\usepackage{graphicx}
\graphicspath{ {images/} }
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[flushleft]{threeparttable}
\usepackage[labelfont=bf]{caption}
\usepackage{caption,booktabs,array}
\usepackage{multirow}
\usepackage{makecell}
\usepackage[usenames, dvipsnames]{color}
\usepackage{setspace}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{rotating}
\usepackage{url}
\usepackage{enumerate}
\allowdisplaybreaks
\usepackage{bbold}
\usepackage{xcolor}         % colors
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}

\setstretch{1.8} % For 25 lines per page

\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\jongmin}[1]{
	{ \color{teal} JM: #1}
}




\title{Partial Least Squares Regression with Multiple Functional and Scalar Predictors}
\author{Jongmin Mun and Jeong Hoon Jang}
\date{\today}

\begin{document}
	\maketitle
	
	
	\section{Introduction} \label{sec: Introduction}
	Functional regression settings involving a large number of predictors, $X_{i1}(t), X_{i2}(t), \ldots, X_{iK}(t)$, are becoming increasingly common. For example, \cite{storeySignificanceAnalysisTime2005} analyzes two gene expression datasets measured over time, which involve only a small number of patients but tens of thousands of functional predictors.
	
	
	
	Infinite dimensional beta estimation problem (ill-posed problem)
	
	Multicollinearity and high-dimensionality in scalar predictors are not handled.
	
	Also, there may be high correlation between functional and scalar predictors.
	
	
	
	
	Many approaches have been proposed
	1. Roughness Penalty
	2. basis approach (power-series, B-splines, wavelets)
	- Power Series splines (Goldsmith et al., 2011, Journal of Computational Statistics)
	- B-splines (H. Cardot, F. Ferraty, P. Sarda, Spline estimators for the functional linear model, Statistica Sinica 13 (2003) 571–591.
	T.T.Cai,P.Hall,Prediction in functional linear regression, Ann.Stat. 34(5) (2006) 2159–2179.)
	- Wavelet (Y.Zhao,R.T.Ogden,P.T.Reiss,Wavelet-based lasso in functional linear regression, J.Comput.Graph.Stat.21(3)(2012)600–61.)7
	
	3. FPCA approach
	- Hall, P. and Horowitz, J. L. (2007), ‘Methodology and convergence rates for functional linear regression’, The Annals of Statistics 35(1), 70–91.
	- Reiss P. T.,  Ogden R. T.. Functional principal component regression and functional partial least squares, Journal of the American Statistical Association, 2007, vol. 102 (pg. 984-996)
	- Febrero-Bande et al., 2017
	
	4. FPLS Approach
	- Preda and Saporta (2005)
	- Reiss and Odgen, 2007
	- Aguilera AM, Escabias M, Preda C, Saporta G. Using basis expansions for estimating functional PLS regression applications withchemometric data.Chemom Intell Lab Syst. 2010;104:289-305.
	- Delaigle and Hall, 2012
	- Febrero-Bande et al., 2017
	- Beyaztas and Shang, (2022) A Robust Functional Partial Least Squares for Scalar-on-Multiple-Function Regression. Journal of Chemometrics.
	- Saricam et al., On partial least-squares estimation in scalar-on-function regression models. Journal of Chemometrics.
	- Mutis  et al. (2025) On function-on-function linear quantile regression
	
	
	
	
	
	\section{Data Objects and Model Formulation} 
	We assume, without loss of generality, that the functional predictors $X_{i1}(t), X_{i2}(t), \ldots, X_{iK}(t)$ are observed over the domain $0 \leq t \leq 1$. We also assume that each functional predictor belongs to the space $L^2([0,1])$, the Hilbert space of square-integrable real-valued functions with respect to the Lebesgue measure $dt$ on $[0,1]$.
	
	
	
	\begin{remark}
		Our method is applicable to settings where each functional predictor belongs to a different Hilbert space, possibly defined over a distinct compact domain in $\mathbb{R}^d$, for arbitrary $d$, and observed at different time points. However, for notational simplicity, our discussion assumes a common Hilbert space over domain $[0,1]$ for all functional predictors.
	\end{remark} 
	
	Write  $X=(X^{(1)}, \ldots, X^{(K)})$ as a multivariate functional object that belongs to $\mathcal{F} = L^2(\mathcal{T}_1) \times \cdots \times L^2([0,1])$---a cartesian product of individual $L^2([0,1])$ spaces. Note that if $K=1$, $X$ reduces to a univariate functional object. We can also express the functional object $X$ evaluated on the multi-dimensional argument $\mathbf{t} = (t_1, \ldots, t_k)^\top \in \mathcal{T} = \mathcal{T}_1 \times \cdots \times [0,1]$ as a $K$-dimensional vector $X(\mathbf{t})=(X^{(1)}(t_1), \ldots, X^{(K)}(t_K))^\top$. The inner product of $f_1=(f_1^{(1)}, \ldots,f_1^{(K)}) $ and $f_2=(f_2^{(1)}, \ldots,f_2^{(K)})$ in $\mathcal{F}$ is defined as $\langle f_1, f_2\rangle_\mathcal{F} =  \sum_{k=1}^K \langle f_1^{(k)}, f_2^{(k)}\rangle_{L^2} = \sum_{k=1}^K \int_0^1 f_1^{(k)}(t_k) f_2^{(k)}(t_k) dt_k$ with norm $\Vert f_1 \Vert_\mathcal{F} = \langle f_1,f_1 \rangle_\mathcal{F}^{1/2} = \{ \sum_{k=1}^K \int_0^1 f_1^{(k)}(t_k)^2 dt_k\}^{1/2}$.  
	
	Let $\mathbf{Z}=(Z_1, \ldots, Z_p)^\top$ denote a $p$-dimensional multivariate scalar data  in $\mathbb{R}^p$. We assume that $\mathbf{Z}$ is a random vector with finite first two moments and equipped with the Euclidean inner product and norm; i.e., for $\mathbf{v_1}=(v_{11}, \ldots, v_{1p})^\top$ and $\mathbf{v_2}=(v_{21}, \ldots, v_{2p})^\top$ in $\mathbb{R}^p$, $\langle \mathbf{v_1}, \mathbf{v_2} \rangle = \mathbf{v_1}^\top \mathbf{v_2} = \sum_{r=1}^p v_{1r}v_{2r}$, and $\Vert \mathbf{v}_1 \Vert = \langle \mathbf{v}_1, \mathbf{v}_1 \rangle^{1/2} = (\sum_{r=1}^p v_{1r}^2)^{1/2}$.
	
	
	Without loss of generality, assume that both multivariate functional and scalar data are centered; that is, $E(X)=0$ and $E(\mathbf{Z})=\mathbf{0}$. Our goal is to predict a real outcome $Y$ based on $X$ and $\mathbf{Z}$ via the following functional regression model:
	
	\begin{equation}
		Y = \sum \limits_{r=1}^p \alpha_r Z_r + \sum \limits_{k=1}^K \int_0^1 \beta^{(k)}(t_k) X^{(k)}(t_k) dt_k + \epsilon = \langle \boldsymbol{\alpha}, \mathbf{Z} \rangle +  \langle \beta, X \rangle_{\mathcal{F}}  + \epsilon
		\label{eq: functional model}
	\end{equation}
	where $\boldsymbol{\alpha}=(\alpha_1, \ldots, \alpha_p)^\top \in \mathbb{R}^p$ and $\beta=(\beta^{(1)}, \ldots, \beta^{(K)}) \in \mathcal{F}$ are respectively the regression coefficient vector and function that characterize the effect of scalar and functional predictors on the outcome.
	
	
	Our strategy is to formulate a hybrid random object, 
	by combining   $X$ and $\mathbf{Z}$ into an ordered pair:
	\begin{equation}\label{def:hybid_predictor}
		\mathbf{W} = (X, \mathbf{Z}) 
		\in \mathcal{H},~\text{where}~\mathcal{H}:=\mathcal{F} \times \mathbb{R}^p
	\end{equation}
	An alternative notation for the hybrid object can be obtained by evaluating its functional part on $\mathbf{t}$ and expressing it as a $(K+p)$-dimensional vector: $\mathbf{W}[\mathbf{t}] = (X(\mathbf{t}), \mathbf{Z})^\top$, with $ X(\mathbf{t}) = (X^{(1)}(t_1), \ldots, X^{(K)}(t_K))^\top \in \mathbb{R}^K$ and $ \mathbf{Z} = (Z_1, \ldots, Z_p)^\top  \in \mathbb{R}^p$. 
	
	\begin{definition}[Hilbert space of hybrid objects]\label{def:hilbert_space}
		We define the inner product between any two hybrid objects,
		$\mathbf{h}_1 = (f_1, \mathbf{v}_1)$ and $\mathbf{h}_2 = (f_2, \mathbf{v}_2)$, as
		\begin{equation}
			\langle \mathbf{h}_1, \mathbf{h}_2\rangle_{\mathcal{H}} 
			:= 
			\langle f_1, f_2\rangle_\mathcal{F} 
			+
			\omega \langle  \mathbf{v_1}, \mathbf{v_2} \rangle = \sum \limits_{k=1}^K \int_0^1 f_1^{(k)}(t_k) f_2^{(k)}(t_k) dt_k
			+
			\omega \sum \limits_{r=1}^p v_{1r}v_{2r}, 
			\label{eq: hybrid inner product}
		\end{equation}
		with norm $\Vert \cdot \Vert_\mathcal{H} = \langle \cdot,\cdot \rangle_\mathcal{H}^{1/2}$.
	\end{definition}
	In \eqref{eq: hybrid inner product}, 
	$\omega$ is a positive weight that needs to be pre-specified or estimated. It is mainly used to take into account heterogeneity between functional and scalar parts in terms of measurement scale and/or amount of variation (see Section \ref{subsec: Data Preprocessing}). Without loss of generality and for the clarity of illustration, all the following theoretical results will be derived for $\omega=1$. The results remain valid for any positive weights. Then the functional regression model \eqref{eq: functional model} with multiple functional and scalar predictors can be re-expressed in terms of the scalar-on-hybrid regression model as follows
	\begin{equation}
		Y = \langle \boldsymbol{\eta}, \mathbf{W} \rangle_{\mathcal{H}} + \epsilon,
		\label{eq: Hybrid functional model}
	\end{equation}
	where $\boldsymbol{\eta} = (\beta, \boldsymbol{\alpha}) \in \mathcal{H}$ is a hybrid regression coefficient characterizing the association between the hybrid predictor $\mathbf{W}$ and the outcome $Y$.
	
	
	The basic idea of partial least squares (PLS) is to simultaneously decompose the hybrid predictor $\mathbf{W}$ and the real outcome $Y$ in terms of zero mean uncorrelated PLS scores $(\rho_l)_{l \in \mathbb{N}}$ with maximum predictive performance as follows
	\begin{equation}
		\mathbf{W}[\mathbf{t}] = \sum \limits_{l=1}^{\infty} \rho_{l} \boldsymbol{\delta}_l[\mathbf{t}] \quad \textnormal{and} \quad Y = \sum \limits_{l=1}^{\infty}\rho_{l} \nu_l + \epsilon,
		\label{eq: decomposition}
	\end{equation}
	where $(\boldsymbol{\delta}_l)_{l \in \mathbb{N}} \in \mathcal{H}$ and $(\nu_l)_{l \in \mathbb{N}} \in \mathbb{R}$ are appropriate bases. The PLS scores are obtained as $\rho_l =  \langle \mathbf{W}^{[l]}, \xi_l \rangle_{\mathcal{H}}$, where $\mathbf{W}^{[l]}$ denotes the residualized predictor sequentially derived as the residual of the regression of $\mathbf{W}^{[l-1]}$ on $\rho_{l-1}$ with $\mathbf{W}^{[1]} = \mathbf{W}$, and $\xi_l=(\psi_l, \boldsymbol{\theta}_l) \in \mathcal{H}$ is the hybrid PLS component, with $\psi_l = (\psi^{(1)}_l, \ldots, \psi_l^{(K)}) \in \mathcal{F}$ and $\boldsymbol{\theta}_l=(\theta_{l1}, \ldots, \theta_{lp})^\top \in \mathbb{R}^p$, sequentially chosen to maximize the squared covariance between $\rho_l$ and the residualized outcome $Y^{[l]}$---i.e., $\textnormal{Cov}^2(\rho_l, Y^{[l]}) = \textnormal{Cov}^2(\langle \mathbf{W}^{[l]}, \xi_l \rangle_\mathcal{H}, Y^{[l]})$. Here, $Y^{[l]}$ is also sequentially obtained as the residual of the regression of $Y^{[l-1]}$ on $\rho_{l-1}$ with $Y^{[1]} = Y$. The sequentially derived hybrid PLS components \{$\xi_l\}_{l=1}^\infty$, are orthonormal in a sense that $\langle \xi_l, \xi_j \rangle_\mathcal{H} = \mathbb{1}(l=j)$.
	
	
	
	\underline{Challenges}
	% The pointwise algorithm is highly inefficient and may not be even feasible for multiple dense and/or irregular functional data. Moreover, the pointwise least squares estimator can show substantial variability across the domain, which becomes problematic for the entire algorithm as its instability passes on to subsequent residualized outcomes and PLS components. Therefore, in this section, we introduce a novel framework for fitting a regression with a hybrid response and scalar predictor. The framework is readily applicable to dense and irregular functional data, and supports regularization techniques to prevent overfitting and reduce variance.
	
	\begin{enumerate}
		\item Independent variables consist of multiple highly structured images and scalar predictors.
		\item Our sample size is small compared to the dimension and number of functional and scalar predictors.
		\item Existing partial least squares (PLS) methods can only accommodate (i) univariate or multivariate functional predictors without any scalar predictors \citep{Preda2005, Delaigle2012, Febrero2017, Beyaztas2020}; or (2) a univariate functional predictor with other scalar predictors \citep{Wang2018}.
	\end{enumerate}
	
	
	
	
	\section{Naive PLS Algorithm} \label{sec: Naive PLS Algorithm}
	This section introduces a naive pointwise hybrid PLS algorithm and highlights its limitations. The algorithm treats each functional predictor as a long vector of discretized points, derives PLS components independently at each observed time point $\mathbf{t}$, and aggregates them into a functional object to compute the PLS score.
	Since the regression coefficient computation follows directly from the PLS components and scores, our discussion here focuses on the latter and omits the former.
	Denote $\{(Y_1, \mathbf{W}_1),  \ldots, (Y_n, \mathbf{W}_n)\}$ as $n$ independent data pairs (observed sample) distributed as $(Y, \mathbf{W})$. The goal of the PLS algorithm is to decompose the predictor $\mathbf{W}_i$ $(i=1, \ldots, n)$ and the real response $Y_i$ in terms of zero mean uncorrelated PLS scores  $(\rho_{il})_{l \in \mathbb{N}}$ with maximum predictive performance. Here and below, a superscript in square brackets denotes the iteration index of the algorithm. We begin by denoting the centered data as
	\begin{equation}\label{def:centering_naive}
		Y_i^{[1]} = Y_i - \bar{Y}~\text{and}~\mathbf{W}_i^{[1]}=(X_i^{[1]}, \mathbf{Z}_i^{[1]})=(X_i - \bar{X}, \mathbf{Z}_i - \bar{\mathbf{Z}}) = \mathbf{W}_i - \bar{\mathbf{W}}. 
	\end{equation}
	We describe the $l$-th step of the algorithm for $l = 1, 2, \ldots, L$, which consists of the following substeps. The complete procedure is summarized in Algorithm~\ref{alg:naive}.
	%
	\begin{algorithm}[t!]
		\caption{Naive Hybrid PLS (regression coefficient omitted)}
		\label{alg:naive}
		\begin{algorithmic}[1]
			\Require 
			Hybrid predictors $\{ \mathbf{W}_1,  \ldots, \mathbf{W}_n\} = \{(X_1, Z_1),  \ldots, (X_n, Z_n)\}$,
			responses $\{Y_1,   \ldots, Y_n\}$,
			functional predictor evaluation points $\mathbf{t}_1, \ldots, \mathbf{t}_m \in [0,1]^K$
			%%%%%%%%%%%%%%%%%%%%%
			\For{ i = 1, 2, \ldots  $n$} \Comment{Centering \eqref{def:centering_naive}}
			\State $\{ (Y_i^{[1]}, X_i^{[1]},Z_i^{[1]}) \} 
			\leftarrow 
			\{
			(Y_i - \bar{Y}, X_i - \bar{X},  Z_i - \bar{Z})
			\}$
			\EndFor
			\For{ l = 1, 2, \ldots  $L$}
			\Statex{\quad \textbf{PLS component computation step:}}
			\For{ $\mathbf{t} = \mathbf{t}_1 , \ldots  \mathbf{t}_m$} \Comment{Compute PLS loading}
			\State 
			$
			\widehat{\xi}_{l}[\mathbf{t}] 
			\leftarrow 
			\dfrac{\sum_{i=1}^n Y_i^{[l]} \mathbf{W}_i^{[l]}[\mathbf{t}]}{\Vert \sum_{i=1}^n Y_i^{[l]} \mathbf{W}_i^{[l]} \Vert_{\mathcal{H}}}
			$
			\Comment{\eqref{def:pls_loading_ptws}}
			\EndFor
			\State $  \{ \widehat{\rho}_{1l}, \ldots, \widehat{\rho}_{nl} \} \leftarrow
			\bigl\{
			\langle \mathbf{W}_{1}^{[l]}, \widehat{\xi}_{l} \rangle_{\mathcal{H}},
			\ldots,
			\langle \mathbf{W}_{n}^{[l]}, \widehat{\xi}_{l} \rangle_{\mathcal{H}}
			\bigr\}
			$
			\Comment{Compute PLS scores \eqref{def:pls_score_naive}}
			\Statex{\quad  \textbf{Orthogonalization step:}}
			\For{ $\mathbf{t} = \mathbf{t}_1 , \ldots  \mathbf{t}_m$} 
			\State 	$
			\widehat{\boldsymbol{\delta}}_{l}[\mathbf{t}] \leftarrow \dfrac{\sum_{i=1}^n \mathbf{W}_i^{[l]}[\mathbf{t}] \widehat{\rho}_{il}}{\sum_{i=1}^n \widehat{\rho}_{il}^2}.
			$
			\EndFor
			\State
			$\mathbf{W}_i^{[l+1]}[\mathbf{t}] \leftarrow \mathbf{W}_i^{[l]}[\mathbf{t}] - \widehat{\boldsymbol{\delta}}_{l}[\mathbf{t}] \widehat{\rho}_{il}$
			\State 	$
			\widehat{\nu}_l \leftarrow \dfrac{\sum_{i=1}^n Y_i^{[l]} \widehat{\rho}_{il}}{\sum_{i=1}^n \widehat{\rho}_{il}^2}
			$
			\State
			$Y_i^{[l+1]} \leftarrow Y_i^{[l]} - \widehat{\nu}_{l} \widehat{\rho}_{il}$ 
			\EndFor
			\State Output PLS scores $\bigl\{ \widehat{\rho}_{il} \bigr\}_{i \in [n], l \in [L]}$
		\end{algorithmic}
	\end{algorithm}
	%
	\begin{enumerate}[Step 1.]
		\item
		We compute the $l$th PLS score of the $i$th subject as
		\begin{equation}\label{def:pls_score_naive}
			\widehat{\rho}_{il} = \langle \mathbf{W}_{i}^{[l]}, \widehat{\xi}_{l} \rangle_{\mathcal{H}} = \langle X_i^{[l]}, \widehat{\psi}_{l} \rangle_{\mathcal{F}} + \langle \mathbf{Z}_i^{[l]}, \widehat{\boldsymbol{\theta}}_{l} \rangle, 
		\end{equation}
		where the $l$-th PLS loading $\widehat{\xi}_{l} = (\widehat{\psi}_{l}, \widehat{\boldsymbol{\theta}}_{l}) \in \mathcal{H}$ maximizes $\widehat{\textnormal{Cov}}^2(\langle \mathbf{W}_{i}^{[l]}, \widehat{\xi}_{l} \rangle_{\mathcal{H}}, Y_i^{[l]})=\widehat{\textnormal{Cov}}^2(\widehat{\rho_{il}}, Y_i^{[l]})$. Specifically, we define the $l$-th PLS loadings pointwise as follows:
		\begin{equation}\label{def:pls_loading_ptws}
			\widehat{\xi}_{l}[\mathbf{t}] = \frac{\sum_{i=1}^n Y_i^{[l]} \mathbf{W}_i^{[l]}[\mathbf{t}]}{\Vert \sum_{i=1}^n Y_i^{[l]} \mathbf{W}_i^{[l]} \Vert_{\mathcal{H}}} = \left[ \frac{\sum_{i=1}^n Y_i^{[l]} X_i^{[l]}(\mathbf{t})}{\Vert \sum_{i=1}^n Y_i^{[l]} \mathbf{W}_i^{[l]} \Vert_{\mathcal{H}}}, \frac{\sum_{i=1}^n Y_i^{[l]}\mathbf{Z}_i^{[l]}}{\Vert \sum_{i=1}^n Y_i^{[l]} \mathbf{W}_i^{[l]} \Vert_{\mathcal{H}}} \right]^\top,
		\end{equation}
		where the normalizing factor  is computed as
		\begin{equation}
			\Vert \sum_{i=1}^n Y_i^{[l]} \mathbf{W}_i^{[l]} \Vert_{\mathcal{H}} = [\sum_{i=1}^n \sum_{j=1}^n Y_i^{[l]} Y_j^{[l]} \{\langle X_i^{[l]},X_j^{[l]} \rangle_\mathcal{F} + \langle \mathbf{Z}_i^{[l]}, \mathbf{Z}_j^{[l]} \rangle \}]^{1/2}.  
		\end{equation}
		
		\item Obtain the subsequent residualized outcomes and predictors as $Y_i^{[l+1]} = Y_i^{[l]} - \widehat{\nu}_{l} \widehat{\rho}_{il}$ and $\mathbf{W}_i^{[l+1]}[\mathbf{t}] = \mathbf{W}_i^{[l]}[\mathbf{t}] - \widehat{\boldsymbol{\delta}}_{l}[\mathbf{t}] \widehat{\rho}_{il}$, where 
		\begin{equation*}
			\widehat{\nu}_l = \frac{\sum_{i=1}^n Y_i^{[l]} \widehat{\rho}_{il}}{\sum_{i=1}^n \widehat{\rho}_{il}^2} \quad \textnormal{and} \quad \widehat{\boldsymbol{\delta}}_{l}[\mathbf{t}] = \frac{\sum_{i=1}^n \mathbf{W}_i^{[l]}[\mathbf{t}] \widehat{\rho}_{il}}{\sum_{i=1}^n \widehat{\rho}_{il}^2}.
		\end{equation*}
		Note, $\widehat{\nu}_l$ is the least squares estimate from linear regression of $Y_i^{[l]}$ on $\widehat{\rho}_{il}$, and  $\widehat{\boldsymbol{\delta}}_{l}[\mathbf{t}]$ is the least squares estimate from a linear regression of $\mathbf{W}_i^{[l]}[\mathbf{t}]$ on $\widehat{\rho}_{il}$.
		
		\item Let $l=l+1$ and back to Step 1.
	\end{enumerate}
	
	This naive pointwise PLS algorithm, however, can be computationally expensive for multiple dense functional data and is not feasible for irregular functional data. Moreover, the pointwise estimates only use information from data on the particular argument value, and thus can show substantial variability across the domain, resulting in overfitting and unstable predictions. 
	
	
	
	
	
	
	
	\section{Proposed PLS Algorithm} \label{sec: Proposed PLS Algorithm}
	To address the limitations outlined in Section \ref{sec: Naive PLS Algorithm}, we propose  novel strategies for implementing the steps of the hybrid PLS algorithm. Our approach provides an efficient and robust means of producing PLS components and scores in the presence of multiple dense and/or irregular functional predictors and scalar predictors. It also incorporates a regularization scheme that enables the algorithm to borrow strength and exploit structural relationships within and between the functions to avoid overfitting of the PLS components and to improve the generalizability and interpretability of the predictive model.
	Each iteration of our approach consists of two subroutines: regularized estimation of smoothed PLS components and orthogonalization, detailed in Sections~\ref{subsec: Step 1: Computing Smoothed PLS components} and~\ref{subsec: Step 2: Computing Predictors via Hybrid-on-Scalar Regression}, respectively. After a suitable number of iterations, the hybrid regression coefficient is estimated, as described in Section~\ref{subsec: Obtaining the Estimated Hybrid Regression Coefficient}. For notational simplicity, we omit the iteration index $l$ in the following discussion, with the understanding that the subroutines apply to any iteration. The complete algorithm is summarized in Algorithm~\ref{alg:main}.
	
	\subsection{Step 1: Computing smoothed PLS components}  \label{subsec: Step 1: Computing Smoothed PLS components}
	The function part $\psi$ of the hybrid PLS component $\xi$ is inherently infinite-dimensional, rendering its estimation impossible with finite sample size. As such, we first approximate each functional predictor (observed version when $l=1$, or residualized version when $l \ge 2$) as a linear combination of $M$ basis functions and coefficients. 
	Due to the hybrid nature of the predictors, this approximation requires careful explanation, provided in Section \ref{sec: finite basis approximation}. This representation allows us to work with coefficient vectors, simplifying notation. Section~\ref{sec: pls component computation intermediate} outlines the core strategy for computing the PLS components in this reduced space. Building on this, we then introduce our regularized and smoothed estimation procedure, which constitutes the first step of the iterative algorithm.
	
	\subsubsection{Finite-basis approximation of the predictors and regression coefficients}\label{sec: finite basis approximation}	
	Given a basis $\{b_l(t)\}$ of $L^2([0,1])$, the functional predictors and the corresponding regression coefficients can be decomposed into
	\begin{equation*}
		X_{ij}(t) = \sum_{l=1}^\infty \theta_{ijl} \, b_l(t), \quad 
		\beta_j(t) = \sum_{l=1}^\infty \eta_{0,jl} \, b_l(t),
	\end{equation*}
	where $\theta_{ijl}$ and $\eta_{0,jl}$ are the coefficients of $X_{ij}(t)$ and $\beta_j(t)$ corresponding to the $l$th basis function $b_l(t)$, respectively.
	In practice, functional predictors are observed only at a finite set of time points rather than as complete curves. Rather than reconstructing infinite-dimensional object stated above via finite observations, we approximate each predictor using a finite-dimensional projection by truncating its basis expansion at a finite number $M$. This $M$ can be set to a sufficiently large value (e.g., 15 or 20) to capture a wide range of functional patterns. 
	Notably, it is unnecessary to fine-tune $M$, as a  penalization scheme introduced in Section \ref{sec: Regularization of PLS Components} regularizes each functional component to achieve the desired smoothness. We denote the projected predictors and coefficients as
	\begin{equation*}
		\widetilde{X}_{ij}(t) = \sum_{l=1}^M \theta_{ijl} \, b_l(t), \quad 
		\tilde{\beta}_j(t) = \sum_{l=1}^M \eta_{0,jl} \, b_l(t).
	\end{equation*}
	The $i$th  hybrid predictor with projected functional parts, is a tuple, denoted as
	\begin{equation*}
		\widetilde{W}_i := 
		(
		X_{i1}, \ldots, X_{iK}, \boldsymbol{Z}_i
		).
	\end{equation*}
	We adopt the following notation for the coefficients:
	\begin{equation}
		\boldsymbol{\theta}_{ij} := (\theta_{ij1}, \ldots, \theta_{ijM})^\top \in \mathbb{R}^M,
		\quad
		\boldsymbol{\eta}_{0j} := (\eta_{0,j1}, \ldots, \eta_{0,jM})^\top  \in \mathbb{R}^M,
	\end{equation}
	and
	\begin{equation}
		\Theta_j := (\boldsymbol{\theta}_{1j}, \ldots, \boldsymbol{\theta}_{nj})^\top  \in \mathbb{R}^{n \times M}.
	\end{equation}
	
	\begin{remark}
		While it is possible to use a different basis for each functional predictor, we adopt a common basis system for all predictors for notational simplicity.
	\end{remark}
	
	
	
	
	Additionally, it serves as a regularization mechanism, borrowing strength across components and helping to prevent overfitting. 	
	
	
	
	
	
	\subsubsection{Smoothed estimation of PLS components}\label{sec: pls component computation intermediate}
	Building on the basis function approximation from Section~\ref{sec: finite basis approximation}, we now introduce an intermediate strategy for implementing the first step of the PLS algorithm. This approach directly obtains a hybrid PLS component direction, sharing the same structure as the hybrid predictors presented in \eqref{eq: Hybrid functional model}, by simply solving a linear equation.
	We now describe the strategy for computing the PLS component direction at the $l$-th iteration. For simplicity, we omit the iteration index and assume that the observations have been residualized and are orthogonal to those from previous steps (see Section~\ref{subsec: Step 2: Computing Predictors via Hybrid-on-Scalar Regression} for the residualization procedure).
	
	The goal is to find a unit-norm direction $\xi \in \mathcal{H}$, whose functional components lie in $\operatorname{span} \bigl( b_1(t), \ldots, b_M(t) \bigr)$, that maximizes the empirical covariance between the resulting PLS component scores $\langle \widetilde{W}_1, \xi \rangle_{\mathcal{H}}, \ldots, \langle \widetilde{W}_n, \xi \rangle_{\mathcal{H}}$ and the responses $y_1, \ldots, y_n$.
	We denote the solution as
	\begin{equation}
		\hat{\xi} 
		= 
		(\hat{\xi}_1, \ldots, \hat{\xi}_K, \hat{\boldsymbol{\zeta}}),
	\end{equation}
	where $\hat{\boldsymbol{\zeta}} \in \mathbb{R}^p$ is the scalar part, and each functional part $\hat{\xi}_j \in L^2([0,1])$ lies in $\operatorname{span}(b_1(t), \ldots, b_M(t))$ and is expressed as
	\begin{equation*}
		\hat{\xi}_j(t) := 
		\sum_{m=1}^M d_{jm} \, b_m(t), \quad j = 1, \ldots, K.
	\end{equation*}
	
	Let $\mathbf{d}_j := (d_{j1}, \ldots, d_{jM})^\top \in \mathbb{R}^M$ and $\mathbf{y} := (y_1, \ldots, y_n)^\top$. 
	Let $B \in \mathbb{R}^{M \times M}$ denote the Gram matrix of the basis functions, with entries
	\[
	B_{m, m'} := \langle b_m, b_{m'} \rangle, \quad m, m' = 1, \ldots, M.
	\]
	We compute $\mathbf{d}_1, \ldots, \mathbf{d}_K$ and $\hat{\boldsymbol{\zeta}}$ as follows.
	First, we obtain the unnormalized coefficient vectors. For each $j$, the unnormalized vector $\bar{\mathbf{d}}_j$ is computed by solving
	\begin{equation*}
		B \bar{\mathbf{d}}_j = (\Theta_j B)^\top \mathbf{y},
	\end{equation*}
	The unnormalized scalar component is given by
	\begin{equation}
		\bar{\boldsymbol{\zeta}} := Z^\top \mathbf{y}.
	\end{equation}
	Next, we normalize all components with respect to the hybrid Hilbert norm (see Definition~\ref{def:hilbert_space}) to ensure $\hat{\xi}$ has unit length. Specifically, we define
	\begin{equation}\label{def:computed_PLS_direction}
		\mathbf{d}_j := 
		\frac{1}{
			\left(
			\sum_{j=1}^K \bar{\mathbf{d}}_j^\top B \bar{\mathbf{d}}_j +
			\bar{\boldsymbol{\zeta}}^\top \bar{\boldsymbol{\zeta}}
			\right)^{1/2}
		} \bar{\mathbf{d}}_j, \quad j = 1, \ldots, K,
	\end{equation}
	and
	\begin{equation}
		\hat{\boldsymbol{\zeta}} :=
		\frac{1}{
			\left(
			\sum_{j=1}^K \bar{\mathbf{d}}_j^\top B \bar{\mathbf{d}}_j +
			\bar{\boldsymbol{\zeta}}^\top \bar{\boldsymbol{\zeta}}
			\right)^{1/2}
		} \bar{\boldsymbol{\zeta}}.
	\end{equation}
	Details of the hybrid norm computation are provided in Appendix~\ref{section:proof:eigenproblem_derivation}.
	
The estimated $l$th PLS component score for the $i$th observations,
	denoted $\hat{\rho}_i$
	is computed via hybrid inner product. In a matrix multiplication form, it is computed as:
	\begin{align*}
	\hat{\rho}_i
		&:=
		\langle \widetilde{\mathbf{W}}_i, \hat{\xi} \rangle_{\mathcal{H}}
		=
		\sum_{j=1}^K \Theta_j B  \mathbf{d}_j
		+
			Z \hat{\boldsymbol{\zeta}} 
		. 
	\end{align*}
	The details of this computation are provided in Appendix~\ref{section:proof:eigenproblem_derivation}.
	Although the unnormalized coefficients are computed separately for the functional and scalar part of the PLS component direction, the normalization step couples them, allowing the components to influence each other. This enables the procedure to account for the correlation between functional and scalar components.
	
	
	%	consider the basis function expansions of the functional predictors $X_i^{(k)}(t_k) = \sum_{m=1}^M c_{im}^{(k)} b^{(k)}_{m}(t_k)$, $i=1, \ldots, n$, and those of the functional parts of the PLS component $\psi^{(k)}(t_k) = \sum_{m=1}^M a^{(k)}_{m} b^{(k)}_m(t_k)$, $k=1, \ldots, K$. Let 
	\begin{proposition} \label{Proposition: generalized rayleigh quotient}
		Let $\xi \in \mathcal{H}$ be any unit-norm direction such that its $1$st through $K$th functional components lie in $\operatorname{span} \bigl( b_1(t), \ldots, b_M(t) \bigr)$, respectively.
		Then, the empirical covariance between the projected component $\langle \widetilde{\mathbf{W}}, \xi \rangle_{\mathcal{H}}$ and the response $Y$, approximated in terms of the basis, is given by
		\begin{equation*}
			\widehat{\textnormal{Cov}} \left( \langle \widetilde{\mathbf{W}}, \xi \rangle_{\mathcal{H}}, Y \right)
			:=
			\frac{1}{n} 
			\sum_{i=1}^n
			y_i
			\langle \widetilde{\mathbf{W}}_i, \xi \rangle_{\mathcal{H}}.
		\end{equation*}
		Then the PLS component direction 
		$	\hat{\xi}$ 
		computed in \eqref{def:computed_PLS_direction} maximizes $\widehat{\textnormal{Cov}} (\langle \widetilde{\mathbf{W}}, \xi \rangle_{\mathcal{H}}, Y )$.
	\end{proposition}
	
	
	The proof of Proposition \ref{Proposition: generalized rayleigh quotient} is given in the Appendix \ref{section:proof:eigenproblem_derivation}.
	
	
	
	
	\subsubsection{Proposed Approach: Regularized and Smoothed Computation} \label{sec: Regularization of PLS Components}
	Although Proposition \ref{Proposition: generalized rayleigh quotient} provides an efficient and principled way to compute the PLS components $\xi$, their functional part $\psi=(\psi^{(1)}, \ldots, \psi^{(K)})$ may not be smooth, which can lead to overfitting and unstable predictions. Thus, in this section, we provide a modification of Proposition  \ref{Proposition: generalized rayleigh quotient} that yields regularized (smoothed) PLS components.
	
	
	
	A widely-used approach to smoothing a function $\psi^{(k)}$ is to penalize its roughness that can be quantified by the integrated squared second derivative:
	\begin{equation}
		\textnormal{PEN} \left(\psi^{(k)} \right) 
		=  \int_0^1 \ddot{\psi}^{(k)}(t)^2 dt = \mathbf{a}^{(k)T} \ddot{J}^{(k)} \mathbf{a}^{(k)},
		\label{eq: roughness penalty}
	\end{equation}
	where the last term is obtained using the basis expansion $\psi^{(k)}(t)=\mathbf{a}^{(k)T}\mathbf{b}^{(k)}(t)$.
	
	
	
	
	Now, maximizing $\widehat{\textnormal{Cov}}^2 \left(\langle \mathbf{W}, \xi \rangle_\mathcal{H}, Y \right)$ is not the only aim; we also want to prevent the roughness of each $\psi^{(k)}$---that is, $\textnormal{PEN}(\psi^{(k)})$---from being too large. One possible way to achieve this is to extend \citet{Rice1991}'s approach of obtaining smoothed functional PCA components, and find $\xi^*$ that maximizes:
	\begin{equation}
		\widehat{\textnormal{Cov}}^2 \left(\langle \mathbf{W}, \xi \rangle_\mathcal{H}, Y \right) - \sum \limits_{k=1}^K \lambda_k \textnormal{PEN} \left(\psi^{(k)} \right),
		\label{eq: penalized squred variance}
	\end{equation}
	subject to $\xi^{*\top} J^\ast \xi^*=1$. The equation \eqref{eq: penalized squred variance} quantifies the trade-off between fidelity to the data (in this case the sample covariance between the $\rho=\langle \mathbf{W}, \xi \rangle_\mathcal{H}$ and $Y$ in the direction of $\xi$) and roughness as measured by the sum of the individual penalty terms in \eqref{eq: roughness penalty}. The smoothing parameters $\{\lambda_k\}_{k=1}^K$ control the relative importance between the two objectives. Note that the penalty term can be written concisely using matrices as:
	\begin{equation*}
		\sum \limits_{k=1}^K \lambda_k \textnormal{PEN} \left(\psi^{(k)} \right) = \sum \limits_{k=1}^K \lambda_k \mathbf{a}^{(k)T} \ddot{J}^{(k)} \mathbf{a}^{(k)} = \xi^{* \top} \Lambda \ddot{J}^\ast \xi^*,
	\end{equation*}
	where $\Lambda \in \mathbb{R}^{(MK+p) \times (MK+p)}$ is defined as
	\begin{equation}\label{def:Lambda}
		\Lambda = \mathrm{blkdiag}(\lambda_1 I_M, \cdots, \lambda_K I_M, 0_{p \times p}) , 
	\end{equation}
	$0_{p \times p}$ denotes a $p \times p$ matrix with all elements equal to zero.
	
	
	In this paper, we take an alternative strategy that extends the idea of \citet{Silverman1996}. This approach is known to produce appropriately smoothed functional PCA components under milder conditions than the \citet{Rice1991}'s approach by replacing the usual $L^2$-orthonormality constraint with the one that takes account the roughness of functions via the modified inner product. Specifically, the following proposition provides a generalized Rayleigh quotient problem which modifies the $\mathcal{H}$-orthonormality constraint of the hybrid PLS components to incorporate roughness penalty, and thus whose solution corresponds to a smoothed PLS component maximizing $\widehat{\textnormal{Cov}}^2 \left(\langle \mathbf{W}, \xi \rangle_\mathcal{H}, Y \right)$ over the class of all functions satisfying sufficient smoothness.
	\begin{proposition} \label{Proposition: Regularized generalized rayleigh quotient}
		At each $l$-th iteration of the first step of the PLS algorithm involving the residualized response and predictors $Y_i \equiv Y_i^{[l]}$ and $\mathbf{W}_i \equiv \mathbf{W}_i^{[l]}$, consider the basis function expansions of the functional predictors $X_i^{(k)}(t_k) = \sum_{m=1}^M c_{im}^{(k)} b^{(k)}_{m}(t_k)$, $i=1, \ldots, n$, and those of the functional parts of the PLS component $\psi^{(k)}(t_k) = \sum_{m=1}^M a^{(k)}_{m} b^{(k)}_m(t_k)$, $k=1, \ldots, K$. Recall from \eqref{def:V_star} that  $V^\ast$ is defined as
		\begin{equation*}
			\mathbf{V}^\ast = n^{-2}\begin{bmatrix}
				J \widetilde{C}^\top \mathbf{y}\mathbf{y}^\top \widetilde{C} J & J \widetilde{C}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \mathbf{Z} \\
				\mathbf{Z}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C} J &   \mathbf{Z}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \mathbf{Z}
			\end{bmatrix} = n^{-2} J^\ast \widetilde{C}^{*\top} \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C}^{*} J^\ast
			\in \mathbb{R}^{(MK+p) \times (MK+p)}.
		\end{equation*}
		and  $\xi^*=(\mathbf{a}^{\top}, \boldsymbol{\theta}^{\top})^\top \in \mathbb{R}^{MK + p}$. Then, the $l$-th smoothed PLS component, $\widehat{\xi} \equiv \widehat{\xi}_l$ can be obtained in the following way.
		We first compute the basis coefficients by solving the optimization problem
		\begin{align}
			\begin{split}
				&\argmax \limits_{\xi^* \in \mathbb{R}^{MK+p}} \; \frac{\xi^{*\top} \mathbf{V}^\ast \xi^*}{\xi^{*\top}(J^\ast+\Lambda \ddot{J}^\ast) \xi^*} , \quad \textit{or equivalently}, \\ 
				&\argmax \limits_{\xi^* \in \mathbb{R}^{MK+p}} \;  \xi^{*\top} \mathbf{V}^\ast \xi^* \quad \textnormal{subject to}  \quad \xi^{*\top} (J^\ast+\Lambda \ddot{J}^\ast) \xi^*=1.
				\label{eq: Regularized generalized rayleigh quotient equation}
			\end{split}
		\end{align}
		We then express the smoothed component in terms of the basis functions as
		\begin{equation*}
			\widehat{\xi}[\mathbf{t}] = [\widehat{\psi}(\mathbf{t})^\top, \widehat{\boldsymbol{\theta}}^\top]^\top =  B^\ast[\mathbf{t}]^\top \widehat{\xi}^\ast.   
		\end{equation*}
		The $l$-th PLS score $\widehat{\rho}_{i} \equiv \widehat{\rho}_{il}$ can be obtained as 
		\begin{equation}\label{compute_pls_score}
			\widehat{\rho}_i = \langle \mathbf{W}_i, \widehat{\xi} \rangle_\mathcal{H}
		\end{equation}
	\end{proposition}
	The proof of Proposition \ref{Proposition: Regularized generalized rayleigh quotient} is provided in Appendix \ref{section:proof:eigenproblem_derivation}.
	Here, $\lambda_k$ $(k=1, \ldots, K)$ is a positive smoothing parameter that controls the trade-off between the goodness of fit and the amount of smoothness in $\psi^{(k)}$. Smaller $\lambda_k$ produces PLS components that fit more closely to the observed data, but selecting too small value may cause overfitting. The estimated PLS component reverts back to the unregularized version of Proposition \ref{Proposition: generalized rayleigh quotient} when $\lambda_k=0$ for all $k$. On the other hand, larger $\lambda_k$ places more emphasis on smoothing $\psi^{(k)}$. In the limit $\lambda_k \rightarrow \infty$, each of the functions is forced to be of the form $\psi^{(k)}(t)=a+bt$ for some constants $a$ and $b$. In practice, $\{\lambda_k\}_{k=1}^K$ as well as the number of PLS components $L$ to be estimated can be chosen by cross-validation based on a certain predictive performance criterion (e.g., mean squared error). 
	
	The constraint $\xi^{*\top} (J^\ast+\Lambda \ddot{J}^\ast) \xi^*=1$ enforces the following modified orthonormality of the estimated PLS components ($\widehat{\xi}_1, \widehat{\xi}_2, \ldots$)  based on the modified inner product $\langle \langle h_l, h_j \rangle \rangle = \langle h_l, h_j \rangle_{\mathcal{H}} + \sum_{k=1}^K \lambda_k \langle \ddot{f}_l^{(k)}, \ddot{f}_j^{(k)} \rangle_{L^2}$ which now takes into account the roughness of the functions:
	\begin{align*}
		\langle \langle \widehat{\xi}_l, \widehat{\xi}_j \rangle \rangle 
		&= \langle \widehat{\xi}_l, \widehat{\xi}_j \rangle_{\mathcal{H}} + \sum_{k=1}^K \lambda_k \left\langle \ddot{\widehat{\psi}}_l^{(k)}, \ddot{\widehat{\psi}}_j^{(k)} \right \rangle_{L^2} \\
		&= \sum \limits_{k=1}^K \int_0^1 \widehat{\psi}_l^{(k)}(t_k) \widehat{\psi}_j^{(k)}(t_k) dt_k + \sum \limits_{r=1}^p \widehat{\theta}_{lr} \widehat{\theta}_{jr} + \sum_{k=1}^K \lambda_k \int_0^1 \ddot{\widehat{\psi}}_l^{(k)}(t_k) \ddot{\widehat{\psi}}_j^{(k)}(t_k) dt_k \\
		& = \mathbb{1}(l=j). 
	\end{align*}
	This modified orthonormality condition is one of the main geometric properties of the proposed PLS algorithm and is formally stated with proof in Section \ref{sec: Properties of the Proposed PLS Framework} (see Proposition \ref{proposition: modified orthnormality of PLS components}), along with other properties. 
	
	
	To solve \eqref{eq: Regularized generalized rayleigh quotient equation} in practice, we first perform a Choleski factorization: $L L^\top = J^\ast + \Lambda \ddot{J}^\ast$, where $L$ is a lower triangular matrix, and then reduce it to maximizing the classical Rayleigh quotient problem:
	\begin{equation*}
		\max \limits_{\mathbf{e} \in \mathbb{R}^{MK + p}} \, \frac{\mathbf{e}^\top E \mathbf{e}}{\mathbf{e}^\top \mathbf{e}} \; \Longleftrightarrow \;   \max \limits_{\substack{\mathbf{e} \in \mathbb{R}^{MK+p}}} \;  \mathbf{e}^\top E \mathbf{e} \quad \textnormal{subject to} \quad \Vert \mathbf{e} \Vert^2 = 1
	\end{equation*}
	through the transformations $\mathbf{e} = L^\top \xi^*$ and $E = L^{-1} \mathbf{V}^\ast L^{\top-1}$. Once $\mathbf{e}$ is obtained as the first eigenvector of $E$, we set $\xi^* = L^{\top-1} \mathbf{e}$ and $\widehat{\xi}[\mathbf{t}] = B^*[\mathbf{t}]^\top \xi^*$.
	
	
	
	\subsection{Step 2: Computing Residualized Predictors via Hybrid-on-Scalar Regression} \label{subsec: Step 2: Computing Predictors via Hybrid-on-Scalar Regression}
	The second step of the PLS algorithm involves iteratively obtaining the ($l+1$)-th residualized predictors as $\mathbf{W}_i^{[l+1]}[\mathbf{t}] = \mathbf{W}_i^{[l]}[\mathbf{t}] - \widehat{\boldsymbol{\delta}}_{l}[\mathbf{t}] \widehat{\rho}_{il}$, where $\widehat{\boldsymbol{\delta}}_{l}[\mathbf{t}]$ is the least squares estimate from regressing the previous ($l$-th) residualized outcome $\mathbf{W}_i^{[l]}[\mathbf{t}]$ on the corresponding  estimated PLS score $\widehat{\rho}_{il}$.  The pointwise least squares estimate, $\widehat{\boldsymbol{\delta}}_{l}[\mathbf{t}] = (\sum_{i=1}^n \mathbf{W}_i^{[l]}[\mathbf{t}] \widehat{\rho}_{il})/(\sum_{i=1}^n \widehat{\rho}_{il}^2)$, presented in Section \ref{sec: Naive PLS Algorithm}, however, is  highly inefficient and may not be even feasible for multiple dense and/or irregular functional data. Moreover, the pointwise least squares estimator can show substantial variability across the domain, which becomes problematic for the entire algorithm as its instability passes on to subsequent residualized outcomes and PLS components. Therefore, in this section, we introduce a novel strategy based on a hybrid-on-scalar regression model for obtaining $\widehat{\boldsymbol{\delta}}_{l}[\mathbf{t}]$ and implementing the second step.
	
	Again, we omit $l$ in the notations. Let $\widehat{\boldsymbol{\rho}}=(\widehat{\rho}_1, \ldots, \widehat{\rho}_n)^\top$ denote an $n$-dimensional vector of PLS scores estimated from the proposed algorithm, and let the $(K+P)$-vector $\boldsymbol{\delta}[\mathbf{t}] = [\pi(\mathbf{t})^\top, \boldsymbol{\gamma}^\top]^\top$ denote the regression coefficient consisting of the functional part $\pi(\mathbf{t}) = (\pi^{(1)}(t_1), \ldots,\pi^{(K)}(t_K))^\top \in \mathbb{R}^K$ and scalar part $\boldsymbol{\gamma} = (\gamma_1, \ldots,\gamma_p)^\top \in \mathbb{R}^p$. The hybrid-on-scalar regression model we are interested in fitting is then:
	\begin{equation}
		\widetilde{W}[\mathbf{t}] = \widehat{\boldsymbol{\rho}} \, \boldsymbol{\delta}[\mathbf{t}]^\top + \mathbf{e}[\mathbf{t}],
		\label{eq: hybrid on scalar regression model}
	\end{equation}
	where $\mathbf{e}[\mathbf{t}]$ is an $n \times (K+p)$ matrix of errors. 
	
	As in Section \ref{subsec: Step 1: Computing Smoothed PLS components}, the hybrid response can be expanded using basis functions as $\widetilde{W}[\mathbf{t}] = \widetilde{C}^* B^*[\mathbf{t}]$. Similarly, we expand each functional regression coefficient using the basis functions as $\pi^{(k)}(t_k) =\sum_{m=1}^M d_m^{(k)} b^{(k)}_m(t_k) = \mathbf{d}^{(k)\top} \mathbf{b}^{(k)}(t_k)$, where $\mathbf{d}^{(k)} = (d_1^{(k)}, \ldots, d_M^{(k)})\top \in \mathbb{R}^M$ are basis coefficients. Here, a different basis system can be flexibly chosen to expand the regression coefficient, but without loss of generality we use the same system as in Section \ref{subsec: Step 1: Computing Smoothed PLS components}. With the $(MK+p)$-dimensional row vector $\mathbf{d}^* = (\mathbf{d}^{(1)\top}, \ldots, \mathbf{d}^{(K)\top}, \gamma_1, \ldots, \gamma_p)$ that stacks the basis coefficients and scalar regression coefficients, we can express the linear predictor of the regression as: $\widehat{\boldsymbol{\rho}} \, \boldsymbol{\delta}[\mathbf{t}]^\top = \widehat{\boldsymbol{\rho}} \, \mathbf{d}^* B^*[\mathbf{t}]$. Finally, these basis expansions of the hybrid response and the regression coefficient enable us to re-write the hybrid-on-scalar regression model \eqref{eq: hybrid on scalar regression model} in a finite-dimensional form:
	\begin{equation}
		\widetilde{C}^* B^*[\mathbf{t}] = \widehat{\boldsymbol{\rho}} \, \mathbf{d}^* B^*[\mathbf{t}] + \mathbf{e}[\mathbf{t}].
		\label{eq: rewritten hybrid on scalar regression model}
	\end{equation}
	
	Based on the model formulation \eqref{eq: rewritten hybrid on scalar regression model}, we can now formulate the following least squares criterion whose minimizing solution gives the estimate of the regression coefficient $\boldsymbol{\delta}[\mathbf{t}]$ in the hybrid-on-scalar regression model \eqref{eq: hybrid on scalar regression model}:
	\begin{equation}
		\begin{aligned}
			\textnormal{SSE}(\delta) &= \int_\mathcal{T} \left\Vert \,  \widetilde{C}^* B^*[\mathbf{t}] - \widehat{\boldsymbol{\rho}} \, \mathbf{d}^* B^*[\mathbf{t}] \, \right\Vert_F^2 \, d\mathbf{t}  \\
			&= tr\left(\widetilde{C}^{*T}\widetilde{C}^*J^\ast \right) - 2\,tr \left(\mathbf{d}^* J^\ast \widetilde{C}^{*T} \widehat{\boldsymbol{\rho}} \right) + tr\left(\widehat{\boldsymbol{\rho}}^\top \widehat{\boldsymbol{\rho}} \, \mathbf{d}^* J^\ast \mathbf{d}^{*T}\right),
		\end{aligned}
		\label{eq: least squares}
	\end{equation}
	where $tr$ denotes the trace of a matrix, and $\Vert \cdot \Vert_F$ is a Frobenius norm. Now taking the derivative of \eqref{eq: least squares} with respect to $\mathbf{d}^*$ and setting the result to zero, we find that  $\mathbf{d}^*$ satisfies the system of linear equations
	\begin{equation*}
		\widehat{\boldsymbol{\rho}}^\top \widehat{\boldsymbol{\rho}} \, \mathbf{d}^*  J^\ast =  \widehat{\boldsymbol{\rho}}^\top  C^* J^\ast,
	\end{equation*}
	whose solutions lead to the following estimates of $\mathbf{d}^*$ and $\boldsymbol{\delta}[\mathbf{t}]$:
	\begin{equation*}
		\widehat{\mathbf{d}}^*  =  \widehat{\boldsymbol{\rho}}^\top  C^* J^\ast \left(\widehat{\boldsymbol{\rho}}^\top \widehat{\boldsymbol{\rho}} J^\ast \right)^{-1} \quad \textnormal{and} \quad \widehat{\boldsymbol{\delta}}[\mathbf{t}] = B^*[\mathbf{t}]^\top \, \widehat{\mathbf{d}}^{*\top}.
	\end{equation*}
	Then, we can use $\widehat{\boldsymbol{\delta}}[\mathbf{t}]$ to obtain the subsequent residualized hybrid predictor as $\mathbf{W}_i^{[l+1]}[\mathbf{t}] = \mathbf{W}_i^{[l]}[\mathbf{t}] - \widehat{\boldsymbol{\delta}}_{l}[\mathbf{t}] \widehat{\rho}_{il}$. 
	
	The step for obtaining the subsequent residualized response is identical to that of the naive PLS algorithm in  in Section \ref{sec: Naive PLS Algorithm}. Specifically, the $(l-1)$-th residualized response can be obtained as $Y_i^{[l+1]} = Y_i^{[l]} - \widehat{\nu}_l \widehat{\rho}_{il}$, where $\widehat{\nu}_l = \sum_{i=1}^n Y_i^{[l]} \widehat{\rho}_{il}/\sum_{i=1}^n \widehat{\rho}_{il}^2$  is the least square estimate of the simple linear model regressing $Y_i^{[l]}$ on $\widehat{\rho}_{il}$.
	
	
	\subsection{Obtaining the Estimated Hybrid Regression Coefficient}
	\label{subsec: Obtaining the Estimated Hybrid Regression Coefficient}
	The regression coefficient $\boldsymbol{\eta}$ in model \eqref{eq: Hybrid functional model} can be written in terms of the estimated PLS components and scores. First, note that we can show $\widehat{\rho}_{il} = \langle \mathbf{W}_i^{[l]}, \widehat{\xi}_l \rangle_\mathcal{H} = \langle \mathbf{W}_i, \widehat{\boldsymbol{\zeta}}_l \rangle_\mathcal{H}$, where $\widehat{\boldsymbol{\zeta}}_l = \widehat{\xi}_l - \sum_{u=1}^{l-1} \langle \widehat{\boldsymbol{\delta}}_u, \widehat{\xi}_l \rangle_{\mathcal{H}} \widehat{\boldsymbol{\zeta}}_u$ for $l \ge 2$, with $\widehat{\boldsymbol{\zeta}}_1 = \widehat{\xi}_1$. Then, the decomposition of $Y_i$ in \eqref{eq: decomposition} leads to
	\begin{equation*}
		Y_i = \sum \limits_{l=1}^L \widehat{\nu}_l \langle \mathbf{W}_i^{[l]}, \widehat{\xi}_l \rangle_\mathcal{H} +\epsilon_i = \left\langle \mathbf{W}_i, \sum \limits_{l=1}^L\widehat{\nu}_l \widehat{\boldsymbol{\zeta}}_l \right\rangle_\mathcal{H}+\epsilon_i,
	\end{equation*}
	which, given the uniqueness of $\boldsymbol{\eta}$, leads to
	\begin{equation*}
		\widehat{\boldsymbol{\eta}} = \sum \limits_{l=1}^L\widehat{\nu}_l \widehat{\boldsymbol{\zeta}}_l.
	\end{equation*}
	Here, the number of PLS components $L$ to be estimated can be chosen by cross-validation.
	
	\begin{algorithm}[b!]
		\caption{Hybrid PLS (under construction)}
		\label{alg:main}
		\begin{algorithmic}[1]
			\Require 
			Response $\mathbf{y} \in \mathbb{R}^n$,
			%functional predictor evalulation points $\mathbf{t}_1, \ldots, \mathbf{t}_m \in [0,1]^K,$
			hybrid predictors approximation coefficient
			$\widetilde{C}^* \ \in \mathbb{R}^{n \times (MK+p)} $ \eqref{eq: basis approximation of W},
			Gram matrix with respect to basis functions
			$J^\ast \in \mathbb{R}^{(MK+p) \times (MK+p)} $
			\eqref{def:J_and_J_star},
			Gram matrix with respect to second derivatives of the basis functions 
			$\ddot{J}^\ast \in \mathbb{R}^{(MK+p) \times (MK+p)}$
			\eqref{def:J_dotdot_ast},
			regularization matrix $\Lambda  \in \mathbb{R}^{(MK+p) \times (MK+p)}$ \eqref{def:Lambda}
			%%%%%%%%%%%%%%%%%%%%%
			\State
			$\widetilde{C}^{*[1]} \leftarrow \widetilde{C}^*$
			\For{ l = 1, 2, \ldots  $T$}
			\State $\mathbf{V}^\ast \leftarrow n^{-2} J^\ast \widetilde{C}^{*\top} \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C}^{*} J^\ast$
			\Comment{ \eqref{def:V_star}}
			\State 	$\widehat{\xi}_l^\ast \leftarrow \argmax_{\xi^* \in \mathbb{R}^{MK+p}} \;  \xi^{*\top} \mathbf{V}^\ast \xi^* \quad \textnormal{subject to}  \quad \xi^{*\top} (J^\ast+\Lambda \ddot{J}^\ast) \xi^*=1
			$
			\Comment{ \eqref{eq: Regularized generalized rayleigh quotient equation}} 
			\State $\widehat{\boldsymbol{\rho}}_l
			\leftarrow 
			\widetilde{C}^* J^\ast  \widehat{\xi}_l^\ast
			$
			\Comment{$\in \mathbb{R}^n$; \eqref{compute_pls_score}}
			\EndFor
			\State Output a
		\end{algorithmic}
	\end{algorithm}
	
	
	\subsection{Data Preprocessing} \label{subsec: Data Preprocessing}
	Functional and scalar elements of the hybrid predictors often have incompatible units and/or exhibit different amounts of variation. This can be problematic for our PLS framework which is not scale invariant as: i) each predictor has different chance of contributing to the predictor/response structure; and ii) a predictor with high correlation to $Y$ but relatively low variance may be overlooked. 
	
	To obtain PLS components that have a meaningful interpretation, we standardize the predictor data via the following steps. The first step is to account for discrepancies \textit{within} respective functional and scalar parts, if needed. If elements of multivariate functional data $X_i = (X_i^{(1)}, \ldots, X_i^{(K)})$ are measured in different units or have quite different domains, one can standardize them to have mean zero and integrated variance of one. If multivariate scalar predictors $\mathbf{Z}_i=(Z_{i1}, \ldots, Z_{ip})^\top$ exhibit different amounts of variation, one can standardize them to have mean zero and unit variance. The second step is to eliminate the discrepancies \textit{between} functional and scalar parts. To accomplish this aim, we choose an appropriate weight $\omega$ in the hybrid inner product \eqref{eq: hybrid inner product} that ensures functional and vector parts have comparable variance. A sensible data-driven approach to choosing an appropriate weight is to set
	\begin{equation*}
		\omega = \frac{\sum_{i=1}^n \Vert X_i \Vert_\mathcal{F}^2}{\sum_{i=1}^n \Vert \mathbf{Z}_i \Vert^2},
		\label{eq: weight}
	\end{equation*}
	In practice, this weighting scheme can be implemented by formulating the hybrid object as $\mathbf{W} = (X, \omega^{1/2} \mathbf{Z})$, whose vector part has been scaled by a factor of $\omega^{1/2}$.
	
	
	
	\section{Properties of the Proposed PLS Framework}
	\label{sec: Properties of the Proposed PLS Framework}
	In this section, we derive some of the theoretical and geometric properties of the proposed PLS framework.
	
	\subsection{Tucker's Criterion}
	We derive that the PLS components obtained from the first step of the proposed algorithm satisfy the Tucker's Criterion \citep{Tucker1938} extended to our scalar-on-hybrid regression model setting \eqref{eq: Hybrid functional model}. We omit $l$ in the notations and first define the cross-covariance terms between the response and predictors (observed versions if $l=1$ and residualized versions if $l \ge 2$): 
	\begin{equation*}
		\begin{gathered}
			\sigma_{YX} = (\sigma_{YX}^{(1)}, \ldots, \sigma_{YX}^{(K)}) = (E(YX^{(1)}),\ldots, E(YX^{(K)})) = E(YX) \in \mathcal{F} \\
			\sigma_{YZ} = (\sigma_{YZ,1}, \ldots, \sigma_{YZ,p})^\top = (E(YZ_1),\ldots, E(YZ_p))^\top = E(Y\mathbf{Z}) \in \mathbb{R}^p \\
			\Sigma_{YW}=(\sigma_{YX}, \sigma_{YZ}) =  (E(YX), E(Y\mathbf{Z}))=E(Y\mathbf{W}) \in \mathcal{H}
		\end{gathered}
	\end{equation*}
	We also introduce two cross-covariance operators, $\mathcal{C}_{YW}=E(\mathbf{W} \otimes_\mathcal{H} Y): \mathcal{H} \rightarrow \mathbb{R}$ and $\mathcal{C}_{WY}=E(Y \otimes \mathbf{W}): \mathbb{R} \rightarrow \mathcal{H}$, which respectively map $h=(f, \mathbf{v}) \in \mathcal{H}$ to $\mathbb{R}$ and $d \in \mathbb{R}$ to $\mathcal{H}$ as follows:
	\begin{align*}
		& \mathcal{C}_{YW} \mathbf{h} = E\{\langle \mathbf{W}, \mathbf{h} \rangle_\mathcal{H} Y \} = \langle \Sigma_{YW}, \mathbf{h} \rangle_\mathcal{H} = \sum \limits_{k=1}^K \int_0^1 \sigma_{YX}^{(k)}(t_k) f^{(k)}(t_k) dt_k + E(Y \mathbf{Z}^\top) \mathbf{v} \\
		& \mathcal{C}_{WY} d = E (\langle Y, d \rangle \mathbf{W} ) = E(Y\mathbf{W})d =  \Sigma_{YW} \, d. 
	\end{align*}
	
	
	Now define a new operator $\mathcal{U} = \mathcal{C}_{WY} \circ \mathcal{C}_{YW}: \mathcal{H} \rightarrow \mathcal{H}$, which performs the following mapping:
	\begin{equation*}
		\mathcal{U} \mathbf{h} =  \mathcal{C}_{WY} ( \mathcal{C}_{YW} \mathbf{h}) 
		= \mathcal{C}_{YW} (\langle \Sigma_{YW}, \mathbf{h} \rangle_\mathcal{H}) 
		=  \Sigma_{YW} \, \langle \Sigma_{YW}, \mathbf{h} \rangle_\mathcal{H} = (\Sigma_{YW} \otimes \Sigma_{YW})\mathbf{h}.
	\end{equation*}
	In other words, $\mathcal{U} = \Sigma_{YW} \otimes \Sigma_{YW}$. The following proposition presents several important properties of $\mathcal{U}$.
	\begin{proposition} \label{Proposition: property of U}
		The operator $\mathcal{U} = \mathcal{C}_{WY} \circ \mathcal{C}_{YW}= \Sigma_{YW} \otimes \Sigma_{YW}: \mathcal{H} \rightarrow \mathcal{H}$ is positive and self-adjoint. Furthermore, if there exist finite constants $Q_1$ and $Q_2$ such that
		\begin{align}
			\max \limits_{k=1, \ldots, K} \sup \limits_{t_k \in [0,1]} \sigma_{YX}^{(k)2}(t_k) < Q_1 \quad \textnormal{and} \quad  \max_{r=1, \ldots, p} \sigma_{YZ, r}^2 < Q_2,
		\end{align}
		and if each $\sigma_{YX}^{(k)}$ ($k=1, \ldots, K)$ is uniformly continuous in a sense that for any $\epsilon > 0$, there exists $\delta_k > 0$ such that for every $t_k,t^*_k \in [0,1]$ with $|t_k - t_k^* | < \delta_k$, we have that
		\begin{equation*}
			|\sigma_{yx}^{(k)}(t_k) - \sigma_{yx}^{(k)}(t_k^*) | < \epsilon,
		\end{equation*}
		then $\mathcal{U}$ is a compact operator.
	\end{proposition}
	The proof of Proposition \ref{Proposition: property of U} is provided in Appendix \ref{section:proof:Proposition: property of U}.
	By the Hilbert-Schmidt theorem (e.g., Theorem 4.2.4 in \citealp{Hsing2015}), Proposition \ref{Proposition: property of U} guarantees the existence of a complete orthonormal system of eigenfunctions $\{\xi_{(u)}\}_{u \in \mathbb{N}}$ of $\mathcal{U}$ in $\mathcal{H}$ such that $\mathcal{U} \xi_{(u)} = \kappa_{(u)}  \xi_{(u)}$, where $\{\kappa_{(u)}\}_{u \in \mathbb{N}}$ are the corresponding sequence of eigenvalues that goes to zero as $u \rightarrow \infty$, that is, $\kappa_{(1)} \ge \kappa_{(2)} \ge \cdots \ge 0$.
	
	
	
	The following proposition introduces the Tucker's Criterion adapted to our scalar-on-hybrid regression model setting based on the aformentioned operators defined in the hybrid space.
	\begin{proposition}\label{proposition: eigenanalysis}
		\begin{equation*}
			\max \limits_{\substack{\xi \in \mathcal{H} \\ \Vert \xi \Vert_\mathcal{H}=1}} \textnormal{Cov}^2 \left(\langle \mathbf{W}, \xi \rangle_\mathcal{H}, Y \right)
		\end{equation*}
		is achieved when $\xi$ is an eigenfunction associated with the largest eigenvalue of $\mathcal{U}$.	
	\end{proposition}
	The proof of Proposition \ref{proposition: eigenanalysis} is provided in Appendix \ref{section:proof:proposition: eigenanalysis}.
	In other words, $l$-th PLS component $\xi_l$ can be obtained as the \textit{first} eigenfunction of $\mathcal{U}^{[l]} = \Sigma_{YW}^{[l]} \otimes \Sigma_{YW}^{[l]}$ whose components are formulated using the $l$-th residualized response and predictors: $\Sigma_{YW}^{[l]} = E(Y^{[l]} \mathbf{W}^{[l]})$.
	
	
	\subsection{Geometric Properties}
	In this section, we derive several geometric properties of the PLS components and and scores obtained from the proposed algorithm.
	
	The following proposition states that the PLS components estimated from Proposition  \ref{Proposition: Regularized generalized rayleigh quotient} are orthonormal with respect to the modified inner product $\langle \langle \cdot \rangle \rangle$ that incorporates the roughness of the functions.
	\begin{proposition}\label{proposition: modified orthnormality of PLS components}
		The estimated PLS components, $\widehat{\xi}_1, \widehat{\xi}_2, \ldots, \widehat{\xi}_L$, from the proposed PLS algorithm are mutually orthonormal with respect to the modified inner product $\langle \langle h_l, h_j \rangle \rangle = \langle h_l, h_j \rangle_{\mathcal{H}} + \sum_{k=1}^K \lambda_k \langle \ddot{f}_l^{(k)}, \ddot{f}_j^{(k)} \rangle_{L^2}$ in $\mathcal{H}$ that incorporates the roughness of the functions; i.e.,
		\begin{equation*}
			\langle \langle \widehat{\xi}_l, \widehat{\xi}_j \rangle \rangle 
			= \langle \widehat{\xi}_l, \widehat{\xi}_j \rangle_{\mathcal{H}} + \sum_{k=1}^K \lambda_k \left\langle \ddot{\widehat{\psi}}_l^{(k)}, \ddot{\widehat{\psi}}_j^{(k)} \right\rangle_{L^2}
			= \mathbb{1}(l=j). 
		\end{equation*}
	\end{proposition}
	The proof of Proposition \ref{proposition: modified orthnormality of PLS components} is provided in Appendix \ref{section:proof:proposition: modified orthnormality of PLS components}.
	Now let $\widehat{\boldsymbol{\rho}}_l=(\widehat{\rho}_{1l}, \ldots, \widehat{\rho}_{nl})^\top$ denote the $n$-dimensional vector whose elements consist of estimated $l$-th PLS scores ($l=1, \ldots, L)$ of $n$ observations. The next proposition states that the vectors of estimated PLS scores $\widehat{\boldsymbol{\rho}}_1, \widehat{\boldsymbol{\rho}}_2, \ldots, \widehat{\boldsymbol{\rho}}_L$ obtained from the proposed algorithm are mutually orthogonal.
	\begin{proposition}	\label{proposition: orthnormality of PLS scores}
		The vectors of estimated PLS scores, $\widehat{\boldsymbol{\rho}}_1, \widehat{\boldsymbol{\rho}}_2, \ldots, \widehat{\boldsymbol{\rho}}_L$, obtained from the proposed PLS algorithm are mutually orthogonal in the sense that
		\begin{equation*}
			\widehat{\boldsymbol{\rho}}_l^\top \widehat{\boldsymbol{\rho}}_j= 0 \quad \textnormal{for} \quad \; l,j \in \{1, \ldots, L\}, \; l \ne j. 
		\end{equation*}
	\end{proposition}
	The proof of Proposition \ref{proposition: orthnormality of PLS scores} is provided in Appendix \ref{section:proof:proposition: orthnormality of PLS scores}.
	
	\section{Simulations}
	To evaluate the superiority of our method under complex dependency structures
	{\textemdash}
	specifically, dependencies among functional predictors,
	among scalar predictors,
	and between scalar and functional predictors
	{\textemdash}
	
	\medskip
	\noindent
	\textit{Matrix-normal setting.}
	We begin by constructing predictors from a matrix-normal distribution, a framework that offers convenient and flexible control over the dependence structure across both rows and columns. Matrix-normal (MN) models, also known as Kronecker-separable covariance models, provide a principled approach to modeling multivariate data with structured covariance. Specifically, the matrix-normal distribution is defined as
	\begin{equation*}
		\mathbf{X} \sim \mathcal{MN}_{m \times n}(\mathbf{M}; \mathbf{R}, \mathbf{C}),
	\end{equation*}
	and its log-density is given by
	\[
	\log p(\mathbf{X} \mid \mathbf{M}, \mathbf{R}, \mathbf{C}) = -\frac{mn}{2} \log(2\pi) - \frac{m}{2} \log |\mathbf{C}| - \frac{n}{2} \log |\mathbf{R}| - \frac{1}{2} \mathrm{Tr}\left[ \mathbf{C}^{-1} (\mathbf{X} - \mathbf{M})^\top \mathbf{R}^{-1} (\mathbf{X} - \mathbf{M}) \right].
	\]
	The key insight behind Kronecker separability is that if
	$
	\mathbf{Y} \sim \mathcal{MN}(\mathbf{M}, \mathbf{R}, \mathbf{C}),
	$
	then its vectorized form follows a multivariate normal distribution:
	$
	\mathrm{vec}(\mathbf{Y}) \sim \mathcal{N}(\mathrm{vec}(\mathbf{M}), \mathbf{C} \otimes \mathbf{R}),
	$
	where $\otimes$ denotes the Kronecker product and $\mathrm{vec}$ is the vectorization operator.
	
	
	\begin{figure}[ht!]
		\centering
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image/cor_strong.pdf}
			\caption{Correlation Matrix - Strong Dependency}
			\label{fig:cor_strong}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image/cor_weak.pdf}
			\caption{Correlation Matrix - Weak Dependency}
			\label{fig:cor_weak}
		\end{subfigure}
		
		\vspace{0.5cm}  % Adjust vertical spacing
		
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image/graph_strong.pdf}
			\caption{Graph Structure - Strong Dependency}
			\label{fig:graph_strong}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image/graph_weak.pdf}
			\caption{Graph Structure - Weak Dependency}
			\label{fig:graph_weak}
		\end{subfigure}
		
		\caption{Comparison of Correlation Matrices and Graph Structures under Strong and Weak Dependencies.}
		\label{fig:correlation_graph_comparison}
	\end{figure}
	\begin{comment}
		The goal of this simulation is to:
		\begin{enumerate}
			\item Utilize a graphical model to introduce a complex dependence structure.
			\item Demonstrate that our method outperforms the penalized functional regression approach by \citet{goldsmith_penalized_2011}.
			\item Show that our method exhibits superior performance as correlation strength increases.
		\end{enumerate}
		
		Our previous setup, presented in Section \ref{simul:meeting_feb}, appeared to achieve these objectives. However, we identified several limitations:
		\begin{enumerate}
			\item The resulting precision matrix was singular, requiring us to scale up the diagonal elements to compute its inverse. This adjustment disrupted the original AR(1) structure of the functional component, making it significantly less smooth.
			\item Increasing conditional correlation did not consistently enhance the superiority of our method.
		\end{enumerate}
		To address these issues, I will explore alternative functional graphical model settings that exhibit a strong dependence structure—one substantial enough to be detected by statistical methodologies and validated by top statistical journal papers. I will evaluate the performance of penalized functional regression on these data-generating models.
		Among them, I will select three to four settings for comparison against other methods.
	\end{comment}
	Building on the functional graphical model simulation of \citet{zhuBayesianGraphicalModels2016}, we generate a mixed graphical model with five nodes, described as follows:  
	\begin{itemize}
		\item Nodes F1 and F2: Two functional predictors modeled as Gaussian processes using a truncated Karhunen-Loève expansion, where the eigenbasis consists of Fourier basis functions with a fixed number of basis functions, \(M = 9\).  
		\item  Nodes S1, S2, and S3: Three scalar predictors, each following an \(s\)-dimensional multivariate normal distribution. Unlike the functional predictors, these scalar predictors are modeled directly without basis expansion.  
	\end{itemize}
	To capture dependencies among predictors, we introduce a graph structure that governs their conditional correlations. We consider two types of graph structures: a weakly connected graph and a strongly connected graph. In the Gaussian process framework, the precision matrix \(\mathbf{R}_0^{-1}\) encodes conditional independence relationships, while its inverse, \(\mathbf{R}_0\), represents marginal covariances. This structure extends to a blockwise correlation matrix \(\mathbf{R} \in \mathbb{R}^{(2M + 3s) \times (2M + 3s)}\), where off-diagonal blocks represent correlations between FPC scores and scalar predictor values.  
	Each block \((i, j)\) of \(\mathbf{R}\) is given by \((R_0)_{ij} \mathbf{I}_{M_i, M_j}\), where \(\mathbf{I}_{M_i, M_j}\) is a rectangular identity matrix. Here, \(M_i = 9\) if node \(i\) corresponds to a functional predictor and \(M_j = s\) if node \(j\) corresponds to a scalar predictor. 
	
	For each functional predictor, we assgin  \(M\) reference eigenvalues (or FPC score) drawn independently from gamma distributions with decreasing means.
	These reference eigenvalues will later be multiplies by randomly  multiplier drawn from correlated multivaraite Normal distribution. These reference eigenvalues are fixed over all samples and all independent repetition of the experiment. We draw it one randomly just to ensure the differentiation between the two functional predictors.
	we independently draw for each functional predictor from . We then sample zero-mean multivariate normal data from the covariance matrix \(\mathbf{R}\). The last \(3s\) components are assigned as scalar predictors, while the first \(2M\) components, scaled by their corresponding eigenvalues, serve as FPC scores. These scores are then expanded into functional data, evaluated over 100 equally spaced points on \([0,1]\) using a Fourier basis.  
	
	Finally, we introduce structured variability by adding a common mean function, defined as a scaled and shifted sine function. To visualize the generated data, Figure \ref{fig:correlation_graph_comparison}  plots functional predictor realizations and heatmaps of sample correlation matrices for both graph structures, illustrating the distinct dependency patterns induced by the precision matrices.  
	
	
	
	\subsection{Simulation shown in february 2025 meeting}\label{simul:meeting_feb}
	To effectively manage the dependencies among functional predictors, scalar covariates, and between functional and scalar predictors, we utilize a Gaussian Markov random field (GMRF) within the framework of Gaussian undirected graphical models. In a GMRF, the off-diagonal elements of the precision matrix capture the conditional correlations between the corresponding components. Given that our setting involves both functional and scalar covariates, we adopt the simulation setup proposed by \cite{kolar_graph_2014}, which focuses on mixed attribute Gaussian graphical models.
	
	We construct a graph consisting of two functional predictor nodes and three vector predictor nodes. Below, we sequentially describe the graph structure and the generation process for each node.
	
	\medskip
	\noindent
	\textit{Functional Predictors.}
	We consider two functional predictors that share the same precision matrix. Denoted as \( \boldsymbol{\Theta} := (\theta_{t \tilde{t}}) \in \mathbb{R}^{p \times p} \), this precision matrix follows an AR(1) structure with a white noise variance of \( 0.1^2 \) and an autoregressive coefficient of \( = 0.95 \), ensuring a smooth functional trajectory.
	
	More formally, each functional predictor is evaluated at \( p \) equally spaced points on \( [0,1] \), with values defined recursively as:
	\begin{equation*}
		X^{(k)}_i(0) = 0, \quad 
		X^{(k)}_i(t)= \rho X^{(k)}_i(t-1) + \varepsilon_{itk}, \quad \varepsilon_{itk} \overset{\text{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2), \quad t = 1, \dots, p,~i=1, \ldots, n,~k = 1,2.
	\end{equation*}
	In this setting, the precision matrix \( \boldsymbol{\Theta} \) exhibits a tridiagonal Toeplitz structure, where the diagonal entries are given by:
	\begin{equation*}
		\theta_{tt} = 
		\begin{cases}
			\dfrac{1}{\sigma^2(1-0.95^2)}, & t = 1, p, \\
			\dfrac{1+0.95^2}{\sigma^2(1-0.95^2)}, & 2 \leq t \leq p-1.
		\end{cases}
	\end{equation*}
	The off-diagonal entries are:
	\begin{equation*}
		\theta_{t, t+1} = \theta_{t+1, t} = -\dfrac{\rho}{\sigma^2(1-\rho^2)}, \quad 1 \leq t \leq p-1.
	\end{equation*}
	Finally, the functional predictors are smoothed using a B-spline basis with 15 basis functions, as described in Section \ref{subsec: Step 1: Computing Smoothed PLS components}.
	
	\medskip
	\noindent
	\textit{Vector Predictors.}
	We consider \( s \) vector predictors, each following a \( d \)-dimensional zero-mean Gaussian distribution with a shared precision matrix. This precision matrix, denoted as \( \Gamma := (\gamma_{ij}) \in \mathbb{R}^{d \times d} \), follows a Toeplitz structure with exponentially decaying entries, given by:
	\begin{equation*}  
		\gamma_{ij} := 0.5^{|i-j|}.
	\end{equation*}
	
	\medskip
	\noindent
	\textit{Graph Structure.}
	We arrange five nodes   in a chain structure, where each node follows a sequential order, and the last node connects back to the first, as illustrated in Figure~\ref{fig:graph}. An edge in the graph indicates that the connected nodes remain correlated when the values of all other nodes are fixed. 
	The resulting marginal dependence structure is significantly more complex than the chain structure itself. We designate nodes \(F_1\) and \(F_2\) as functional predictors and nodes \(V_1\), \(V_2\), and \(V_3\) as vector predictors.
	%
	\begin{figure}
		\centering
		\includegraphics[width=0.4\linewidth]{image/graph}
		\caption{}
		\label{fig:graph}
	\end{figure}
	%
	To introduce conditional dependencies between the components, we set the off-diagonal blocks of the precision matrix to be $0.5 \mathbf{1}$ if the corresponding components are connected in the graph, and zero otherwise. Here, $\mathbf{1}$ denotes a matrix of appropriate dimensions where all elements are equal to 1. Overall, we generate a $2p + 3d$-dimensional multivariate Gaussian distribution with mean zero and a precision matrix $\Omega$, structured as follows:
	
	\[
	\Omega =
	\begin{pmatrix}
		\Omega_{F} & 0.5 \mathbf{1} & 0 & 0 & 0.5 \mathbf{1} \\
		0.5 \mathbf{1} & \Omega_{F} & 0.5 \mathbf{1} & 0 & 0 \\
		0 & 0.5 \mathbf{1} & \Omega_{V} & 0.5 \mathbf{1} & 0 \\
		0 & 0 & 0.5 \mathbf{1} & \Omega_{V} & 0.5 \mathbf{1} \\
		0.5 \mathbf{1} & 0 & 0 & 0.5 \mathbf{1} & \Omega_{V} \\
	\end{pmatrix}
	\]
	
	For each observation drawn from this multivariate Gaussian distribution, we process 
	
	The regression coefficients for the first functional predictor, $\beta_{F_1}$, are drawn from a multivariate normal distribution $N(0, 5 \mathbf{I}_p)$, with a fixed random seed. The coefficients for the second functional predictor, $\beta_{F_2}$, are drawn independently from the same distribution and are also smoothed using a B-spline basis with 10 basis functions.
	For the vector covariates, the regression coefficients are sampled from $N(0, I_d)$. After calculating the inner product of the covariates and their corresponding coefficients, independent Gaussian noise from $N(0, 0.1^2)$ is added to the generated responses to simulate measurement noise.
	
	The baseline methods are:
	\begin{itemize}
		\item Penalized functional regression (pfr)~\cite
		{goldsmith_penalized_2011}\item Principal component regression (fpcr): run both of PCA for multple functional predictors~\cite{happ_multivariate_2018} and scalar PCA and run OLS on the PC scores.
	\end{itemize}
	% TODO: \usepackage{graphicx} required
	
	We compare our method with these baseline methods using $p=100$. We consider scenarios with $d=1, 2, 3, 4, 5$ and $n = 100, 200, 300, 400$. For each scenario, we use 70\% of the data for training and evaluate prediction performance on the remaining 30\% test set, using the root prediction mean squared error as the evaluation metric. For our method and PCR, the maximum number of components are set as 20. The number of components is chosen by 5-fold cross validation. The results, summarized in Table~\ref{table
	}, demonstrate that our method consistently outperforms the baseline methods across all scenarios.
	\begin{figure}
		\centering
		\includegraphics[width=0.9\linewidth]{Screenshot 2025-02-25 125839.png}
		\caption{Enter Caption}
		\label{fig:enter-label}
	\end{figure}
	
	
	\section{Data Application}
	
	\medskip
	\noindent
	\textit{Renal study data.}~
	We applied our proposed hybrid functional PLS regression, along with other regression methods, to the Emory renal study data. The study collected data on 226 kidneys (left and right) from 113 subjects, including: (i) baseline renogram curves; (ii) post-furosemide renogram curves; (iii) ordinal ratings of kidney obstruction status (non-obstructed, equivocal, or obstructed) independently assessed by three nuclear medicine experts; (iv) eight kidney-level pharmacokinetic variables derived from radionuclide imaging; and (v) two subject-level variables (age and gender). The subjects had a mean age of 57.8 years (SD = 15.5; range = 18–83), with 54 males (48\%) and 59 females (52\%). The three experts unanimously classified 153 kidneys as non-obstructed, 5 as equivocal, and 40 as obstructed, while 28 kidneys had discrepant ratings.
	
	The two renogram curves, (i) and (ii), were treated as functional predictors and smoothed using a B-spline basis of order 15. The remaining variables, excluding the diagnosis, were treated as scalar predictors. Given the nature of these variables, we assume they are correlated with the renogram curves but not entirely redundant, as they may contain additional useful information. Finally, the diagnoses provided by the three experts were averaged and transformed using a min-max logit transformation. We splitted the data into 70\% of training data and 30\% of testing data, and evalauted the prediction perforamnec by root mean squared error on the test data, normalized by the range of the test data response.
	
	
	\subsection{}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	\bibliographystyle{apalike}
	\bibliography{Reference}
	
	
	
	\appendix
	
	\section{Overview of Appendix}
	
	\section{Notations}
	\begin{itemize}
		\item aaa
	\end{itemize}
	\section{Technical lemmas}
	
	
	
	\subsection{old notations}
	
	
	Then we can approximate the functional predictor observations as
	\begin{equation*}
		X_i^{(k)}(t_k) = \sum_{m=1}^M c_{im}^{(k)} b^{(k)}_{m}(t_k)=\mathbf{c}_i^{(k)T}\mathbf{b}^{(k)}(t_k), \;\; t_k \in [0,1], \;\; k=1, \ldots, K, \;\; i=1, \ldots, n,
	\end{equation*}
	This basis approximation can be expressed collectively across $n$ observations as: $\widetilde{\mathbf{X}^{(k)}}(t_k) = \widetilde{\mathbf{C}}^{(k)} \mathbf{b}^{(k)}(t_k)$, where $\widetilde{\mathbf{X}}^{(k)}(t_k)=(X_1^{(k)}(t_k), \ldots, X_n^{(k)}(t_k))^\top \in \mathbb{R}^n$, 
	This notation can be further extended to simultaneously express the basis expansions of $K$ functional predictors as 
	\begin{equation}
		\widetilde{X}(\mathbf{t}) = \widetilde{C}B(\mathbf{t}), \quad t \in \mathcal{T},
		\label{eq: basis approximation of X}
	\end{equation}
	where $\widetilde{X}(\mathbf{t}) = [\widetilde{\mathbf{X}}^{(1)}(t_1), \cdots, \widetilde{\mathbf{X}}^{(K)}(t_K)] \in \mathbb{R}^{n \times K}$, $\widetilde{C} = [\widetilde{C}^{(1)}, \cdots, \widetilde{\mathbf{C}}^{(k)}] \in \mathbb{R}^{n \times MK}$, and 
	$B(\mathbf{t}) = \mathrm{blkdiag}[\mathbf{b}^{(1)}(t_1), \cdots, \mathbf{b}^{(K)}(t_K)] \in \mathbb{R}^{MK \times K}$.
	
	
	
	
	Furthermore, let $
	\mathbf{Z} = [\mathbf{Z}_1, \cdots, \mathbf{Z}_n]^\top \in \mathbb{R}^{n \times p}
	$ denote $n \times p$ matrix of the scalar predictors (observed version when $l=1$, or residualized version when $l \ge 2$), and let $\widetilde{W}[\mathbf{t}]=[\widetilde{X}(\mathbf{t}), \mathbf{Z}]$ denote $n \times (K+p)$ hybrid predictor matrix that stacks $\widetilde{X}(\mathbf{t})$ and $\mathbf{Z}$ as columns. Then we can also write the hybrid predictor, whose functional part is approximated using the basis functions, as a form of linear transformation similar to \eqref{eq: basis approximation of X}:
	\begin{equation}\label{eq: basis approximation of W}
		\widetilde{W}[\mathbf{t}] 
		=  
		\widetilde{ \mathbf{C} }^\ast B^*[\mathbf{t}], \quad t \in \mathcal{T}.
	\end{equation}
	where  $B^*[\mathbf{t}] = \mathrm{blkdiag}(B(\mathbf{t}), \mathbf{I}_p) \in \mathbb{R}^{(MK+p) \times (K+p)}$ and $\widetilde{ \mathbf{C} }^\ast=[\widetilde{C}, \mathbf{Z}] \in \mathbb{R}^{n \times (MK+p)}$.
	
	
	
	
	\paragraph{Basis approximation of the PLS component.}
	We can also approximate the functional part of the PLS component in terms of the same basis functions: $\psi^{(k)}(t_k) = \sum_{m=1}^M a^{(k)}_{m} b^{(k)}_m(t_k) = \mathbf{a}^{(k)\top} \mathbf{b}^{(k)}(t_k)$ and $\psi(\mathbf{t}) = B(\mathbf{t})^\top \mathbf{a} \in \mathbb{R}^K$, where $\mathbf{a}^{(k)} = (a^{(k)}_{1}, \ldots, a^{(k)}_{M})^\top \in \mathbb{R}^M$, and $\mathbf{a}=(\mathbf{a}^{(1)\top}, \ldots, \mathbf{a}^{(K)\top})^\top \in \mathbb{R}^{MK}$, so that $\xi[\mathbf{t}] = (\psi(\mathbf{t})^\top, \boldsymbol{\theta}^\top)^\top = (\mathbf{a}^\top B(\mathbf{t}), \boldsymbol{\theta}^\top)^\top \in \mathbb{R}^{K+p}$. 
	%
	
	Using this definition, we construct the following block-diagonal matrices:
	\begin{equation}\label{def:J_and_J_star}
		\mathbf{J} = \mathrm{blkdiag}(\mathbf{J}^{(1)}, \cdots, \boldsymbol{\Phi}^{(k)}) \in \mathbb{R}^{MK \times MK}~\text{and}~\mathbf{J}^\ast=\mathrm{blkdiag}(\mathbf{J}, \mathbf{I}_p) \in \mathbb{R}^{(MK+p) \times (MK+p)}.
	\end{equation}
	
	\paragraph{Regularization based on second-order derivatives.}
	Our proposed strategy for computing PLS component, detailed in Section \ref{sec: Regularization of PLS Components}, incorporates regularization based on second-order derivatives. To this end, let $\ddot{J}^{(k)}$ denote the $M \times M$ Gram matrix formed by the second derivatives of the basis functions,  defined as
	\begin{equation}\label{def:dodot_J_k}
		\ddot{J}^{(k)} = \left[ \int_0^1 \ddot{b}^{(k)}_m(t) \ddot{b}^{(k)}_n(t) \, dt \right]_{m,n=1}^M.
	\end{equation}
	and define a block-diagonal matrix 
	$ \ddot{J}^\ast \in \mathbb{R}^{(MK+p) \times (MK+p)}$ as
	\begin{equation}\label{def:J_dotdot_ast}
		\ddot{J}^\ast=\mathrm{blkdiag}(\ddot{J}^{(1)}, \cdots, \ddot{J}^{(K)}, 0_{p \times p}).
	\end{equation}
	%
	The finite-basis approximation allows us to represent the hybrid predictor observations using the coefficient matrix $\widetilde{ \mathbf{C} }^\ast$, enabling all associated computations to be carried out via matrix operations involving $\widetilde{ \mathbf{C} }^\ast$, $J^\ast$, and $\ddot{J}^\ast$.
	
	\section{Proofs of Propositions}
	$\boldsymbol{\cdot}$ \underline{Abbreviations}
	\begin{itemize}
		\item[--] ``CSI": Cauchy–Schwarz inequality
	\end{itemize}
	
	\subsection{Proof of new Proposition}
	For any $\xi = (\xi_1, \ldots, \xi_K, \boldsymbol{\zeta})\in \mathcal{H}$, we have:
	\begin{align*}
		\widehat{\textnormal{Cov}} (
		\langle \widetilde{W}, \xi  \rangle_{\mathcal{H}}, Y 
		) 
		&=  %1
		\frac{1}{n} 
		\sum 
		\limits_{i=1}^n 
		y_i
		\langle \widetilde{W}_i, \xi \rangle_\mathcal{H} \\
		&= %2
		\frac{1}{n} 
		\sum \limits_{i=1}^n
		y_i
		\biggl(
		\sum_{j=1}^K
		\langle \widetilde{X}_{ij}, \xi_j \rangle  
		+
	 \mathbf{Z}_i^\top 
		\boldsymbol{\zeta} 
		\biggr)
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\\&= %3
		\frac{1}{n}
		\sum
		\limits_{i=1}^n 
		y_i
		\biggl( \sum \limits_{k=1}^K \int_0^1 \widetilde{X}_{ij}(t) \, \xi_j(t) \, dt \biggr)
		+
		 \frac{1}{n}
		 \sum
		 \limits_{i=1}^n
		 y_i
		 ( \mathbf{Z}_i^\top \boldsymbol{\zeta} )
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		  \\&=  
		  \frac{1}{n} \sum \limits_{i=1}^n 
		  y_i
		\biggl\{ 
		  \sum \limits_{k=1}^K \int_0^1
		   \biggl(
		   \sum_{m=1}^M \theta_{ijm} b_{m}(t) 
		   \biggr) 
		   \biggl(
		   \sum_{m'=1}^M d_{jm'} b_{m'}(t) 
		   \biggr) 
		   dt
		\biggr\}  
		  +
	\frac{1}{n}
	\mathbf{y}^\top
	Z \boldsymbol{\zeta} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\\&=  
\frac{1}{n} \sum \limits_{i=1}^n 
y_i
\biggl\{ 
\sum \limits_{k=1}^K 
\sum_{m=1}^M
\sum_{m'=1}^M  
\theta_{ijm}
d_{jm'}
\int_0^1
 b_{m}(t) 
 b_{m'}(t) 
dt
\biggr\}  
+
\frac{1}{n}
\mathbf{y}^\top
Z \boldsymbol{\zeta} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\\&=  
\sum \limits_{k=1}^K 
\biggl\{
\frac{1}{n} \sum \limits_{i=1}^n 
y_i
(
\boldsymbol{\theta}_{ij}^\top
B
\mathbf{d}_j
)  
\biggr\}
+
\frac{1}{n}
\mathbf{y}^\top
Z \boldsymbol{\zeta}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\\&=  
\frac{1}{n}
\biggl(
\sum \limits_{k=1}^K 
\mathbf{y}^\top
\Theta_{j}
B
\mathbf{d}_j
+
\mathbf{y}^\top
Z \boldsymbol{\zeta} 
\biggr).
	\end{align*}	
		\jongmin{
		Another way of computing \eqref{def:eigenproblem_smoothed}, rather than solving an eigenproblem, is as follows. As shown in the proof in Appendix \ref{section:proof:eigenproblem_derivation}, the empirical covariance is simplified as:
		\begin{align*}
			\widehat{\textnormal{Cov}} (\langle \mathbf{W}, \xi \rangle_{\mathcal{H}}, Y ) &= \frac{1}{n} \sum \limits_{i=1}^n \langle \mathbf{W}_i, \xi \rangle_\mathcal{H} Y_i \\
			&= n^{-1} \widetilde{\mathbf{y}}^\top (\widetilde{C}J\mathbf{a} + \mathbf{Z} \boldsymbol{\theta}).
			\\&=
			\frac{1}{n}\tilde{\mathbf{y}}^\top
			\begin{bmatrix} \widetilde{C}J \; \mathbf{Z} \end{bmatrix} \;      \underbrace{
				\begin{bmatrix} \mathbf{a} \\ \mathbf{\theta} \end{bmatrix}
			}_{:= \boldsymbol{\beta}}.
		\end{align*}
		Let us 
		denote $\mathbf{b} =  \widetilde{\mathbf{y}}^\top \begin{bmatrix} \widetilde{C}J \; \mathbf{Z} \end{bmatrix}$. This is the weighted row sum of $\begin{bmatrix} \widetilde{C}J \; \mathbf{Z} \end{bmatrix}$, weighted by $\widetilde{\mathbf{y}}$.
		Then using the symmetricity of $J^\ast$, we solve a linear system $J^\ast \mathbf{d} =
		\mathbf{b}^\top$ to obtain $\mathbf{d}$. Then we have
		\begin{align*}
			\max_{\sqrt{\boldsymbol{\beta} J^\ast  \boldsymbol{\beta}} = 1}
			\widehat{\textnormal{Cov}} (\langle \mathbf{W}, \xi \rangle_{\mathcal{H}}, Y ) 
			%%%%%%%%%%%%%%%%%%%%%
			&= 
			%%%%%%%%%%%%%%%%%%%%%%%%
			\max_{\sqrt{\boldsymbol{\beta} J^\ast  \boldsymbol{\beta}} = 1}
			\frac{1}{n} \mathbf{d}^\top J^\ast  \boldsymbol{\beta}
			\\& \overset{(i)}{\leq} 
			\max_{\sqrt{\boldsymbol{\beta} J^\ast  \boldsymbol{\beta}} = 1}
			\frac{1}{n} \sqrt{\mathbf{d}^\top J^\ast  \mathbf{d}}
			\sqrt{\boldsymbol{\beta} J^\ast  \boldsymbol{\beta}}
			\\& \overset{(ii)}{=}
			\frac{1}{n} \sqrt{\mathbf{d}^\top J^\ast  \mathbf{d}}    ,
		\end{align*}
		where step $(i)$ uses the Cauchy-Schwarz inequality with the inner product $\langle x, y \rangle_{J^\ast} = x^\top J^\ast y$,
		and step $(ii)$ uses the norm constraint.
		By the Cauchy-Schwarz inequality, the maximum displayed in the last term is obtained by a unit vector (with respect to $\langle \cdot, \cdot \rangle_{J^\ast}$) parallel to $\mathbf{d}$. Therefore we have
		\begin{equation*}
			\begin{bmatrix} \mathbf{a} \\ \mathbf{\theta} \end{bmatrix}
			= \frac{1}{\sqrt{\mathbf{d}^\top  J^\ast \mathbf{d} }}\mathbf{d},
		\end{equation*}
		which only involves solving one linear equation.
	}
	
\subsection{Proof of Lemma}
		We verify that the function
		\[
		\langle \mathbf{u}, \mathbf{v} \rangle_{\mathcal{H}} := \sum_{j=1}^K \mathbf{d}_j^\top B \mathbf{g}_j + \boldsymbol{\zeta}^\top \boldsymbol{\omega},
		\]
		defines a valid inner product on $\mathcal{H} := \mathbb{R}^{KM} \times \mathbb{R}^p$, where $\mathbf{u} = (\mathbf{d}_1, \ldots, \mathbf{d}_K, \boldsymbol{\zeta})$ and $\mathbf{v} = (\mathbf{g}_1, \ldots, \mathbf{g}_K, \boldsymbol{\omega})$.
		
		\textit{Linearity in the first argument}: For any $a, b \in \mathbb{R}$ and $\mathbf{u}_1, \mathbf{u}_2, \mathbf{v} \in \mathcal{H}$,
		\[
		\langle a \mathbf{u}_1 + b \mathbf{u}_2, \mathbf{v} \rangle_{\mathcal{H}} 
		= a \langle \mathbf{u}_1, \mathbf{v} \rangle_{\mathcal{H}} + b \langle \mathbf{u}_2, \mathbf{v} \rangle_{\mathcal{H}}.
		\]
		
		\textit{Symmetry}: Since $B$ is symmetric and the standard inner product is symmetric,
		\[
		\langle \mathbf{u}, \mathbf{v} \rangle_{\mathcal{H}} 
		= \langle \mathbf{v}, \mathbf{u} \rangle_{\mathcal{H}}.
		\]
		
		\textit{Positive definiteness}: If $\mathbf{u} \ne 0$, then either some $\mathbf{d}_j \ne 0$ or $\boldsymbol{\zeta} \ne 0$. Since $B$ is positive definite,
		\[
		\langle \mathbf{u}, \mathbf{u} \rangle_{\mathcal{H}} = \sum_{j=1}^K \mathbf{d}_j^\top B \mathbf{d}_j + \boldsymbol{\zeta}^\top \boldsymbol{\zeta} > 0.
		\]
		Thus, $\langle \cdot, \cdot \rangle_{\mathcal{H}}$ is a valid inner product.
		
		Now let
		\[
		\mathbf{g} := \left( \tfrac{1}{n} \Theta_1^\top \mathbf{y}, \ldots, \tfrac{1}{n} \Theta_K^\top \mathbf{y}, \tfrac{1}{n} Z^\top \mathbf{y} \right).
		\]
		Then, for any $\mathbf{u} = (\mathbf{d}_1, \ldots, \mathbf{d}_K, \boldsymbol{\zeta}) \in \mathcal{H}$,
		\[
		\left\langle \mathbf{u}, \mathbf{g} \right\rangle_{\mathcal{H}} 
		= \frac{1}{n} \sum_{j=1}^K \mathbf{d}_j^\top B \Theta_j^\top \mathbf{y} + \frac{1}{n} \boldsymbol{\zeta}^\top Z^\top \mathbf{y}
		= \frac{1}{n} \left( \sum_{j=1}^K \mathbf{y}^\top \Theta_j B \mathbf{d}_j + \mathbf{y}^\top Z \boldsymbol{\zeta} \right),
		\]
		as claimed.

	
	\subsection{Proof of Propositions \ref{Proposition: generalized rayleigh quotient} and \ref{Proposition: Regularized generalized rayleigh quotient} }\label{section:proof:eigenproblem_derivation}
	%
	%
	\begin{align*}
		\widehat{\textnormal{Cov}} (
		\langle \widetilde{\mathbf{W}}, \xi \rangle_{\mathcal{H}}, Y 
		) 
		&= 
		\frac{1}{n} \sum \limits_{i=1}^n 
		y_i
		\langle \widetilde{\mathbf{W}}_i, \xi \rangle_\mathcal{H} \\
		&= 
		\frac{1}{n} 
		\sum \limits_{i=1}^n
		y_i
		\langle \widetilde{X_i}, \psi \rangle_\mathcal{F}  
		+  
		\frac{1}{n} 
		\sum \limits_{i=1}^n  
		y_i
		\langle \mathbf{Z}_i, 
		\boldsymbol{\theta} 
		\rangle 
		\\
		&=  \frac{1}{n} \sum \limits_{i=1}^n  \left\{ \sum \limits_{k=1}^K \int_0^1 X_i(t_k) \psi^{(k)}(t_k) dt_k \right\} Y_i +  \frac{1}{n} \sum \limits_{i=1}^n \mathbf{Z}_i^\top \boldsymbol{\theta} Y_i \\
		&=  \frac{1}{n} \sum \limits_{i=1}^n \left[ \sum \limits_{k=1}^K \int_0^1 \left\{\sum_{m=1}^M c_{im}^{(k)} b^{(k)}_{m}(t) \right\} \left\{\sum_{m=1}^M a^{(k)}_{m} b^{(k)}_m(t) \right\} dt_k \right] Y_i + \frac{1}{n} \sum \limits_{i=1}^n \mathbf{Z}_i^\top \boldsymbol{\theta} Y_i \\
		&= n^{-1} \widetilde{\mathbf{y}}^\top \left[ \sum \limits_{k=1}^K \widetilde{\mathbf{C}}^{(k)} \left\{ \int_0^1 \mathbf{b}^{(k)}(t_k) \mathbf{b}^{(k)}(t_k)^\top dt_k \right\} \mathbf{a}^{(k)} \right] + n^{-1}\widetilde{\mathbf{y}}^\top \mathbf{Z} \boldsymbol{\theta} \\
		&= n^{-1} \widetilde{\mathbf{y}}^\top (\widetilde{C} J \mathbf{a}) + n^{-1}\widetilde{\mathbf{y}}^\top \mathbf{Z} \boldsymbol{\theta} \\
		&= n^{-1} \widetilde{\mathbf{y}}^\top (\widetilde{C}J\mathbf{a} + \mathbf{Z} \boldsymbol{\theta}).
	\end{align*}	
	
	we first construct the following matrix:
	\begin{equation}\label{def:V_star}
		\mathbf{V}^\ast 
		:= 
		n^{-2}\begin{bmatrix}
			\mathbf{J}
			\widetilde{ \mathbf{C} }^\top \mathbf{y}\mathbf{y}^\top \widetilde{ \mathbf{C} } \mathbf{J} 
			& 
			\mathbf{J} \widetilde{ \mathbf{C} }^\top 
			\mathbf{y}
			\mathbf{y}^\top 
			\mathbf{Z} 
			\\
			\mathbf{Z}^\top 
			\mathbf{y}
			\mathbf{y}^\top \widetilde{ \mathbf{C} }
			\mathbf{J} 
			&   
			\mathbf{Z}^\top 
			\mathbf{y}
			\mathbf{y}^\top 
			\mathbf{Z}
		\end{bmatrix} = n^{-2} \mathbf{J}^\ast \widetilde{ \mathbf{C} }^{*\top} \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{ \mathbf{C} }^{*} \mathbf{J}^\ast
		\in \mathbb{R}^{(MK+p) \times (MK+p)}.
	\end{equation}
	
	
	Here, $\widehat{\textnormal{Cov}}^2(\langle \mathbf{W}, \xi \rangle_{\mathcal{H}}, Y)$ is expressed as $\xi^{*\top} \mathbf{V}^\ast \xi^*$ based on the aforementioned basis approximations. Thus, finding $\xi^*$ that maximizes $\xi^{*\top} \mathbf{V}^\ast \xi^*$ amounts to finding $\widehat{\xi}$ that maximizes the sample squared covariance $\widehat{\textnormal{Cov}}^2(\langle \mathbf{W}, \xi \rangle_{\mathcal{H}}, Y)$, which is the first step of the PLS algorithm. The constraint $\xi^{*\top} J^\ast \xi^*=1$ scales the obtained PLS component to have a unit norm, that is, $\langle \widehat{\xi}, \widehat{\xi} \rangle_\mathcal{H}=1$. 
	
	and  $\xi^*=(\mathbf{a}^{\top}, \boldsymbol{\theta}^{\top})^\top \in \mathbb{R}^{MK + p}$. Then, the $l$-th PLS component, $\widehat{\xi} \equiv \widehat{\xi}_l$ can be obtained by solving
	\begin{equation}\label{def:eigenproblem_smoothed}
		\argmax \limits_{\xi^* \in \mathbb{R}^{MK+p}} \; \frac{\xi^{*\top} \mathbf{V}^\ast \xi^*}{\xi^{*\top} J^\ast \xi^*}, \quad \textit{or equivalently}, \quad \argmax \limits_{\xi^* \in \mathbb{R}^{MK+p}} \;  \xi^{*\top} \mathbf{V}^\ast \xi^* \quad \textnormal{subject to}  \quad \xi^{*\top} J^\ast \xi^*=1,
	\end{equation}
	with respect to $\widehat{\xi}^*=(\widehat{\mathbf{a}}^{\top}, \widehat{\boldsymbol{\theta}}^{\top})^\top$
	
	We show that the solutions to the generalized eigenvalue problems \eqref{def:eigenproblem_smoothed} and \eqref{eq: Regularized generalized rayleigh quotient equation} maximize the covariance with the response.
	Based on the basis function expansions  $X_i^{(k)}(t) = \sum_{m=1}^M c_{im}^{(k)} b^{(k)}_{m}(t)$ and $\psi^{(k)}(t) = \sum_{m=1}^M a^{(k)}_{m} b^{(k)}_m(t)$, $k=1, \ldots, K$, we can express the sample covariance between the PLS score and the outcome, which needs to be maximized to estimate the corresponding PLS component, as:
	\begin{align*}
		\widehat{\textnormal{Cov}} (\langle \mathbf{W}, \xi \rangle_{\mathcal{H}}, Y ) &= \frac{1}{n} \sum \limits_{i=1}^n \langle \mathbf{W}_i, \xi \rangle_\mathcal{H} Y_i \\
		&=  \frac{1}{n} \sum \limits_{i=1}^n  \langle X_i, \psi \rangle_\mathcal{F} Y_i +  \frac{1}{n} \sum \limits_{i=1}^n  \langle \mathbf{Z}_i, \boldsymbol{\theta} \rangle Y_i \\
		&=  \frac{1}{n} \sum \limits_{i=1}^n  \left\{ \sum \limits_{k=1}^K \int_0^1 X_i(t_k) \psi^{(k)}(t_k) dt_k \right\} Y_i +  \frac{1}{n} \sum \limits_{i=1}^n \mathbf{Z}_i^\top \boldsymbol{\theta} Y_i \\
		&=  \frac{1}{n} \sum \limits_{i=1}^n \left[ \sum \limits_{k=1}^K \int_0^1 \left\{\sum_{m=1}^M c_{im}^{(k)} b^{(k)}_{m}(t) \right\} \left\{\sum_{m=1}^M a^{(k)}_{m} b^{(k)}_m(t) \right\} dt_k \right] Y_i + \frac{1}{n} \sum \limits_{i=1}^n \mathbf{Z}_i^\top \boldsymbol{\theta} Y_i \\
		&= n^{-1} \widetilde{\mathbf{y}}^\top \left[ \sum \limits_{k=1}^K \widetilde{\mathbf{C}}^{(k)} \left\{ \int_0^1 \mathbf{b}^{(k)}(t_k) \mathbf{b}^{(k)}(t_k)^\top dt_k \right\} \mathbf{a}^{(k)} \right] + n^{-1}\widetilde{\mathbf{y}}^\top \mathbf{Z} \boldsymbol{\theta} \\
		&= n^{-1} \widetilde{\mathbf{y}}^\top (\widetilde{C} J \mathbf{a}) + n^{-1}\widetilde{\mathbf{y}}^\top \mathbf{Z} \boldsymbol{\theta} \\
		&= n^{-1} \widetilde{\mathbf{y}}^\top (\widetilde{C}J\mathbf{a} + \mathbf{Z} \boldsymbol{\theta}).
	\end{align*}
	This implies that
	\begin{align*}
		\widehat{\textnormal{Cov}}^2(\langle \mathbf{W}, \xi \rangle_{\mathcal{H}}, Y ) &= n^{-2} \widetilde{\mathbf{y}}^\top (\widetilde{C}J\mathbf{a} + \mathbf{Z} \boldsymbol{\theta}) (\widetilde{C}J\mathbf{a} + \mathbf{Z} \boldsymbol{\theta})^\top \widetilde{\mathbf{y}} \\
		&= n^{-2} \{\mathbf{a}^\top J \widetilde{C}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C} J \mathbf{a} +  \mathbf{a}^\top J \widetilde{C}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \mathbf{Z} \boldsymbol{\theta} +  \boldsymbol{\theta} \mathbf{Z}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C} J \mathbf{a} + \boldsymbol{\theta}^\top \mathbf{Z}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \mathbf{Z} \boldsymbol{\theta}\} \\
		&= \begin{bmatrix}
			\mathbf{a}^\top & \boldsymbol{\theta}^\top
		\end{bmatrix}
		\begin{bmatrix}
			n^{-2} J \widetilde{C}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C} J & n^{-2} J \widetilde{C}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \mathbf{Z}  \\
			n^{-2} \mathbf{Z}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C} J & n^{-2} \mathbf{Z}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \mathbf{Z}
		\end{bmatrix} 
		\begin{bmatrix}
			\mathbf{a} \\ \boldsymbol{\theta}
		\end{bmatrix} \\
		&\equiv \xi^{*T} \mathbf{V}^\ast \xi^*.
	\end{align*}
	Also, the unit norm constraint of the PLS components in $\mathcal{H}$ can be translated into:
	\begin{align*}
		\langle \xi, \xi \rangle_{\mathcal{H}} &= \sum \limits_{k=1}^K \int_{\mathcal{T}} \psi^{(k)}(t_k) \psi^{(k)}(t_k) dt_k + \boldsymbol{\theta}^{\top} \boldsymbol{\theta} \\
		&= \sum \limits_{k=1}^K  \int_\mathcal{T} \mathbf{a}^{(k)\top} \mathbf{b}^{(k)}(t_k) \mathbf{b}^{(k)}(t_k)^\top   \mathbf{a}^{(k)} dt_k + \boldsymbol{\theta}^{\top} \boldsymbol{\theta} \\
		&= \sum \limits_{k=1}^K  \mathbf{a}^{(k)\top} J^{(k)}   \mathbf{a}^{(k)} + \boldsymbol{\theta}^{\top} \boldsymbol{\theta} \\
		&= \mathbf{a}^\top J \mathbf{a} + \boldsymbol{\theta}^{\top} \boldsymbol{\theta} \\
		&= \xi^{*\top} J^\ast  \xi^{*} \\ 
		&= 1 \\
	\end{align*}
	
	
	
	Thus, the first step of the PLS algorithm translates into solving
	\begin{equation*}
		\max \limits_{\xi^* \in \mathbb{R}^{MK+p}} \;  \xi^{*\top} \mathbf{V}^\ast \xi^* \quad \textnormal{subject to}  \quad \xi^{*\top} J^\ast \xi^*=1,
	\end{equation*}
	or equivalently solving
	\begin{equation*}
		\max \limits_{\xi^* \in \mathbb{R}^{MK+p}} \; \frac{\xi^{*\top} \mathbf{V}^\ast \xi^*}{\xi^{*\top} J^\ast \xi^*},
	\end{equation*}
	where $J^\ast = \mathrm{blkdiag}[J, \mathbf{I}_p] \in \mathbb{R}^{MK+p}$. In other words, the PLS component is chosen to maximize the squared sample covariance  $\widehat{\textnormal{Cov}}^2(\langle \mathbf{W}, \xi \rangle_{\mathcal{H}}, Y ) = \xi^{*\top} \mathbf{V}^\ast \xi^*$ subject to the constraint $\xi^{*\top} J^\ast \xi^*=1$. 
	

	
	
	
	% Similar in spirit to the regularized principal component analysis (see equation [9.1] of \citealp{Ramsay2005}), one way to penalize the squared sample covariance $\widehat{\textnormal{Cov}}^2(\langle \mathbf{W}, \xi \rangle_{\mathcal{H}}, Y )$ is to divide it by $\{ 1+ \kappa \textnormal{PEN}(\psi) \}$, where $\textnormal{PEN}(\psi)$ is a roughness penalty term. This leads to the penalized squared sample covariance
	% \begin{equation*}
		%         \frac{\xi^{*T} \mathbf{V}^\ast \xi^*}{\xi^{*T} J^\ast \xi^* + \kappa \textnormal{PEN}(\psi)},
		% \end{equation*}
	% whose maximizer corresponds to a smoothed PLS component that maximizes $\widehat{\textnormal{Cov}}^2 \left(\langle \mathbf{W}, \xi \rangle_\mathcal{H}, Y \right)$ over the class of all functions satisfying sufficient smoothness conditions. 
	
	\section{Proof of Proposition \ref{Proposition: property of U}}\label{section:proof:Proposition: property of U}
	$\mathcal{U}$ is \textit{positive} because for every $\mathbf{h} \in \mathcal{H}$,
	\begin{equation*}
		\langle \mathcal{U} \mathbf{h}, \mathbf{h} \rangle_\mathcal{H} = \langle \langle \Sigma_{YW}, \mathbf{h} \rangle_{\mathcal{H}} \, \Sigma_{YW}, \mathbf{h} \rangle_\mathcal{H} = \langle \Sigma_{YW}, \mathbf{h} \rangle_\mathcal{H}^2 \ge 0.
	\end{equation*}
	
	$\mathcal{U}$ is \textit{self-adjoint} because for any $\mathbf{h}_1, \mathbf{h}_2 \in \mathcal{H}$,
	\begin{align*}
		\langle \mathcal{U} \mathbf{h}_1, \mathbf{h}_2 \rangle_\mathcal{H}  &= \langle \langle \mathbf{h}_1, \Sigma_{YW} \rangle_\mathcal{H} \Sigma_{YW}, \mathbf{h}_2 \rangle_\mathcal{H} \\
		&= \langle \mathbf{h}_1, \Sigma_{YW} \rangle_\mathcal{H} \langle \Sigma_{YW}, \mathbf{h}_2 \rangle_\mathcal{H} \\
		&= \langle \mathbf{h}_1,  \langle \Sigma_{YW}, \mathbf{h}_2 \rangle_\mathcal{H} \Sigma_{YW} \rangle_\mathcal{H}\\ 
		&= \langle \mathbf{h}_1, \mathcal{U} \mathbf{h}_2 \rangle_\mathcal{H}.
	\end{align*}
	
	Next, We will show that an image of a bounded family in $\mathcal{H}$ under $\mathcal{U}$ is uniformly bounded and equicontinuous, and then apply the Arzel\'{a}-Ascoli theorem to show $\mathcal{U}$ is \textit{compact}. Let $\mathcal{B}=\{\mathbf{h} \in \mathcal{H}: \Vert \mathbf{h} \Vert_\mathcal{H}^2 \le B\}$ denote a bounded family in $\mathcal{H}$ for some constant $0 < B < \infty$. Clearly, $\mathbf{h}=(f, \mathbf{v}) \in \mathcal{B}$ also implies $\Vert f \Vert_\mathcal{F}^2 \le B$ and $\Vert \mathbf{v} \Vert^2 \le B$. Define $\mathcal{I}  = \{\mathcal{U}\mathbf{h}: \mathbf{h} \in \mathcal{B}\}$ as the image of  $\mathcal{B}$ under $\mathcal{U}$. We will first show that $\mathcal{I}$ is a family of uniformly bounded functions with respect to arguments on $\mathcal{T}$. Let $\mu([0,1])$ denote a Lebesgue measure of $[0,1]$, and let $T = \max_{k=1, \ldots, K} \, \mu([0,1])$. Then for any $g \in \mathcal{I}$ and $\mathbf{t} \in \mathcal{T}$,
	\begin{align*}
		\Vert g[\mathbf{t}] \Vert^2 &= \Vert (\mathcal{U}\mathbf{h})[\mathbf{t}] \Vert^2 \\
		&= \Vert \langle \Sigma_{YW}, \mathbf{h} \rangle_\mathcal{H} \, \Sigma_{YW}[\mathbf{t}] \Vert^2 \\
		&= \left( \langle \sigma_{YX}, f \rangle_\mathcal{F} + \langle \sigma_{YZ}, \mathbf{v} \rangle \right)^2 \Vert [\sigma_{YX}^\top(\mathbf{t}), \sigma_{YZ}^\top]^\top \Vert^2 \\
		&\le \left( \langle \sigma_{YX}, \sigma_{YX} \rangle_\mathcal{F}^{1/2} \langle f, f \rangle_\mathcal{F}^{1/2} + \langle \sigma_{YZ}, \sigma_{YZ} \rangle^{1/2} \langle \mathbf{v}, \mathbf{v} \rangle^{1/2} \right)^2 \Vert [\sigma_{YX}^\top(\mathbf{t}), \sigma_{YZ}^\top]^\top \Vert^2 \quad (\because \textnormal{CSI})\\
		&= \left[ \left\{ \sum \limits_{k=1}^K \int_0^1 \sigma_{YX}^{(k)2}(t_k) dt_k \right\}^{\frac{1}{2}} \Vert f \Vert_\mathcal{F} + \left(\sum \limits_{r=1}^p \sigma_{YZ, r}^2 \right)^{\frac{1}{2}} \Vert \mathbf{v} \Vert \right]^2 \left(\sum \limits_{k=1}^K \sigma_{YX}^{(k)2}(t_k) + \sum \limits_{r=1}^p \sigma^2_{YZ,r} \right) \\
		& \le \left\{ \left(\sum \limits_{k=1}^K \int_0^1 Q_1 dt_k \right)^{\frac{1}{2}} B^{\frac{1}{2}} + \left(\sum \limits_{r=1}^p Q_2 \right)^{\frac{1}{2}} B^{\frac{1}{2}} \right\}^2 \left( \sum \limits_{k=1}^K Q_1 + \sum \limits_{r=1}^p Q_2 \right) \\
		&= B \left\{(KTQ_1)^{\frac{1}{2}} + (PQ_2)^\frac{1}{2} \right\}^2 (KQ_1 +pQ_2) < \infty.
	\end{align*}
	We now show that $\mathcal{I}$ is equicontinuous. For $\epsilon > 0$, define 
	\begin{equation*}
		\tilde{\epsilon} = \frac{\epsilon}{(KB)^{1/2} \{(KTQ_1)^{1/2} + (PQ_2)^{1/2}\}}
	\end{equation*}
	By the continuity assumption of $\sigma_{YX}^{(k)}$, there exists $\delta_k > 0$ such that
	\begin{equation*}
		|t_k - t_k^*| < \delta_k \implies |\sigma^{(k)}(t_k) - \sigma^{(k)}(t_k^*)|< \tilde{\epsilon},
	\end{equation*}
	for all $k=1, \ldots, K$.
	Set $\delta = \min_{k=1, \ldots, K} \delta_k$, and let $\Vert \mathbf{t} - \mathbf{t}^* \Vert < \delta$, with $\mathbf{t}=(t_1, \ldots, t_K)$ and $\mathbf{t}^*=(t_1^*, \ldots, t_K^*)$. Clearly, $|t_k - t^*_k| < \delta$ for all $k=1, \ldots, K$, so for $g \in \mathcal{I}$, we can establish that
	\begin{align*}
		\Vert g[\mathbf{t}] - g[\mathbf{t}^*] \vert^2 &= \Vert (\mathbf{U}\mathbf{h})[\mathbf{t}] - (\mathbf{U}\mathbf{h})[\mathbf{t}^*] \Vert^2 \\
		&= \Vert \langle \Sigma_{YW}, \mathbf{h} \rangle_\mathcal{H} \Sigma_{YW}[\mathbf{t}] - \langle \Sigma_{YW}, \mathbf{h} \rangle_\mathcal{H} \Sigma_{YW}[\mathbf{t}^*] \Vert^2 \\
		&= \langle \Sigma_{YW}, \mathbf{h} \rangle_\mathcal{H}^2 \Vert \Sigma_{YW}[\mathbf{t}]- \Sigma_{YW}[\mathbf{t}^*] \Vert^2 \\
		&= \left( \langle \sigma_{YX}, f \rangle_\mathcal{F} + \langle \sigma_{YZ}, \mathbf{v} \rangle \right)^2 \left\Vert 
		\begin{bmatrix}
			\sigma_{YX}(\mathbf{t}) \\
			\sigma_{YZ}
		\end{bmatrix} -
		\begin{bmatrix}
			\sigma_{YX}(\mathbf{t}^*) \\
			\sigma_{YZ}
		\end{bmatrix}
		\right\Vert^2  \\
		&\stackrel{CSI}{\le} \left( \langle \sigma_{YX}, \sigma_{YX} \rangle_\mathcal{F}^{1/2} \langle f, f \rangle_\mathcal{F}^{1/2} + \langle \sigma_{YZ}, \sigma_{YZ} \rangle^{1/2} \langle \mathbf{v}, \mathbf{v} \rangle^{1/2} \right)^2 \left\Vert 
		\begin{bmatrix}
			\sigma_{YX}(\mathbf{t})-\sigma_{YX}(\mathbf{t}^*) \\
			\mathbf{0}
		\end{bmatrix}
		\right\Vert^2 \\
		&= \left[ \left\{ \sum \limits_{k=1}^K \int_0^1 \sigma_{YX}^{(k)2}(t_k) dt_k \right\}^{\frac{1}{2}} \Vert f \Vert_\mathcal{F} + \left(\sum \limits_{r=1}^p \sigma_{YZ, r}^2 \right)^{\frac{1}{2}} \Vert \mathbf{v} \Vert \right]^2 \sum \limits_{k=1}^K  \left\{\sigma_{YX}^{(k)}(t_k) -\sigma_{YX}^{(k)}(t_k^*) \right\}^2 \\
		&\le B \left\{(KTQ_1)^{\frac{1}{2}} + (PQ_2)^\frac{1}{2} \right\} K\tilde{\epsilon}^2 = \epsilon^2.
	\end{align*}
	Now we can apply the Arzel\'{a}-Ascoli theorem to conflude that for any bounded sequence $\{\mathbf{h}_n\}_{n \in \mathbb{N}} \in \mathcal{B}$, the sequence $\{\mathbf{g}_n = \mathcal{U}\mathbf{h}_n\}_{n \in \mathbb{N}} \in \mathcal{I}$ contains a convergent subsequence. Therefore, $\mathcal{U}$ is a compact operator.
	
	
	
	\section{Proof of Proposition \ref{proposition: eigenanalysis}}\label{section:proof:proposition: eigenanalysis}
	Before we prove Proposition \ref{proposition: eigenanalysis}, we state and prove the following lemma:
	\begin{lemma}
		$\mathcal{C}_{WY}$ is an adjoint operator of $\mathcal{C}_{YW}$. That is, $\mathcal{C}_{WY} = \mathcal{C}_{YW}^*$.
	\end{lemma}
	Proof. 
	
	\noindent First, we have:
	\begin{equation*}
		\langle \mathcal{C}_{YW} \mathbf{h}, d \rangle = E\{ \langle \mathbf{W}, \mathbf{h} \rangle_\mathcal{H} Y \} d
	\end{equation*}
	Also, we have:
	\begin{equation*}
		\langle \mathbf{h}, \mathcal{C}_{WY} d \rangle_{\mathcal{H}} = \langle \mathbf{h}, E(Y\mathbf{W})d \rangle_{\mathcal{H}} = E \langle \mathbf{h}, Y \mathbf{W} d \rangle_{\mathcal{H}}  = E\{ \langle \mathbf{W}, \mathbf{h} \rangle_\mathcal{H} Y \} d.
	\end{equation*}
	Thus, $\langle \mathcal{C}_{YW} \mathbf{h}, d \rangle =  \langle \mathbf{h}, \mathcal{C}_{WY} d \rangle_{\mathcal{H}}$, and this implies $\mathcal{C}_{WY} = \mathcal{C}_{YW}^*$.
	
	\noindent Now we prove Proposition \ref{proposition: eigenanalysis}. The singular value decomposition of $\mathcal{C}_{YW}$ is given by
	\begin{equation*}
		\mathcal{C}_{YW} = \sum \limits_{j=1}^\infty \iota_j (f_{1j} \otimes f_{2j}).
	\end{equation*}
	Let $\Vert \cdot \Vert_{op}$ denote an operator norm. Then, applying Theorem 4.3.4 in \citet{Hsing2015}, we can show that
	\begin{equation*}
		\Vert \mathcal{C}_{YW} \Vert_{op} = \sup_{\substack{\mathbf{h} \in \mathcal{H} \\ \Vert \mathbf{h} \Vert_\mathcal{H}=1}} | \mathcal{C}_{YW} \mathbf{h} |^2 
		= \sup_{\substack{\mathbf{h} \in \mathcal{H} \\ \Vert \mathbf{h} \Vert_\mathcal{H}=1}} | E(\langle \mathbf{W}, \mathbf{h} \rangle_\mathcal{H} Y)|^2 
		= \sup_{\substack{\mathbf{h} \in \mathcal{H} \\ \Vert \mathbf{h} \Vert_\mathcal{H}=1}} \textnormal{Cov}^2(\langle \mathbf{W}, \mathbf{h} \rangle_\mathcal{H}, Y) 
		= \kappa_1^2,
	\end{equation*}
	with maximum attained at $\mathbf{h}=f_{11}$, which is an eigenfunction of  $\mathcal{C}_{YW}^* \, \circ \, \mathcal{C}_{YW} = \mathcal{C}_{WY} \, \circ \, \mathcal{C}_{YW} = \mathcal{U}$ corresponding to the largest eigenvalue $\kappa_1^2.$
	
	
	
	
	\section{Proof of Proposition \ref{proposition: modified orthnormality of PLS components}}
	\label{section:proof:proposition: modified orthnormality of PLS components}
	
	Firstly, it is obvious by the unit-norm constraint enforced in Proposition \ref{proposition: eigenanalysis} that:
	\begin{align*}
		\langle \langle \hat{\xi}_l,  \hat{\xi}_l \rangle \rangle &= \langle \widehat{\xi}_l, \widehat{\xi}_j \rangle_{\mathcal{H}} + \sum_{k=1}^K \lambda_k \left\langle \ddot{\widehat{\psi}}_l^{(k)}, \ddot{\widehat{\psi}}_j^{(k)} \right\rangle_{L^2}  \\
		&= \sum \limits_{k=1}^K \int_0^1 \widehat{\psi}_l^{(k)}(t_k) \widehat{\psi}_j^{(k)}(t_k) dt_k + \sum \limits_{r=1}^p \widehat{\theta}_{lr} \widehat{\theta}_{jr} + \sum_{k=1}^K \lambda_k \int_0^1 \ddot{\widehat{\psi}}_l^{(k)}(t_k) \ddot{\widehat{\psi}}_j^{(k)}(t_k) dt_k \\
		&= \sum \limits_{k=1}^K \int_0^1 \hat{\mathbf{a}}^{(k) \top}_l \mathbf{b}^{(k)}(t_k) \mathbf{b}^{(k) \top}(t_k) \hat{\mathbf{a}}^{(k)}_l  dt_k + \boldsymbol{\theta}_l^\top \boldsymbol{\theta}_l + \sum \limits_{k=1}^K \lambda_k \int_0^1 \hat{\mathbf{a}}^{(k) \top}_l \ddot{\mathbf{b}}^{(k)}(t_k) \ddot{\mathbf{b}}^{(k) \top}(t_k) \hat{\mathbf{a}}^{(k)}_l  dt_k \\
		& = \sum \limits_{k=1}^K \hat{\mathbf{a}}_l^{(k) \top} J^{(k)} \hat{\mathbf{a}}_l^{(k)} + \boldsymbol{\theta}_l^\top \boldsymbol{\theta}_l +  \sum \limits_{k=1}^K \lambda_k \hat{\mathbf{a}}^{(k) \top}_l \ddot{J}^{(k)} \hat{\mathbf{a}}^{(k)}_l \\
		& = \hat{\xi}_l^{*\top} J^\ast \hat{\xi}_l^* + \hat{\xi}_l^{*\top} \Lambda \ddot{J}^\ast \hat{\xi}_l^* = \hat{\xi}_l^{*\top} (J^\ast +\Lambda \ddot{J}^\ast) \hat{\xi}_l^* = 1
	\end{align*}
	where $\ddot{\mathbf{b}}^{(k)}(t_k) = (\ddot{b}_1^{(k)}(t_k), \ldots, \ddot{b}_M^{(k)}(t_k))^\top$
	
	Now to prove the orthogonality, we first express the $l$-th residualized hybrid predictor in terms of its previous $(l-1)$-th version:
	\begin{equation*}
		\widetilde{W}^{[l]}[\mathbf{t}] = \widetilde{W}^{[l]}[\mathbf{t}] - \widehat{\boldsymbol{\rho}}_{l-1} \widehat{\boldsymbol{\delta}}_l[\mathbf{t}]^\top 
	\end{equation*}
	which, after applying the finite-dimensional form (9), implies that
	\begin{align*}
		\widetilde{C}^{*[l]}B^*[\mathbf{t}] &=\widetilde{C}^{*[l-1]}B^*[\mathbf{t}] - \widehat{\boldsymbol{\rho}}_{l-1}\widehat{\mathbf{d}}^*_{l-1}B^*[\mathbf{t}] \\
		&= \widetilde{C}^{*[l-1]}B^*[\mathbf{t}] - \widehat{\boldsymbol{\rho}}_{l-1} \widehat{\boldsymbol{\rho}}_{l-1}^\top \widetilde{C}^{*[l-1]} J^\ast \left(\widehat{\boldsymbol{\rho}}_{i,l-1}^\top \widehat{\boldsymbol{\rho}}_{l-1} J^\ast \right)^{-1} B^*[\mathbf{t}] \\
		& =\widetilde{C}^{*[l-1]}B^*[\mathbf{t}] -  \left(\frac{\widehat{\boldsymbol{\rho}}_{l-1} \widehat{\boldsymbol{\rho}}_{l-1}^\top}{\widehat{\boldsymbol{\rho}}_{l-1}^\top \widehat{\boldsymbol{\rho}}_{l-1}} \right) \widetilde{C}^{*[l-1]} B^*[\mathbf{t}] \\
		&= \left(I_n - \frac{\widehat{\boldsymbol{\rho}}_{l-1} \widehat{\boldsymbol{\rho}}_{l-1}^\top}{\widehat{\boldsymbol{\rho}}_{l-1}^\top \widehat{\boldsymbol{\rho}}_{l-1}} \right) \widetilde{C}^{*[l-1]} B^*[\mathbf{t}].
	\end{align*}
	Thus, we have
	\begin{align*}
		\widetilde{W}^{[l]}[\mathbf{t}] &= \left(I_n - \frac{\widehat{\boldsymbol{\rho}}_{l-1} \widehat{\boldsymbol{\rho}}_{l-1}^\top}{\widehat{\boldsymbol{\rho}}_{l-1}^\top \widehat{\boldsymbol{\rho}}_{l-1}} \right) \widetilde{W}^{[l-1]}[\mathbf{t}] \\
		&= \left(I_n - \frac{\widehat{\boldsymbol{\rho}}_{l-1} \widehat{\boldsymbol{\rho}}_{l-1}^\top}{\widehat{\boldsymbol{\rho}}_{l-1}^\top \widehat{\boldsymbol{\rho}}_{l-1}} \right) \left(I_n - \frac{\widehat{\boldsymbol{\rho}}_{l-2} \widehat{\boldsymbol{\rho}}_{l-2}^\top}{\widehat{\boldsymbol{\rho}}_{l-2}^\top \widehat{\boldsymbol{\rho}}_{l-2}} \right) \widetilde{W}^{[l-2]}[\mathbf{t}] \\
		&= P \left(I_n - \frac{\widehat{\boldsymbol{\rho}}_{j} \widehat{\boldsymbol{\rho}}_{j}^\top}{\widehat{\boldsymbol{\rho}}_{j}^\top \widehat{\boldsymbol{\rho}}_{j}} \right)\widetilde{W}^{[j]}[\mathbf{t}]
	\end{align*}
	where $P$ is an $n \times n$ matrix defined as
	\begin{equation*}
		P = \prod \limits_{h=j+1}^{l-1} \left(I_n - \frac{\widehat{\boldsymbol{\rho}}_{h} \widehat{\boldsymbol{\rho}}_{h}^\top}{\widehat{\boldsymbol{\rho}}_{h}^\top \widehat{\boldsymbol{\rho}}_{h}} \right)
	\end{equation*}
	Then for $j < l$,
	\begin{align*}
		\int_{\mathcal{T}} \widetilde{W}^{[l]}[\mathbf{t}] \widehat{\xi}_j[\mathbf{t}] d\mathbf{t} &= \int_{\mathcal{T}} P   \left(I_n - \frac{\widehat{\boldsymbol{\rho}}_{j} \widehat{\boldsymbol{\rho}}_{j}^\top}{\widehat{\boldsymbol{\rho}}_{j}^\top \widehat{\boldsymbol{\rho}}_{j}} \right)\widetilde{W}^{[j]}[\mathbf{t}]  \widehat{\xi}_k[\mathbf{t}] d\mathbf{t}  \\
		&= P \left\{\int_{\mathcal{T}} \widetilde{W}^{[j]}[\mathbf{t}] \widehat{\xi}_j[\mathbf{t}] d\mathbf{t} - \frac{\widehat{\boldsymbol{\rho}}_{j} \widehat{\boldsymbol{\rho}}_{j}^\top}{\widehat{\boldsymbol{\rho}}_{j}^\top \widehat{\boldsymbol{\rho}}_{j}} \int_{\mathcal{T}} \widetilde{W}^{[j]}[\mathbf{t}] \widehat{\xi}_j[\mathbf{t}] d\mathbf{t} \right\} \\ 
		&=P \left( \widehat{\boldsymbol{\rho}}_j - \frac{\widehat{\boldsymbol{\rho}}_{j} \widehat{\boldsymbol{\rho}}_{j}^\top}{\widehat{\boldsymbol{\rho}}_{j}^\top \widehat{\boldsymbol{\rho}}_{j}} \widehat{\boldsymbol{\rho}}_j \right) \\
		&= P(\widehat{\boldsymbol{\rho}}_j-\widehat{\boldsymbol{\rho}}_j) \\
		&= \mathbf{0} \in \mathbb{R}^n, 
	\end{align*}
	noting that $\int_{\mathcal{T}} \widetilde{W}^{[j]}[\mathbf{t}] \widehat{\xi}_j[\mathbf{t}] d\mathbf{t} = \widehat{\boldsymbol{\rho}}_j$. Now by the generalized eigenvalue problem (i.e., generalized Rayleigh quotient problem) presented in Proposition 2, we have that for $j < l$,
	\begin{align*}
		\langle \langle \widehat{\xi}_l, \widehat{\xi}_j \rangle \rangle &= \widehat{\xi}_l^{*\top} (J^\ast + \Lambda \ddot{J}^\ast)\widehat{\xi}_j^{*} \\
		& = \frac{1}{\kappa_l} (V^\ast_l \widehat{\xi}_l^{*})^\top \widehat{\xi}_j^{*} \\
		&=  \frac{1}{\kappa_l}\widehat{\xi}_l^{* \top} V^\ast_l \widehat{\xi}_j^{*} \\
		&= \frac{1}{\kappa_l}\widehat{\xi}_l^{* \top} \left(n^{-2} J^\ast \widetilde{C}^{*[l]\top} \widetilde{\mathbf{y}}^{[l]}\widetilde{\mathbf{y}}^{[l]\top} \widetilde{C}^{*[l]} J^\ast \right) \widehat{\xi}_j^{*} \\
		&= \frac{1}{n^2\kappa_l}\widehat{\xi}_l^{* \top} \ J^\ast \widetilde{C}^{*[l]\top} \widetilde{\mathbf{y}}^{[l]}\widetilde{\mathbf{y}}^{[l]\top}  \widetilde{C}^{*[l]}\left(\int_{\mathcal{T}} B^*[\mathbf{t}] B^*[\mathbf{t}]^\top d\mathbf{t}  \right) \widehat{\xi}_j^{*} \\
		&= \frac{1}{n^2\kappa_l}\widehat{\xi}_l^{* \top}  J^\ast \widetilde{C}^{*[l]\top} \widetilde{\mathbf{y}}^{[l]}\widetilde{\mathbf{y}}^{[l]\top}  \left(\int_{\mathcal{T}} \widetilde{C}^{*[l]} B^*[\mathbf{t}] B^*[\mathbf{t}]^\top \widehat{\xi}_j^{*} d\mathbf{t}  \right)   \\
		&= \frac{1}{n^2\kappa_l}\widehat{\xi}_l^{* \top}  J^\ast \widetilde{C}^{*[l]\top} \widetilde{\mathbf{y}}^{[l]}\widetilde{\mathbf{y}}^{[l]\top}  \left(\int_{\mathcal{T}} \widetilde{W}^{[l]}[\mathbf{t}] \widehat{\xi}_j[\mathbf{t}] d\mathbf{t}  \right)   \\
		&=0, 
	\end{align*}
	where the last equality follows from the above result that $\int_{\mathcal{T}} \widetilde{W}^{[l]}[\mathbf{t}] \widehat{\xi}_j[\mathbf{t}] d\mathbf{t} = \mathbf{0}$.
	
	
	\section{Proof of Proposition \ref{proposition: orthnormality of PLS scores}}\label{section:proof:proposition: orthnormality of PLS scores}
	
	
	
	First note that $\widehat{\boldsymbol{\rho}}_l = \int_{\mathcal{T}} \widetilde{W}^{[l]}[\mathbf{t}] \widehat{\xi}_l[\mathbf{t}] d\mathbf{t}= \widetilde{C}^{*[l]} J^\ast \widehat{\xi}^*_{l}$ for $l=1, \ldots, L$. Then, as above, we first express the $l$-th residualized hybrid predictor in terms of its previous $(l-1)$-th version:
	\begin{equation*}
		\widetilde{W}^{[l]}[\mathbf{t}] = \widetilde{W}^{[l]}[\mathbf{t}] - \widehat{\boldsymbol{\rho}}_{l-1} \widehat{\boldsymbol{\delta}}_l[\mathbf{t}]^\top 
	\end{equation*}
	which, after applying the finite-dimensional form (9), implies that
	\begin{equation*}
		\setlength{\jot}{10pt}
		\begin{aligned}
			\widetilde{C}^{*[l]}B^*[\mathbf{t}] &=\widetilde{C}^{*[l-1]}B^*[\mathbf{t}] - \widehat{\boldsymbol{\rho}}_{l-1}\widehat{\mathbf{d}}^*_{l-1}B^*[\mathbf{t}] \\
			&= \widetilde{C}^{*[l-1]}B^*[\mathbf{t}] - \widehat{\boldsymbol{\rho}}_{l-1} \widehat{\boldsymbol{\rho}}_{l-1}^\top \widetilde{C}^{*[l-1]} J^\ast \left(\widehat{\boldsymbol{\rho}}_{i,l-1}^\top \widehat{\boldsymbol{\rho}}_{l-1} J^\ast \right)^{-1} B^*[\mathbf{t}] \\
			& =\widetilde{C}^{*[l-1]}B^*[\mathbf{t}] -  \frac{\widehat{\boldsymbol{\rho}}_{l-1} \widehat{\boldsymbol{\rho}}_{l-1}^\top \widetilde{C}^{*[l-1]} B^*[\mathbf{t}]}{\widehat{\boldsymbol{\rho}}_{l-1}^\top \widehat{\boldsymbol{\rho}}_{l-1}}  \\
			& =\widetilde{C}^{*[l-1]}B^*[\mathbf{t}] -  \frac{\widetilde{C}^{*[l-1]} J^\ast \widehat{\xi}^{*}_{l-1} \widehat{\boldsymbol{\rho}}_{l-1}^\top \widetilde{C}^{*[l-1]} B^*[\mathbf{t}]}{\widehat{\boldsymbol{\rho}}_{l-1}^\top \widehat{\boldsymbol{\rho}}_{l-1}},
		\end{aligned}
	\end{equation*}
	Since the equation should hold for all argument values $\mathbf{t} \in \mathcal{T}$, we can write:
	\begin{equation*}
		\setlength{\jot}{10pt}
		\begin{aligned}
			\widetilde{C}^{*[l]} &= \widetilde{C}^{*[l-1]} -  \frac{\widetilde{C}^{*[l-1]} J^\ast \widehat{\xi}^{*}_{l-1} \widehat{\boldsymbol{\rho}}_{l-1}^\top \widetilde{C}^{*[l-1]}}{\widehat{\boldsymbol{\rho}}_{l-1}^\top \widehat{\boldsymbol{\rho}}_{l-1}} \\
			&= \widetilde{C}^{*[l-1]} \left(I_{MK+p} - \frac{J^\ast \widehat{\xi}^{*}_{l-1} \widehat{\boldsymbol{\rho}}_{l-1}^\top \widetilde{C}^{*[l-1]}}{\widehat{\boldsymbol{\rho}}_{l-1}^\top \widehat{\boldsymbol{\rho}}_{l-1}} \right) \\
			&= \widetilde{C}^{*[j]} \left(I_{MK+p} - \frac{J^\ast \widehat{\xi}^{*}_{j} \widehat{\boldsymbol{\rho}}_{j}^\top \widetilde{C}^{*[j]}}{\widehat{\boldsymbol{\rho}}_{j}^\top \widehat{\boldsymbol{\rho}}_{j}} \right) \widetilde{P},
		\end{aligned}
	\end{equation*}
	where $\widetilde{P}$ is a $(MK+p) \times (MK+p)$ matrix defined as
	\begin{equation*}
		\widetilde{P} = \prod \limits_{h=j+1}^{l-1} \left(I_{MK+p} - \frac{J^\ast \hat{\xi}^*_h \widehat{\boldsymbol{\rho}}_{h}^\top \widetilde{C}^{*[h]}}{\widehat{\boldsymbol{\rho}}_{h}^\top \widehat{\boldsymbol{\rho}}_{h}} \right)
	\end{equation*}
	Then, for $j < l$,
	\begin{equation*}
		\setlength{\jot}{10pt}
		\begin{aligned}
			\widehat{\boldsymbol{\rho}}_j^\top \widetilde{C}^{*[l]} &=  \widehat{\boldsymbol{\rho}}_j^\top \widetilde{C}^{*[j]} \left(I_{MK+p} - \frac{J^\ast \widehat{\xi}^{*}_{j} \widehat{\boldsymbol{\rho}}_{j}^\top \widetilde{C}^{*[j]}}{\widehat{\boldsymbol{\rho}}_{j}^\top \widehat{\boldsymbol{\rho}}_{j}} \right) \widetilde{P} \\
			&=\left( \widehat{\boldsymbol{\rho}}_j^\top \widetilde{C}^{*[j]} - \frac{ \widehat{\boldsymbol{\rho}}_j^\top \widetilde{C}^{*[j]} J^\ast \widehat{\xi}^{*}_{j} \widehat{\boldsymbol{\rho}}_{j}^\top \widetilde{C}^{*[j]}}{\widehat{\boldsymbol{\rho}}_{j}^\top \widehat{\boldsymbol{\rho}}_{j}} \right) \widetilde{P} \\
			&=\left( \widehat{\boldsymbol{\rho}}_j^\top \widetilde{C}^{*[j]} - \frac{ \widehat{\boldsymbol{\rho}}_j^\top \widehat{\boldsymbol{\rho}}_j  \widehat{\boldsymbol{\rho}}_{j}^\top \widetilde{C}^{*[j]}}{\widehat{\boldsymbol{\rho}}_{j}^\top \widehat{\boldsymbol{\rho}}_{j}} \right) \widetilde{P} \\
			&= \left(  \widehat{\boldsymbol{\rho}}_j^\top \widetilde{C}^{*[j]} -  \widehat{\boldsymbol{\rho}}_j^\top \widetilde{C}^{*[j]} \right) \widetilde{P} \\
			&= \mathbf{0} \in \mathbb{R}^{1 \times (MK+p)},
		\end{aligned}
	\end{equation*}
	Therefore, for $j < l$,
	\begin{equation*}
		\widehat{\boldsymbol{\rho}}_j^\top \widehat{\boldsymbol{\rho}}_l =  \widehat{\boldsymbol{\rho}}_j^\top \widetilde{C}^{*[l]} J^\ast \widehat{\boldsymbol{\rho}}_{l} = \mathbf{0} J^\ast \widehat{\boldsymbol{\rho}}_{l}=  0.
	\end{equation*}
	Thus, the proposition is proved.
	
	
	\section{Simulations} \label{Simulations}
	
	
	
\end{document}