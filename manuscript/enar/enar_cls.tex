\documentclass[referee,12pt,usenatbib]{biom}


\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
%\RequirePackage{,,}
\bibliographystyle{biom} % Biometrics bib style if installed
%\usepackage[authoryear,round]{natbib}
\title{Hybrid Partial Least Squares Regression with Multiple Functional and Scalar Predictors}

\begin{document}
	\begin{abstract}
	Motivated by renal imaging data with accompanying side information, we propose a hybrid partial least squares (PLS) regression method to capture cross-modality correlations in settings with both functional and scalar predictors. Our approach extends the nonlinear iterative PLS (NIPALS) algorithm to a hybrid Hilbert space, embedding functional and scalar predictors into a unified vector representation and iteratively maximizing their empirical cross-covariance with the response. The resulting low-dimensional representations effectively capture within- and between-modality variation and predictor-response correlation. Out method  preserves the desirable theoretical properties of PLS. Moreover, the procedure is computationally efficient, requiring only the solution of linear systems at each iteration.
\end{abstract}

\begin{keywords}
Dimension reduction;
Functional data analysis;
Multiple data modalities;
Multivariate data analysis;
Multivariate functional data;
Partial least square
\end{keywords}

\maketitle
\newpage
\section{Introduction} \label{sec: Introduction}
Biomedical studies often collect multi-modal data.
Our running example is the Emory University renal study \citep{changBayesianLatentClass2020, jangPrincipalComponentAnalysis2021}, which records two renogram curves (functional data) along with  side information (Euclidean vector) for each kidney.
Such distinct yet related physiological signals can be represented as functional- and scalar-valued i.i.d. covariates in a linear regression model:
\begin{equation}	\label{eq:hybrid_regression_model}
	Y_i
	= 
	\boldsymbol{\beta}^\top \mathbf{Z}_i
	+ 
	\sum 
	\limits_{k=1}^K 
	\int  \beta_k(t) X_{ik}(t) dt + \epsilon_i,
	\quad
	i = 1, \ldots, n,
	%= \langle \boldsymbol{\alpha}, \mathbf{Z} \rangle +  \langle \beta, X \rangle_{\mathcal{F}}  + \epsilon
\end{equation}
where $Y_i$ is a scalar response, 
$X_{i1}(t), \ldots, X_{iK}(t) \in \mathbb{L}^2[0,1]$ are functional predictors, 
$\mathbf{Z}_i = (Z_{i1}, \ldots, Z_{ip})^\top$ is a Euclidean vector covariate,
and $\epsilon_i$ denotes observational noise.
For notational convenience, we assume throughout that the responses and predictors are centered, so the intercept term can be omitted.
The key   challenge here is the strong correlations between functional and scalar predictors,
in addition to the ill-posedness of the infinite-dimensional slope functions $\beta_k(t)$ and the high dimensionality of the predictors ($K + p$). This paper addresses all three issues in a unified manner by introducing a hybrid partial least squares (PLS) regression framework defined on a novel hybrid Hilbert space.

	\paragraph{Previous works and limitations}
Basis expansion with regularization of roughness addresses the ill-posedness of functional data by exploiting their smoothness \citep{cardotSplineEstimatorsFunctional2003,aguileraPenalizedVersionsFunctional2016, 
	caiPredictionFunctionalLinear2006, zhaoWaveletbasedLASSOFunctional2012}.
High dimensionality can be addressed by using derived inputs, with PCA regression as a straightforward example. We can apply PCA  separately to functional (e.g., \citealt{happ_multivariate_2018}) and scalar predictors, followed by multivariate regression on the combined scores. While well studied for functional predictors alone \citep{hallMethodologyConvergenceRates2007, reissFunctionalPrincipalComponent2007, febrero-bandeFunctionalPrincipalComponent2017}, PCA regression may fail to capture the core regression relationship since the derived inputs are not guided by the response.
Partial least squares (PLS) regression is a powerful alternative that constructs orthogonal latent components from predictors to maximize covariance with the response.
For functional predictors, \citet{predaPLSRegressionStochastic2005} introduced PLS regression for a single predictor, with subsequent extensions incorporating
basis approximations and roughness penalties
\citep{reissFunctionalPrincipalComponent2007, aguileraUsingBasisExpansions2010, aguileraPenalizedVersionsFunctional2016}, and multiple functional predictors \citep{beyaztasRobustFunctionalPartial2022}.
Theoretical and computational advances were developed by \citet{delaigleMethodologyTheoryPartial2012} and \citet{saricamPartialLeastsquaresEstimation2022}, respectively; see \citet{febrero-bandeFunctionalPrincipalComponent2017} for a review.
These approaches ignore correlations between functional and scalar components, leading to multicollinearity and unreliable prediction. Multimodal correlations have mainly been explored in unsupervised settings,
such as graphical models, precision matrix estimation, and joint PCA \citep{kolar_graph_2014, gengJointNonparametricPrecision2020, jangPrincipalComponentAnalysis2021}. However,  these methods are response-agnostic and may miss outcome-relevant correlations.

\paragraph{Our contributions}
To address these gaps, we propose a hybrid PLS regression framework that unifies basis expansion, PLS, and functional-scalar correlations. Treating functional and Euclidean components as a single vector in a Hilbert space with a suitable inner product, we iteratively extract joint directions that maximize covariance with the response under a unit-norm constraint. The framework handles dense or irregular functional data, supports regularization to reduce overfitting, and comes with theoretical guarantees.

\section{Hybrid Hilbert Space}
Our framework leverages that the nonlinear iterative PLS (NIPALS; \citealp{woldPathModelsLatent1975}) algorithm, a standard PLS fitting method, operates solely via inner-product space operations. We define a Hilbert space for hybrid predictors with a complete inner product to extend the algorithm.
%
\begin{theorem}[Hybrid space]\label{def:hilbert_space}
	An element $h \in \mathbb{H} := (\mathbb{L}^2[0,1])^K \times \mathbb{R}^p$ is an ordered tuple $h = (f_1, \dots, f_K, \mathbf{u})$, where $f_k \in \mathbb{L}^2[0,1]$ for $k=1, \dots, K$ and $\mathbf{u} \in \mathbb{R}^p$.  
We define an inner product on $\mathbb{H}$ for any two elements $h_1 = (f_1, \dots, f_K, \mathbf{u})$ and $h_2 = (g_1, \dots, g_K, \mathbf{v})$ as:
\begin{equation}\label{eq: hybrid inner product}
	\langle h_1, h_2 \rangle_{\mathbb{H}} := \sum_{k=1}^K \int_0^1 f_k(t) g_k(t) \, dt +   \mathbf{u}^\top \mathbf{v}.
\end{equation}
	The inner product induces a norm $\Vert \cdot \Vert_{\mathbb{H}}$ on the space, defined as $\Vert h \Vert_{\mathbb{H}} := \langle h, h \rangle_{\mathbb{H}}^{1/2}$, and a corresponding metric $d(h_1, h_2) = \Vert h_1 - h_2 \Vert_{\mathbb{H}}$.
	Then the hybrid inner product space $\mathbb{H}$ is a separable Hilbert space.
\end{theorem}


\begin{definition}[Hybrid Predictor]\label{def:hybrid_predictor}
	For the Hilbert space  $\mathbb{H}$ 
	defined in Definition \ref{def:hilbert_space},
	The Borel $\sigma$-field on $\mathbb{H}$, denoted $\mathfrak{B}(\mathbb{H})$, is the smallest $\sigma$-field containing the class $\mathfrak{M}$ of all sets of the form
	$
	\{q \in \mathbb{H} \mid \langle q, h \rangle \in O\},
	$
	for any $h \in \mathbb{H}$ and any open subset $O \subseteq \mathbb{R}$ (details can be found in Theorem 7.1.1 of \citealt{hsingTheoreticalFoundationsFunctional2015}).
	A hybrid predictor $ W_i = (X_{i1}(t), \ldots, X_{iK}(t), \mathbf{Z}_i)$ is a measurable mapping from a probability space $(\Omega, \mathfrak{F}, P)$ into   $
	\bigl(
	\mathbb{H}, \mathfrak{B}(\mathbb{H})
	\bigr)
	$. 
\end{definition}
Then the joint regression model \eqref{eq:hybrid_regression_model}  can be concisely written as
\begin{equation}		\label{eq: Hybrid functional model}
	Y_i = \langle \beta, W_i \rangle_{\mathbb{H}} + \epsilon_i,~\text{where}~\beta := \bigl( \beta_1(t), \ldots, \beta_K(t), \boldsymbol{\beta} \bigr)\in \mathbb{H}.
\end{equation}

\section{Proposed PLS Algorithm}\label{section:main:our_algorithm}
Our approach efficiently computes PLS components and scores for multiple dense or irregular functional predictors alongside scalar predictors.   It incorporates regularization to exploit structural relationships within and between the functions, to prevent overfitting and improve interpretability.
Each iteration involves two subroutines: regularized component estimation (Section \ref{section:sub:compute_PLS_component})  and residualization 
(Section \ref{section:sub:residualization})
After iteration terminates 
the hybrid regression coefficient is estimated (Section~\ref{section:sub:regression_coeff}).
  
\subsection{Preliminary step 1: finite-basis approximation}\label{sec: finite basis approximation}
Let $\{b_m(t)\}$ be a twice-differentiable basis of $\mathbb{L}^2([0,1])$ whose second derivatives are also linearly independent, for example, cubic B-splines, Fourier, or orthonormal cubic polynomials. 
Fourier coefficients of
functional predictors
$X_{ij}(t)$,
functional regression coefficients
$\beta_j(t)$,
PLS directions,
$\xi_j(t)$,
and residualization coefficients
$\delta_j(t)$
(with iteration indices suppressed) with respect to 
$\{b_m(t)\}$ are denoted as
$\{\theta_{ijm}\},
\{ \eta_{jm} \},
\{ \gamma_{jm} \},
$
and
$\{ \pi_{jm} \}$.
For practicality, we truncate the expansion at a moderate $M$ (e.g., 15-20) to capture most functional variation, while smoothness is handled via penalization (Section~\ref{sec: Regularization of PLS Components}).
The truncated expansion for the functional data is denoted
as
are denoted as $\widetilde{X}_{ij}(t)
:=
\sum_{m=1}^M \theta_{ijm}  b_m(t)$, but for the other funtions, we denote
\begin{equation}\label{def:truncated_zeta_and_delta}
	\beta_j(t)
	:=
	\sum_{m=1}^M \eta_{jm} b_m(t),
	%
	\xi_j(t) := \sum_{m=1}^M \gamma_{jm}  b_m(t),
	%
	\delta_j(t)
	=
	\sum_{m=1}^M
	\pi_{jm}
	b_m(t),
\end{equation}
since these functions are estimated, using a hat over the tilde notation would be cumbersome.     Let Let $\boldsymbol{\theta}_{ij}$, $\boldsymbol{\eta}_j$, $\boldsymbol{\gamma}_j$, and $\boldsymbol{\pi}_j$ denote these coefficients as $M$-dimensional vectors.
The truncations imply that all computations in this paper are carried out entirely within the subspace
\begin{equation}\label{def:truncated_hilbert_space}
	\widetilde{ \mathbb{H} } := \operatorname{span}\bigl(b_1(t), \ldots, b_M(t)\bigr)^K \times \mathbb{R}^p \subset \mathbb{H}.
\end{equation}
The $i$th hybrid predictor, projected on $\widetilde{ \mathbb{H} }$, is represented by the tuple
$ 
\widetilde{W}_i := 
(
\widetilde{X}_{i1}, \ldots, \widetilde{X}_{iK}, \boldsymbol{Z}_i
).
$ 
For notational convenience, we stack the predictor coefficient vectors as
\begin{equation}\label{def:theta}
	\Theta_j := (\boldsymbol{\theta}_{1j}, \ldots, \boldsymbol{\theta}_{nj})^\top  \in \mathbb{R}^{n \times M},
	\quad 
	\Theta := (\Theta_1, \ldots, \Theta_K, \mathbf{Z}) \in \mathbb{R}^{n \times (MK + p)}.
\end{equation}
Let us denote the response vector as $\mathbf{y} := (y_1, \ldots, y_n)^\top$.
Let $B, B^{\prime\prime} \in \mathbb{R}^{M \times M}$ denote the Gram matrices of the basis functions and their second derivatives, with entries
\begin{equation}\label{def:B_matrix}
	B_{m, m'} := \int_0^1 b_m(t)\, b_{m'}(t)\, dt, \quad
	B^{\prime\prime}_{m, m'} := \int_0^1 b_m''(t)\, b_{m'}''(t)\, dt,
\end{equation}
for $m, m' = 1, \ldots, M$. We then define the block-diagonal matrices
\begin{equation}\label{def:gram_block}
	\mathbb{B} := \operatorname{blkdiag}(B, \ldots, B, I_p), \quad
	\mathbb{B}^{\prime\prime} := \operatorname{blkdiag}(B^{\prime\prime}, \ldots, B^{\prime\prime}, I_p),
\end{equation}

Then the full data for the hybrid PLS problem at the $l$-th iteration can be represented by the tuple
\begin{equation}\label{def:problem_instance}
	(\mathbb{B}, \mathbb{B}^{\prime\prime}, \Theta, \mathbf{y}) \in 
	\mathbb{R}^{(MK+p) \times (MK+p)} \times 
	\mathbb{R}^{(MK+p) \times (MK+p)} \times 
	\mathbb{R}^{n \times (MK+p)} \times 
	\mathbb{R}^n,
\end{equation}
with the index $l$ omitted for brevity.


\begin{remark}
	Different bases could be used for each functional predictor to handle  multiple dense or irregular functional predictors. We adopt a common basis for simplicity. The definitions of $\mathbb{B}$ and $\mathbb{B}^{\prime\prime}$ remain general enough to accommodate distinct bases if needed.
\end{remark}

			\bibliography{bibliography.bib}
			
\end{document}
