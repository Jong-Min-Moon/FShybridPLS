---
title: "simulation"
output: html_document
date: "2025-12-29"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# ==============================================================================
# STUDY: GEOMETRIC PROPERTIES & PREDICTIVE RELEVANCE VALIDATION
# Verifies:
#   1. Penalized Orthonormality of Weights (Xi)
#   2. Orthogonality of Scores (Rho)
#   3. Decreasing Correlation between Response (Y) and Scores
# ==============================================================================

library(fda)
library(dplyr)
library(tidyr)
library(ggplot2)
library(FSHybridPLS)

# ------------------------------------------------------------------------------
# 2. DATA GENERATION (Latent Factor Model)
# ------------------------------------------------------------------------------
generate_torture_data <- function(n_sample = 100) {
  # 1. Latent Factors (Scale Mismatch 100:1)
  t1 <- rnorm(n_sample, sd = 10)   # High variance
  t2 <- rnorm(n_sample, sd = 0.1)  # Low variance
  
  # 2. Functional Predictors (Basis M=15)
  eval_points <- seq(0, 1, length.out = 100)
  basis <- create.bspline.basis(c(0, 1), 15)
  
  # X1: Pure signal from t1 (Low frequency sine)
  X1_mat <- outer(t1, sin(2 * pi * eval_points))
  fd1 <- Data2fd(eval_points, t(X1_mat), basis)
  
  # X2: Signal from t2 (High frequency sine) + Noise
  # Noise added to the curves as per specification
  noise_func <- matrix(rnorm(n_sample * 100, sd = 0.01), 100, n_sample)
  X2_mat <- outer(t2, sin(10 * pi * eval_points)) + noise_func
  fd2 <- Data2fd(eval_points, t(X2_mat), basis)
  
  # 3. Rank-Deficient Scalar Predictors (Z)
  # Z = A * [t1, t2]^T + Noise
  # P = 50 predictors derived from 2 latent factors
  P <- 50
  A <- matrix(runif(P * 2, min = -1, max = 1), nrow = P, ncol = 2) # Fixed loading matrix
  
  # Combine latent factors into (n_sample x 2) matrix
  T_factors <- cbind(t1, t2)
  
  # Generate Z (n_sample x 50)
  Z_clean <- T_factors %*% t(A) 
  Z_noise <- matrix(rnorm(n_sample * P, sd = 1), n_sample, P) # Standard normal noise
  Z <- Z_clean + Z_noise
  
  # 4. Response Y
  # Implicit regression coefficients: 0.5 for t1, 10 for t2
  y_clean <- 0.5 * t1 + 10 * t2
  y <- y_clean + rnorm(n_sample, sd = 1) # Standard normal noise
  
  # 5. Pack into Hybrid Object
  W_init <- predictor_hybrid(Z, list(fd1, fd2), eval_points)
  
  return(list(W = W_init, y = y))
}

# ------------------------------------------------------------------------------
# 3. SIMULATION CONFIGURATION
# ------------------------------------------------------------------------------
n_rep  <- 100  # Increased for robust stats
n_comp <- 6
scenarios <- list(
  "Weak"   = c(0.1, 0.1),
  "Mixed"  = c(0.1, 3.0),
  "Strong" = c(3.0, 3.0)
)

results_ortho <- data.frame() # Stores max deviations per rep
results_corr  <- data.frame() # Stores Y-Score correlations per component

# ------------------------------------------------------------------------------
# 4. MAIN LOOP WITH SMOOTH PROGRESS BAR
# ------------------------------------------------------------------------------
set.seed(2025)
message("Starting Geometric Validation Simulation...")

# Total iterations = Scenarios * Repetitions
total_iters <- length(scenarios) * n_rep
counter <- 0

# Initialize Progress Bar
pb <- txtProgressBar(min = 0, max = total_iters, style = 3)

start_time <- Sys.time()

    
  for(i in 1:n_rep) {
    # A. Generate Data
    data <- generate_torture_data(100)
    processed <- split_and_normalize.all(data$W, data$y, train_ratio=0.99)
    W_train <- processed$predictor_train
    y_train <- processed$response_train
    
    # B. Prep Matrices
    basis_obj <- W_train$functional_list[[1]]$basis
    J_mat     <- fda::inprod(basis_obj, basis_obj)
    Omega_mat <- fda::getbasispenalty(basis_obj, Lfdobj=2)
    
    # Manually inject pre-computed matrices if your constructor doesn't do it
    # (Assuming predictor_hybrid constructor handles this, but explicit here for safety)
    W_train$gram_list <- list(J_mat, J_mat)
    W_train$gram_deriv2_list <- list(Omega_mat, Omega_mat) 
    
    # C. Fit Model
    fit <- fit.hybridPLS(W_train, y_train, n_iter=n_comp, lambda=lam_vec)
    
    # --- CHECK 1: WEIGHT ORTHONORMALITY ---
    max_dev_xi <- 0
    # We only check off-diagonals and diagonals. 
    # Diagonal should be 1, Off-diagonal should be 0.
    for(r in 1:n_comp) {
      for(c in 1:n_comp) {
        val <- inprod_pen.predictor_hybrid(fit$xi[[r]], fit$xi[[c]],  lam_vec)
        
        expected <- ifelse(r == c, 1.0, 0.0)
        diff <- abs(val - expected)
        if(diff > max_dev_xi) max_dev_xi <- diff
      }
    }
    
    # --- CHECK 2: SCORE ORTHOGONALITY ---
    Rho_mat <- do.call(cbind, fit$rho)
    # Compute correlation matrix of scores
    # Handle edge case where a score vector is constant (sd=0) -> returns NA
    rho_cor_mat <- cor(Rho_mat)
    rho_cor_mat[is.na(rho_cor_mat)] <- 0 
    
    # Get max off-diagonal absolute value
    diag(rho_cor_mat) <- 0
    max_dev_rho <- max(abs(rho_cor_mat))
    
    # Store Ortho Results
    results_ortho <- rbind(results_ortho, data.frame(
      Scenario = scen_name,
      Rep = i,
      Max_Error_Weight = max_dev_xi,
      Max_Error_Score = max_dev_rho
    ))
    
    # --- CHECK 3: Y-SCORE CORRELATION ---
    y_corrs <- abs(cor(y_train, Rho_mat))
    for(k in 1:n_comp) {
      results_corr <- rbind(results_corr, data.frame(
        Scenario = scen_name,
        Rep = i,
        Component = k,
        Abs_Correlation = y_corrs[1, k]
      ))
    }
    
    # Update Progress Bar
    counter <- counter + 1
    setTxtProgressBar(pb, counter)
  }
 

close(pb)
duration <- round(as.numeric(difftime(Sys.time(), start_time, units="secs")), 2)
message(paste("\nSimulation Complete in", duration, "seconds."))

# ------------------------------------------------------------------------------
# 5. SUMMARY TABLES & LATEX
# ------------------------------------------------------------------------------

# Summary for Orthogonality
summary_ortho <- results_ortho %>%
  group_by(Scenario) %>%
  summarise(
    Mean_Weight_Error = mean(Max_Error_Weight),
    SE_Weight_Error = sd(Max_Error_Weight)/sqrt(n()),
    Mean_Score_Error = mean(Max_Error_Score),
    SE_Score_Error = sd(Max_Error_Score)/sqrt(n())
  )

cat("\n% --- LaTeX Table: Geometric Validation ---\n")
cat("\\begin{table}[ht]\n\\centering\n")
cat("\\caption{Validation of Geometric Properties. Values indicate the mean maximum deviation from theoretical orthogonality. 'Weight Error' measures deviation from orthonormality in $\\langle \\cdot, \\cdot \\rangle_{\\mathbb{H}, \\Lambda}$. 'Score Error' measures deviation from orthogonality in Euclidean space.}\n")
cat("\\label{tab:geom_validation}\n")
cat("\\begin{tabular}{l c c c c}\n")
cat("\\hline\n")
cat(" & \\multicolumn{2}{c}{\\textbf{Weight Orthonormality}} & \\multicolumn{2}{c}{\\textbf{Score Orthogonality}} \\\\\n")
cat("Scenario & Mean Max Error & SE & Mean Max Error & SE \\\\\n")
cat("\\hline\n")

for(r in 1:nrow(summary_ortho)) {
  s <- summary_ortho$Scenario[r]
  w_mu <- formatC(summary_ortho$Mean_Weight_Error[r], format="e", digits=2)
  w_se <- formatC(summary_ortho$SE_Weight_Error[r], format="e", digits=2)
  s_mu <- formatC(summary_ortho$Mean_Score_Error[r], format="e", digits=2)
  s_se <- formatC(summary_ortho$SE_Score_Error[r], format="e", digits=2)
  cat(sprintf("%-10s & %s & %s & %s & %s \\\\\n", s, w_mu, w_se, s_mu, s_se))
}
cat("\\hline\n\\end{tabular}\n\\end{table}\n")

# ------------------------------------------------------------------------------
# 6. VISUALIZATION (Y-Score Trend)
# ------------------------------------------------------------------------------
summary_corr <- results_corr %>%
  group_by(Scenario, Component) %>%
  summarise(
    Mean_Corr = mean(Abs_Correlation, na.rm=TRUE),
    SE_Corr = sd(Abs_Correlation, na.rm=TRUE)/sqrt(n())
  )

ggplot(summary_corr, aes(x=Component, y=Mean_Corr, color=Scenario, group=Scenario)) +
  geom_line(linewidth=1) +
  geom_point(size=2) +
  geom_errorbar(aes(ymin=Mean_Corr-SE_Corr, ymax=Mean_Corr+SE_Corr), width=0.2) +
  scale_x_continuous(breaks=1:n_comp) +
  labs(title="Predictive Relevance",
       subtitle="Correlation between Response Y and PLS Scores",
       y="Absolute Correlation |Cor(y, rho)|",
       x="PLS Component Index") +
  theme_bw() +
  theme(legend.position="bottom")
```
```{r}
# ==============================================================================
# MANUSCRIPT TABLE: PREDICTIVE RELEVANCE (Y-SCORE CORRELATION)
# ==============================================================================

library(dplyr)
library(tidyr)
library(knitr)
library(kableExtra)

# 1. SUMMARIZE DATA
# ------------------------------------------------------------------------------
# We calculate the Mean and Standard Error for each Component/Scenario pair
summary_corr <- results_corr %>%
  group_by(Scenario, Component) %>%
  summarise(
    Mean_Corr = mean(Abs_Correlation),
    SE_Corr   = sd(Abs_Correlation) / sqrt(n()),
    .groups   = 'drop'
  ) %>%
  # Create a combined string "Mean (SE)"
  mutate(
    Value = sprintf("%.3f (%.3f)", Mean_Corr, SE_Corr)
  )

# 2. RESHAPE FOR TABLE
# ------------------------------------------------------------------------------
# Rows: Component Index
# Columns: Scenarios
table_wide <- summary_corr %>%
  select(Component, Scenario, Value) %>%
  pivot_wider(names_from = Scenario, values_from = Value) %>%
  # Reorder columns logically
  select(Component, Weak, Mixed, Strong)
```



# regression coefficient

 
 
```{r coeffcient_3}
# ==============================================================================
# STUDY: COEFFICIENT RECOVERY (Full-Rank, Aggregate Viz)
# ==============================================================================
# ==============================================================================
# STUDY: ROBUST COEFFICIENT RECOVERY (Complex Shapes + CV Grid Search)
# ==============================================================================
```{r}

library(fda)
library(dplyr)
library(tidyr)
library(ggplot2)

# ------------------------------------------------------------------------------
# 1. SETUP: COMPLEX GROUND TRUTH & DATA GENERATION
# ------------------------------------------------------------------------------
generate_complex_data <- function(n_sample = 200) {
  # Basis Setup
  eval_points <- seq(0, 1, length.out=100)
  # Use enough basis functions to capture the complexity
  basis <- create.bspline.basis(c(0,1), 20) 
  nbasis <- basis$nbasis
  
  # A. TRUE COEFFICIENTS (More Complicated)
  # Beta 1: Growing Sine Wave (Modulated Amplitude)
  # f(t) = 2 * t * sin(3 * pi * t)
  beta1_vec <- 2 * eval_points * sin(3 * pi * eval_points)
  beta1_fd  <- Data2fd(eval_points, beta1_vec, basis)
  
  # Beta 2: Double Gaussian Bump (Multimodal)
  # f(t) = exp(-50(t-0.3)^2) + 0.5 * exp(-50(t-0.7)^2)
  beta2_vec <- exp(-50 * (eval_points - 0.3)^2) + 0.5 * exp(-50 * (eval_points - 0.7)^2)
  beta2_fd  <- Data2fd(eval_points, beta2_vec, basis)
  
  beta_z_true <- c(1.5, -1.0)
  
  # B. PREDICTORS (Rich but Correlated)
  # Generate random coefficients to ensure full rank (identifiability)
  coefs1 <- matrix(rnorm(n_sample * nbasis), nbasis, n_sample)
  coefs2 <- matrix(rnorm(n_sample * nbasis), nbasis, n_sample)
  
  # Introduce Correlation: X2 depends partially on X1
  coefs2 <- 0.6 * coefs2 + 0.4 * coefs1
  
  fd1 <- fd(coefs1, basis)
  fd2 <- fd(coefs2, basis)
  
  # Z correlated with X
  z_latent <- t(coefs1[1:2, ]) # Z depends on first few modes of X1
  Z <- z_latent + matrix(rnorm(n_sample*2, sd=0.5), n_sample, 2)
  
  # C. RESPONSE Y
  y_func1 <- inprod(fd1, beta1_fd)
  y_func2 <- inprod(fd2, beta2_fd)
  y_scalar <- Z %*% beta_z_true
  
  # SNR: Signal to Noise Ratio
  y_clean <- as.vector(y_func1 + y_func2 + y_scalar)
  sigma   <- sd(y_clean) * 0.1 # 10% Noise
  y <- y_clean + rnorm(n_sample, sd=sigma)
  
  # Pack Object
  W <- predictor_hybrid(Z, list(fd1, fd2), eval_points)
  W$gram_list <- list(inprod(basis, basis), inprod(basis, basis))
  
  return(list(
    W = W, y = y,
    truth = list(beta_fd = list(beta1_fd, beta2_fd), beta_z = beta_z_true,
                 vec1 = beta1_vec, vec2 = beta2_vec)
  ))
}

# ------------------------------------------------------------------------------
# 2. HELPER: CV GRID SEARCH
# ------------------------------------------------------------------------------
select_best_lambda <- function(data, grid, n_comp) {
  n <- length(data$y)
  # 70/30 Train/Validation Split
  train_idx <- sample(1:n, size=floor(0.7*n))
  
  # Helper to subset the hybrid object
  subset_W <- function(W, idx) {
    new_Z <- W$Z[idx, , drop=FALSE]
    new_F <- lapply(W$functional_list, function(f) fd(f$coefs[, idx, drop=FALSE], f$basis))
    res <- predictor_hybrid(new_Z, new_F, W$eval_points)
    res$gram_list <- W$gram_list
    return(res)
  }
  
  W_train <- subset_W(data$W, train_idx)
  y_train <- data$y[train_idx]
  W_val   <- subset_W(data$W, -train_idx)
  y_val   <- data$y[-train_idx]
  val_data <- list(W_test = W_val, y_test = y_val)
  
  best_rmse <- Inf
  best_lam  <- grid[1]
  
  for(lam in grid) {
    # Fit
    fit <- fit.hybridPLS(W_train, y_train, n_iter = n_comp, 
                         lambda = c(lam, lam), validation_data = val_data)
    
    # Check RMSE at the target component
    curr_rmse <- fit$validation_rmse[n_comp]
    
    if(curr_rmse < best_rmse) {
      best_rmse <- curr_rmse
      best_lam  <- lam
    }
  }
  return(best_lam)
}

# ------------------------------------------------------------------------------
# 3. MAIN SIMULATION LOOP
# ------------------------------------------------------------------------------
n_rep <- 100
n_comp_fixed <- 12 # Sufficient to capture shape
lambda_grid  <- c(5, 3, 1, 1e-1, 1e-2, 1e-3)

# Storage for Aggregation
eval_points <- seq(0, 1, length.out=100)
store_beta1 <- matrix(0, 100, n_rep)
store_beta2 <- matrix(0, 100, n_rep)
errors_log  <- data.frame()

set.seed(2025)
message("Starting Simulation with Grid Search...")

for(i in 1:n_rep) {
  # Generate
  sim <- generate_complex_data(1000)
  
  # Tune
  opt_lam <- select_best_lambda(sim, lambda_grid, n_comp_fixed)
  
  # Refit on Full Data
  fit <- fit.hybridPLS(sim$W, sim$y, n_iter = n_comp_fixed, lambda = c(opt_lam, opt_lam))
  beta_est <- fit$beta[[n_comp_fixed]]
  
  # Store Curves
  curve1 <- eval.fd(eval_points, beta_est$functional_list[[1]])
  curve2 <- eval.fd(eval_points, beta_est$functional_list[[2]])
  store_beta1[, i] <- as.vector(curve1)
  store_beta2[, i] <- as.vector(curve2)
  
  # Compute Relative Errors
  norm_t1 <- sqrt(inprod(sim$truth$beta_fd[[1]], sim$truth$beta_fd[[1]]))
  norm_t2 <- sqrt(inprod(sim$truth$beta_fd[[2]], sim$truth$beta_fd[[2]]))
  norm_ts <- sqrt(sum(sim$truth$beta_z^2))
  
  d1 <- minus.fd(sim$truth$beta_fd[[1]], beta_est$functional_list[[1]])
  d2 <- minus.fd(sim$truth$beta_fd[[2]], beta_est$functional_list[[2]])
  ds <- sim$truth$beta_z - as.vector(beta_est$Z)
  
  errors_log <- rbind(errors_log, data.frame(
    Rep = i,
    Best_Lambda = opt_lam,
    Rel_Err_F1 = sqrt(inprod(d1, d1)) / norm_t1,
    Rel_Err_F2 = sqrt(inprod(d2, d2)) / norm_t2,
    Rel_Err_Sc = sqrt(sum(ds^2)) / norm_ts
  ))
}

```


```{r}

# ------------------------------------------------------------------------------
# 4. RESULTS: TABLE
# ------------------------------------------------------------------------------
# Summary Table
final_table <- errors_log %>%
  summarise(
    `Mean Lambda` = mean(Best_Lambda),
    `Func 1 Error (Mean)` = mean(Rel_Err_F1),
    `Func 1 Error (SE)`   = sd(Rel_Err_F1)/sqrt(n()),
    `Func 2 Error (Mean)` = mean(Rel_Err_F2),
    `Func 2 Error (SE)`   = sd(Rel_Err_F2)/sqrt(n()),
    `Scalar Error (Mean)` = mean(Rel_Err_Sc),
    `Scalar Error (SE)`   = sd(Rel_Err_Sc)/sqrt(n())
  )

print(t(final_table)) # Transpose for readability

# ------------------------------------------------------------------------------
# 5. RESULTS: PLOT
# ------------------------------------------------------------------------------
process_curves <- function(mat, truth, label) {
  df <- data.frame(
    Time = eval_points,
    Mean = rowMeans(mat),
    SD   = apply(mat, 1, sd),
    Truth = truth,
    Type = label
  )
  df$Lower <- df$Mean - 2 * df$SD
  df$Upper <- df$Mean + 2 * df$SD
  return(df)
}

df1 <- process_curves(store_beta1, sim$truth$vec1, "Beta 1: t * Sin(3pi*t)")
df2 <- process_curves(store_beta2, sim$truth$vec2, "Beta 2: Double Bump")
plot_data <- rbind(df1, df2)

p <- ggplot(plot_data, aes(x=Time)) +
  # Confidence Band
  geom_ribbon(aes(ymin=Lower, ymax=Upper), fill="steelblue", alpha=0.3) +
  # Truth
  geom_line(aes(y=Truth, color="True Coefficient"), linewidth=1.2) +
  # Mean Estimate
  geom_line(aes(y=Mean, color="Mean Estimate"), linetype="dashed", linewidth=1.2) +
  
  facet_wrap(~Type, scales="free_y") +
  scale_color_manual(values=c("True Coefficient"="black", "Mean Estimate"="firebrick")) +
  labs(title = "Recovery of Complex Functional Coefficients",
       subtitle = sprintf("Mean Estimate +/- 2 SD over %d Replications (Lambda chosen via CV)", n_rep),
       y = "Coefficient Value", x = "t") +
  theme_bw() +
  theme(legend.position="bottom", 
        strip.text = element_text(face="bold", size=11))

print(p)
```


```{r}
 
# Define paper-ready colors
color_truth <- "black"
color_est   <- "#D55E00" # Vermilion (Colorblind-friendly)
fill_band   <- "#56B4E9" # Sky Blue (Colorblind-friendly)

p <- ggplot(plot_data, aes(x=Time)) +
  # 1. Confidence Band (Empirical 95% Interval)
  geom_ribbon(aes(ymin=Lower, ymax=Upper, fill="Empirical 95% CI"), alpha=0.25) +
  
  # 2. Truth Line
  geom_line(aes(y=Truth, color="True Coefficient"), linewidth=1) +
  
  # 3. Mean Estimate Line
  geom_line(aes(y=Mean, color="Mean Estimate"), linetype="longdash", linewidth=1) +
  
  # 4. Faceting with clean labels
  facet_wrap(~Type, scales="free_y", 
             labeller = labeller(Type = c(
               "Beta1" = expression(bold(beta[1](t)) ~ "(Sine Wave)"),
               "Beta2" = expression(bold(beta[2](t)) ~ "(Gaussian Bumps)")
             ))) +
  
  # 5. Manual Scales for Legend Control
  scale_color_manual(name = NULL, 
                     values = c("True Coefficient"=color_truth, "Mean Estimate"=color_est)) +
  scale_fill_manual(name = NULL, 
                    values = c("Empirical 95% CI"=fill_band)) +
  
  # 6. Labels using expressions
  labs(
    x = "Domain (t)",
    y = expression(beta(t))
  ) +
  
  # 7. Professional Theme
  theme_classic(base_size = 14) +
  theme(
    # Fonts and Text
    text = element_text(family = "serif"), # Matches LaTeX/Word documents
    strip.background = element_blank(),
    strip.text = element_text(size = 14, face = "bold", hjust = 0),
    
    # Legend Placement
    legend.position = "bottom",
    legend.box = "horizontal",
    legend.margin = margin(t = -5),
    legend.key.width = unit(1.5, "cm"), # Longer lines in legend
    
    # Axis cleanliness
    axis.line = element_line(linewidth = 0.5, color = "black"),
    axis.ticks = element_line(color = "black"),
    panel.grid.major.y = element_line(color = "grey90", linewidth = 0.2), # Subtle grid
    panel.grid.major.x = element_blank(),
    panel.grid.minor = element_blank()
  ) + 
  
  # Ensure Y-axis zero line is clear if relevant, or just let scales handle it
  geom_hline(yintercept = 0, linetype = "solid", color = "grey80", size = 0.3)

# Print
print(p)

# Suggestion for saving (Standard 2-column width for journals is approx 7 inches)
 ggsave("coeff_recovery_plot.pdf", p, width = 8, height = 4, device = cairo_pdf)
```



```{r}
# ==============================================================================
# SEQUENTIAL SIMULATION: 9 REGULARIZATION COMBINATIONS
# ==============================================================================

library(fda)
library(dplyr)
library(tidyr)
# library(FSHybridPLS) # Ensure your package/functions are loaded

# ------------------------------------------------------------------------------
# 1. DATA GENERATION
# ------------------------------------------------------------------------------
generate_complex_data <- function(n_sample = 200) {
  eval_points <- seq(0, 1, length.out=100)
  basis <- create.bspline.basis(c(0,1), 20) 
  nbasis <- basis$nbasis
  
  # Truth
  beta1_vec <- 2 * eval_points * sin(3 * pi * eval_points)
  beta1_fd  <- Data2fd(eval_points, beta1_vec, basis)
  
  # "Easier" Single Bump
  beta2_vec <- 2 * exp(-10 * (eval_points - 0.5)^2) 
  beta2_fd  <- Data2fd(eval_points, beta2_vec, basis)
  
  beta_z_true <- c(1.5, -1.0)
  
  # Predictors
  coefs1 <- matrix(rnorm(n_sample * nbasis), nbasis, n_sample)
  coefs2 <- matrix(rnorm(n_sample * nbasis), nbasis, n_sample)
  coefs2 <- 0.6 * coefs2 + 0.4 * coefs1
  
  fd1 <- fd(coefs1, basis)
  fd2 <- fd(coefs2, basis)
  
  z_latent <- t(coefs1[1:2, ]) 
  Z <- z_latent + matrix(rnorm(n_sample*2, sd=0.5), n_sample, 2)
  
  # Response
  y_func1 <- inprod(fd1, beta1_fd)
  y_func2 <- inprod(fd2, beta2_fd)
  y_scalar <- Z %*% beta_z_true
  y_clean <- as.vector(y_func1 + y_func2 + y_scalar)
  y <- y_clean + rnorm(n_sample, sd=sd(y_clean)*0.05)
  
  W <- predictor_hybrid(Z, list(fd1, fd2), eval_points)
  
  return(list(
    W = W, y = y,
    truth = list(beta_fd = list(beta1_fd, beta2_fd), beta_z = beta_z_true)
  ))
}

# ------------------------------------------------------------------------------
# 2. MAIN SIMULATION LOOP
# ------------------------------------------------------------------------------
n_rep <- 30
n_comp_fixed <- 10

# Define the 3 Levels of Regularization
# Low (0.001), Medium (0.1), High (5.0)
lam_levels <- c(0, 0.001, 0.1, 1) 
grid_combos <- expand.grid(L1 = lam_levels, L2 = lam_levels)

# Setup Progress Bar
total_iterations <- n_rep * nrow(grid_combos)
pb <- txtProgressBar(min = 0, max = total_iterations, style = 3)

results_log <- data.frame()
counter <- 0

message(paste("Starting Sequential Simulation:", total_iterations, "total fits..."))
start_time <- Sys.time()

for(i in 1:n_rep) {
  
  # 1. Generate Data (Once per Repetition)
  sim <- generate_complex_data(200)
  
  # Pre-calculate Norms
  norm_t1 <- sqrt(inprod(sim$truth$beta_fd[[1]], sim$truth$beta_fd[[1]]))
  norm_t2 <- sqrt(inprod(sim$truth$beta_fd[[2]], sim$truth$beta_fd[[2]]))
  norm_ts <- sqrt(sum(sim$truth$beta_z^2))
  
  # 2. Loop through the 9 Combinations
  for(k in 1:nrow(grid_combos)) {
    l1 <- grid_combos$L1[k]
    l2 <- grid_combos$L2[k]
    
    # Fit Model
    fit <- fit.hybridPLS(sim$W, sim$y, n_iter = n_comp_fixed, lambda = c(l1, l2))
    beta_est <- fit$beta[[n_comp_fixed]]
    
    # Calculate Errors
    d1 <- minus.fd(sim$truth$beta_fd[[1]], beta_est$functional_list[[1]])
    d2 <- minus.fd(sim$truth$beta_fd[[2]], beta_est$functional_list[[2]])
    ds <- sim$truth$beta_z - as.vector(beta_est$Z)
    
    # Store
    results_log <- rbind(results_log, data.frame(
      Rep = i,
      Lam1 = l1,
      Lam2 = l2,
      Err_Beta1 = sqrt(inprod(d1, d1)) / norm_t1,
      Err_Beta2 = sqrt(inprod(d2, d2)) / norm_t2,
      Err_Scalar = sqrt(sum(ds^2)) / norm_ts
    ))
    
    # Update Progress
    counter <- counter + 1
    setTxtProgressBar(pb, counter)
  }
}

close(pb)
duration <- round(as.numeric(difftime(Sys.time(), start_time, units="secs")), 2)
message(paste("\nSimulation Complete in", duration, "seconds."))

# ------------------------------------------------------------------------------
# 3. SUMMARY
# ------------------------------------------------------------------------------
final_summary <- results_log %>%
  group_by(Lam1, Lam2) %>%
  summarise(
    Mean_Err_Beta1 = mean(Err_Beta1),
    Mean_Err_Beta2 = mean(Err_Beta2),
    Mean_Err_Scalar = mean(Err_Scalar),
    .groups = 'drop'
  ) %>%
  arrange(Lam1, Lam2)

print(final_summary)
```


```{r}
# ==============================================================================
# kidney_analysis_comparison.R
# Compare HybridPLS vs HybridPCR on Real Kidney Data
# 100 Replications | 70/30 Split | Full Normalization
# ==============================================================================

library(fda)
library(dplyr)
library(tidyr)
library(FSHybridPLS) # Assumes your package functions are loaded
# If package not installed, ensure 'split_and_normalize.all', 'fit_hybrid_pcr_iterative', etc. are in env

# ------------------------------------------------------------------------------
# 1. LOAD DATA
# ------------------------------------------------------------------------------
# Read CSV
raw_df <- read.csv("C:/Users/jongmin/Documents/GitHub/fsPLS/data/renogram_data.csv")

# Preprocess (Medical Norm + Smoothing) using your specific function
# This returns list(W = predictor_hybrid, y = transformed_response)
kidney_data <- load_and_preprocess_kidney_data(raw_df, n_basis = 20)

W_full <- kidney_data$W
y_full <- kidney_data$y

# ------------------------------------------------------------------------------
# 2. SETUP SIMULATION
# ------------------------------------------------------------------------------
n_replications <- 5
n_comp_max     <- 3
lambda_val     <- 0
# Lambda vector for 2 functional predictors
lambda_vec     <- c(lambda_val, lambda_val) 

# Storage for results
results_list <- list()

message(sprintf("=== Starting Analysis: %d Replications ===", n_replications))
pb <- txtProgressBar(min = 0, max = n_replications, style = 3)

# ------------------------------------------------------------------------------
# 3. MAIN LOOP
# ------------------------------------------------------------------------------
for (rep_id in 1:n_replications) {
  set.seed(rep_id * 123) # Reproducibility
  
  # A. Split and Normalize
  # This function (from your package code) handles:
  # 1. Splitting indices 70/30
  # 2. Normalizing Train (Scalar/Func/Between/Response)
  # 3. Normalizing Test using Train statistics (No Leakage)
  processed <- split_and_normalize.all(W_full, y_full, train_ratio = 0.6)
  
  W_train <- processed$predictor_train
  W_test  <- processed$predictor_test
  y_train <- processed$response_train
  y_test  <- processed$response_test
  
  # Note: y_test is standardized (mean~0, sd~1) by split_and_normalize.all
  # We calculate RMSE on this standardized scale, then normalize by range if requested.
  # However, split_and_normalize.all usually returns y standardized.
  # The user asked for "test RMSE normalized by the response range".
  # We will calculate RMSE on the normalized scale, then divide by the range of y_test.
  
  y_range <- max(y_test) - min(y_test)
  if(y_range == 0) y_range <- 1
  
  # B. Hybrid PLS
  # fit.hybridPLS returns the full trajectory (rho, xi, etc) but we need to extract predictions manually
  # or rely on the object structure.
  # Alternatively, use a simpler loop if fit.hybridPLS doesn't return RMSE vector directly
  # Your 'fit.hybridPLS' provided earlier returns 'validation_rmse' IF validation_data is passed.
  
  pls_fit <- fit.hybridPLS(
    W_train, y_train, 
    n_iter = n_comp_max, 
    lambda = lambda_vec,
    validation_data = list(W_test = W_test, y_test = y_test)
  )
  rmse_pls <- pls_fit$validation_rmse
  
  # C. Hybrid PCR
  # Use the iterative function defined previously
  rmse_pcr <- fit_hybrid_pcr_iterative(W_train, y_train, W_test, y_test, n_comp_max)$validation_rmse
  
  # D. Store Results (Normalized by Range)
  # NRMSE = RMSE / Range(y_test)
  
  df_pls <- data.frame(
    Rep = rep_id,
    Method = "HybridPLS",
    Component = 1:n_comp_max,
    NRMSE = rmse_pls / y_range
  )
  
  df_pcr <- data.frame(
    Rep = rep_id,
    Method = "HybridPCR",
    Component = 1:n_comp_max,
    NRMSE = rmse_pcr / y_range
  )
  
  results_list[[rep_id]] <- rbind(df_pls, df_pcr)
  setTxtProgressBar(pb, rep_id)
}
close(pb)

# ------------------------------------------------------------------------------
# 4. AGGREGATE AND REPORT
# ------------------------------------------------------------------------------
final_results <- do.call(rbind, results_list)

summary_stats <- final_results %>%
  group_by(Method, Component) %>%
  summarise(
    Mean_NRMSE = mean(NRMSE),
    SE_NRMSE   = sd(NRMSE) / sqrt(n()), # Standard Error
    .groups    = "drop"
  ) %>%
  arrange(Method, Component)

print(as.data.frame(summary_stats))

# Optional: Plot
library(ggplot2)
ggplot(summary_stats, aes(x = Component, y = Mean_NRMSE, color = Method)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = Mean_NRMSE - SE_NRMSE, ymax = Mean_NRMSE + SE_NRMSE), width = 0.2) +
  theme_minimal() +
  labs(
    title = "Kidney Data Analysis: HybridPLS vs HybridPCR",
    y = "Normalized RMSE (Mean +/- SE)",
    x = "Number of Components"
  )
```

```{r}
# ==============================================================================
# kidney_analysis_subsample.R
# Compare HybridPLS vs HybridPCR on Real Kidney Data
# Setting: Subsample 40% of data per rep (Low Sample Size Evaluation)
# ==============================================================================

library(fda)
library(dplyr)
library(tidyr)
library(FSHybridPLS) 

# ------------------------------------------------------------------------------
# 1. HELPER FUNCTION: SUBSET HYBRID OBJECT
# ------------------------------------------------------------------------------
# We need this to robustly slice the S3 object 'W'
subset_hybrid <- function(W, idx) {
  # 1. Subset Scalar Z
  Z_sub <- W$Z[idx, , drop=FALSE]
  
  # 2. Subset Functional Objects (fd objects support [idx] subsetting)
  fd_list_sub <- lapply(W$functional_list, function(fd_obj) fd_obj[idx])
  
  # 3. Reconstruct Object
  predictor_hybrid(Z_sub, fd_list_sub, W$eval_point)
}

# ------------------------------------------------------------------------------
# 2. LOAD DATA
# ------------------------------------------------------------------------------
raw_df <- read.csv("C:/Users/jongmin/Documents/GitHub/fsPLS/data/renogram_data.csv")
kidney_data <- load_and_preprocess_kidney_data(raw_df, n_basis = 20)

W_full <- kidney_data$W
y_full <- kidney_data$y

# ------------------------------------------------------------------------------
# 3. SETUP SIMULATION
# ------------------------------------------------------------------------------
n_replications <- 10
n_comp_max     <- 20  # Reduced max components for smaller sample size
lambda_vec     <- c(0.2, 0.2) # Unregularized for fair comparison, or tweak as needed

# Define Subsampling Ratio
subsample_ratio <- 0.40  # Use only 40% of available patients per rep

results_list <- list()
message(sprintf("=== Starting Analysis: %d Replications (Using %.0f%% of Data) ===", 
                n_replications, subsample_ratio * 100))
pb <- txtProgressBar(min = 0, max = n_replications, style = 3)

# ------------------------------------------------------------------------------
# 4. MAIN LOOP
# ------------------------------------------------------------------------------
n_total <- length(y_full)
n_sub   <- floor(n_total * subsample_ratio)

for (rep_id in 1:n_replications) {
  set.seed(rep_id * 123) 
  
  # A. Random Subsampling (The "40% Data" Step)
  # Select random indices from the full dataset
  idx_sub <- sample(1:n_total, n_sub, replace = FALSE)
  
  # Create the "Working Dataset" for this replication
  W_working <- subset_hybrid(W_full, idx_sub)
  y_working <- y_full[idx_sub]
  
  # B. Split and Normalize (Within the subsample)
  # We split the 40% chunk into 70% Train / 30% Test
  processed <- split_and_normalize.all(W_working, y_working, train_ratio = 0.7)
  
  W_train <- processed$predictor_train
  W_test  <- processed$predictor_test
  y_train <- processed$response_train
  y_test  <- processed$response_test
  
  # Calculate Range for NRMSE normalization
  y_range <- max(y_test) - min(y_test)
  if(y_range == 0) y_range <- 1
  
  # C. Hybrid PLS
  pls_fit <- fit.hybridPLS(
    W_train, y_train, 
    n_iter = n_comp_max, 
    lambda = lambda_vec,
    validation_data = list(W_test = W_test, y_test = y_test)
  )
  rmse_pls <- pls_fit$validation_rmse
  
  # D. Hybrid PCR
  rmse_pcr <- fit_hybrid_pcr_iterative(W_train, y_train, W_test, y_test, n_comp_max)$validation_rmse
  
  # E. Store Results
  df_pls <- data.frame(
    Rep = rep_id, Method = "HybridPLS", Component = 1:n_comp_max, 
    NRMSE = rmse_pls / y_range
  )
  
  df_pcr <- data.frame(
    Rep = rep_id, Method = "HybridPCR", Component = 1:n_comp_max, 
    NRMSE = rmse_pcr / y_range
  )
  
  results_list[[rep_id]] <- rbind(df_pls, df_pcr)
  setTxtProgressBar(pb, rep_id)
}
close(pb)

# ------------------------------------------------------------------------------
# 5. AGGREGATE AND REPORT
# ------------------------------------------------------------------------------
final_results <- do.call(rbind, results_list)

summary_stats <- final_results %>%
  group_by(Method, Component) %>%
  summarise(
    Mean_NRMSE = mean(NRMSE),
    SE_NRMSE   = sd(NRMSE) / sqrt(n()),
    .groups    = "drop"
  ) %>%
  arrange(Method, Component)

print(as.data.frame(summary_stats))
```
```{r}
# ==============================================================================
# kidney_pfr.R
# Compare HybridPLS vs HybridPCR vs PFR (Penalized Functional Regression)
# ==============================================================================

library(fda)
library(dplyr)
library(tidyr)
library(DBI)
library(RSQLite)
library(FSHybridPLS)
library(refund) # Required for pfr()

 

# ------------------------------------------------------------------------------
# 1. HELPER FUNCTIONS
# ------------------------------------------------------------------------------
subset_hybrid <- function(W, idx) {
  Z_sub <- W$Z[idx, , drop=FALSE]
  fd_list_sub <- lapply(W$functional_list, function(fd_obj) fd_obj[idx])
  predictor_hybrid(Z_sub, fd_list_sub, W$eval_point)
}

 
# --- PFR Wrapper (FIXED) ---
fit_pfr_legacy <- function(W_train, y_train, W_test, y_test) {
  eval_pts <- W_train$eval_point
  
  # 1. Prepare Scalar Data Frame (Unrolled)
  # Instead of putting matrix Z into df$Z, we make columns Z1, Z2...
  # This prevents the "length != nobs" error in pfr/gam.
  Z_train_df <- as.data.frame(W_train$Z)
  colnames(Z_train_df) <- paste0("Z", 1:ncol(Z_train_df))
  
  Z_test_df  <- as.data.frame(W_test$Z)
  colnames(Z_test_df) <- paste0("Z", 1:ncol(Z_test_df))
  
  # 2. Combine with Response and Functional Matrices
  # Note: PFR handles matrix columns for functional terms fine, just not scalar linear terms usually.
  df_train <- cbind(response = y_train, Z_train_df)
  df_train$F1 <- t(eval.fd(eval_pts, W_train$functional_list[[1]])) 
  df_train$F2 <- t(eval.fd(eval_pts, W_train$functional_list[[2]]))
  
  df_test <- cbind(response = y_test, Z_test_df) # placeholder response
  df_test$F1 <- t(eval.fd(eval_pts, W_test$functional_list[[1]]))
  df_test$F2 <- t(eval.fd(eval_pts, W_test$functional_list[[2]]))
  
  # 3. Construct Formula String
  # Create string "Z1 + Z2 + ..."
  scalar_formula_part <- paste(colnames(Z_train_df), collapse = " + ")
  
  f_str <- paste0(
    "response ~ ", scalar_formula_part, " + ",
    "lf(F1, argvals = eval_pts, presmooth = 'fpca.sc') + ",
    "lf(F2, argvals = eval_pts, presmooth = 'fpca.sc' )"
  )
  
  # 4. Fit and Predict
  tryCatch({
    # We must pass eval_pts explicitly or ensure it's in the environment. 
    # Passing it in the data list is safer if wrapped in a list, but data.frame prevents that.
    # pfr() looks in environment for argvals.
    fit <- pfr(as.formula(f_str), data = df_train, method="GCV.Cp", gamma=1.2)
    
    pred <- predict(fit, newdata = df_test)
    rmse <- sqrt(mean((y_test - pred)^2))
    return(rmse)
  }, error = function(e) {
    message("PFR Error: ", e$message)
    return(NA)
  })
}
 
#, presmooth = 'fpca.ssvd', presmooth.opts = list(nbasis = 20)
# ------------------------------------------------------------------------------
# 2. LOAD DATA
# ------------------------------------------------------------------------------
 

raw_df <- read.csv("C:/Users/jongmin/Documents/GitHub/fsPLS/data/renogram_data.csv")
kidney_data <- load_and_preprocess_kidney_data(raw_df, n_basis = 20)
W_full <- kidney_data$W
y_full <- kidney_data$y

# ------------------------------------------------------------------------------
# 3. RUN SIMULATION
# ------------------------------------------------------------------------------
n_replications  <- 100
 subsample_ratio <- 0.40

results_list <- list()
n_total <- length(y_full)
n_sub   <- floor(n_total * subsample_ratio)

for (rep_id in 1:n_replications) {
  set.seed(rep_id * 123) 
  
  idx_sub <- sample(1:n_total, n_sub, replace = FALSE)
  W_working <- subset_hybrid(W_full, idx_sub)
  y_working <- y_full[idx_sub]
  
  processed <- split_and_normalize.all(W_working, y_working, train_ratio = 0.7)
  W_train <- processed$predictor_train; W_test <- processed$predictor_test
  y_train <- processed$response_train;  y_test <- processed$response_test
  
  y_range <- max(y_test) - min(y_test); if(y_range == 0) y_range <- 1
  
 
  
  # --- 3. PFR (Refund) ---
  rmse_pfr_single <- fit_pfr_legacy(W_train, y_train, W_test, y_test)
  
 
  
  # PFR (Replicated for plotting convenience)
  # PFR doesn't have "Components" in the same way, but we fill the DF so it appears as a baseline
  df_pfr <- data.frame(Rep = rep_id, Method = "PFR",  
                         NRMSE = rmse_pfr_single / y_range)
  
  results_list[[rep_id]] <- df_pfr
}

final_df <- do.call(rbind, results_list)
print(final_df)

summary_stats <- final_df %>%
  summarise(
    Mean_NRMSE = mean(NRMSE, na.rm = TRUE),
    SE_NRMSE   = sd(NRMSE, na.rm = TRUE) / sqrt(n()),
    .groups    = "drop"
  )

print(as.data.frame(summary_stats))
```


```{r}
# Calculate Mean and Standard Error for each Method/Component
summary_stats <- final_df %>%
  summarise(
    Mean_NRMSE = mean(NRMSE, na.rm = TRUE),
    SE_NRMSE   = sd(NRMSE, na.rm = TRUE) / sqrt(n()),
    .groups    = "drop"
  )

print(as.data.frame(summary_stats))
```

```{r}
library(ggplot2)
library(dplyr)

# 1. Load Data
df <- read.csv("C:/Users/jongmin/Documents/GitHub/fsPLS/simul_new/simul_pred/cv_results_smallscalar.csv")

# --- DATA PREP ---
# 1. Handle potential column name variations (Scaled_RM vs Scaled_RMSE)
if("Scaled_RM" %in% names(df)) {
  names(df)[names(df) == "Scaled_RM"] <- "NRMSE"
} else if ("Scaled_RMSE" %in% names(df)) {
  names(df)[names(df) == "Scaled_RMSE"] <- "NRMSE"
}

# 2. Ensure Numeric NRMSE
df$NRMSE <- as.numeric(as.character(df$NRMSE)) 

# 3. Rename Methods to Publication Style
df$Method[df$Method == "HybridPLS"] <- "Hybrid PLS"
df$Method[df$Method == "HybridPCR"] <- "PCR"

# 4. Reorder Factor (Hybrid PLS first)
df$Method <- factor(df$Method, levels = c("Hybrid PLS", "PCR"))

# 2. Draw Plot
ggplot(df, aes(x = factor(Component), y = NRMSE)) +
  
  # --- Boxplots ---
  geom_boxplot(aes(fill = Method), outlier.shape = NA, width = 0.5, size = 0.3, fatten = 1) +
  
  # --- Scales ---
  scale_fill_manual(
    name = NULL,
    values = c("Hybrid PLS" = "white", "PCR" = "gray70"),
    breaks = c("Hybrid PLS", "PCR")
  ) +
  
  # --- Cartesian Zoom ---
  # Adjust ylim if your data range is different, 0.75 fits the previous snippets well
  coord_cartesian(ylim = c(0, 0.97)) +
  
  # --- Labels ---
  labs(x = "Number of Components", y = "Scaled RMSE") +
  
  theme_bw() + 
  theme(
    text = element_text(family = "serif", size = 12),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "gray70", size = 0.25, linetype = "dashed"),
    axis.ticks = element_line(color = "black", size = 0.3),
    axis.text = element_text(color = "black", size = 11),
    axis.title = element_text(color = "black", size = 12),
    
    # Legend Settings (Top Left)
    legend.position = c(0.75, 0.95), 
    legend.justification = c("left", "top"),
    legend.background = element_rect(fill = "white", color = "black", size = 0.2),
    legend.key = element_blank(),
    legend.key.width = unit(0.5, "cm"), 
    legend.key.height = unit(0.5, "cm"), 
    legend.text = element_text(size = 10),
    legend.margin = margin(t = 4, r = 4, b = 4, l = 4),
    
    plot.title = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, size = 0.5)
  )

# Save
ggsave("cv_results_smallscalar.pdf", 
       width = 7, # Slightly wider to accommodate side legend
       height = 4, 
       units = "in", 
       device = cairo_pdf)
```





```{r}
library(ggplot2)
library(dplyr)

# 1. Load Data
# Make sure the path matches your new file location
df <- read.csv("C:/Users/jongmin/Documents/GitHub/fsPLS/simul_new/simul_pred/results_orthogonal_robust.csv")

# --- DATA PREP: Handle column names from your screenshot ---
# If the CSV has "Scaled_RMSE" or "Scaled_RM", rename it to "NRMSE" for consistency
if("Scaled_RMSE" %in% names(df)) {
  names(df)[names(df) == "Scaled_RMSE"] <- "NRMSE"
} else if ("Scaled_RM" %in% names(df)) {
  names(df)[names(df) == "Scaled_RM"] <- "NRMSE"
}

# Ensure NRMSE is numeric
df$NRMSE <- as.numeric(as.character(df$NRMSE)) 

# --- RENAME METHODS & REORDER ---
# 1. Rename to match publication style
df$Method[df$Method == "HybridPLS"] <- "Hybrid PLS"
df$Method[df$Method == "HybridPCR"] <- "PCR"

# 2. Reorder Factor (Hybrid PLS first = Left)
df$Method <- factor(df$Method, levels = c("Hybrid PLS", "PCR"))

# 3. Draw Plot (No PFR)
ggplot(df, aes(x = factor(Component), y = NRMSE)) +
  
  # --- Boxplots ---
  geom_boxplot(aes(fill = Method), outlier.shape = NA, width = 0.5, size = 0.3, fatten = 1) +
  
  # --- Scales ---
  scale_fill_manual(
    name = NULL,
    values = c("Hybrid PLS" = "white", "PCR" = "gray70"),
    breaks = c("Hybrid PLS", "PCR")
  ) +
  
  # --- Cartesian Zoom ---
  coord_cartesian(ylim = c(0, 1)) +
  
  # --- Labels ---
  labs(x = "Number of Components", y = "Scaled RMSE") +
  
  # --- Theme (Identical to previous) ---
  theme_bw() + 
  theme(
    text = element_text(family = "serif", size = 12),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "gray70", size = 0.25, linetype = "dashed"),
    axis.ticks = element_line(color = "black", size = 0.3),
    axis.text = element_text(color = "black", size = 11),
    axis.title = element_text(color = "black", size = 12),
    
    # Legend Settings (Top Left)
    legend.position = c(0.75, 0.95), 
    legend.justification = c("left", "top"),
    legend.background = element_rect(fill = "white", color = "black", size = 0.2),
    legend.key = element_blank(),
    legend.key.width = unit(0.5, "cm"), 
    legend.key.height = unit(0.5, "cm"), 
    legend.text = element_text(size = 10),
    legend.margin = margin(t = 4, r = 4, b = 4, l = 4),
    
    plot.title = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, size = 0.5)
  )

# Save
ggsave("results_orthogonal_robust.pdf", 
       width = 6, height = 4, units = "in", device = cairo_pdf)
```
```{r}
library(ggplot2)
library(dplyr)
library(grid) 

# 1. Load Data
df <- read.csv("C:/Users/jongmin/Documents/GitHub/fsPLS/simul_new/simul_pred/results_fixed.csv")
df$NRMSE <- as.numeric(as.character(df$NRMSE)) 

# --- CHANGE 1: Rename Method strings ---
df$Method[df$Method == "HybridPLS"] <- "Hybrid PLS"
df$Method[df$Method == "HybridPCR"] <- "PCR"

# --- CHANGE 2: Reorder Factor (Hybrid PLS first = Left) ---
df$Method <- factor(df$Method, levels = c("Hybrid PLS", "PCR"))

# 2. Define PFR Stats (Assuming summary_stats exists)
pfr_mean <- summary_stats$Mean_NRMSE 
pfr_sd   <- summary_stats$SE_NRMSE 

# 3. DEFINE CUSTOM LEGEND KEY
draw_key_pfr <- function(data, params, size) {
  if (data$fill == "gray50") {
    grobTree(
      rectGrob(width = 0.74, height = 0.75,
               gp = gpar(fill = data$fill, col = NA, alpha = 0.2)),
      segmentsGrob(x0 = 0.135, x1 = 0.92, y0 = 0.5, y1 = 0.5,
                   gp = gpar(col = "black", lty = "dashed", lwd = 1))
    )
  } else {
    draw_key_boxplot(data, params, size)
  }
}

# 4. Create Dummy Data
dummy_pfr <- data.frame(Component = 1, NRMSE = 0, Method = "PFR")

# 5. Draw Plot
ggplot(df, aes(x = factor(Component), y = NRMSE)) +
  
  # --- PFR Background ---
  annotate("rect", xmin = -Inf, xmax = Inf, 
           ymin = pfr_mean - pfr_sd, ymax = pfr_mean + pfr_sd,
           fill = "gray50", alpha = 0.2) +
  geom_hline(yintercept = pfr_mean, linetype = "dashed", color = "black", size = 0.4) +
  
  # --- Boxplots ---
  geom_boxplot(aes(fill = Method), outlier.shape = NA, width = 0.5, size = 0.3, fatten = 1) +
  
  # --- Dummy Layer ---
  geom_boxplot(data = dummy_pfr, aes(fill = Method), alpha = 0, color = NA, 
               key_glyph = draw_key_pfr) + 
  
  # --- CHANGE 3: Update Names in Scale ---
  scale_fill_manual(
    name = NULL,
    values = c("Hybrid PLS" = "white", "PCR" = "gray70", "PFR" = "gray50"),
    breaks = c("Hybrid PLS", "PCR", "PFR"),
    drop = FALSE
  ) +
  
  coord_cartesian(ylim = c(0, 0.75)) +
  
  labs(x = "Number of Components", y = "Scaled RMSE") +
  
  theme_bw() + 
  theme(
    text = element_text(family = "serif", size = 12),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "gray70", size = 0.25, linetype = "dashed"),
    axis.ticks = element_line(color = "black", size = 0.3),
    axis.text = element_text(color = "black", size = 11),
    axis.title = element_text(color = "black", size = 12),
    
    legend.position = c(0.05, 0.95), 
    legend.justification = c("left", "top"),
    legend.background = element_rect(fill = "white", color = "black", size = 0.2),
    legend.key = element_blank(),
    legend.key.width = unit(0.8, "cm"), 
    legend.key.height = unit(0.5, "cm"), 
    legend.text = element_text(size = 10),
    legend.margin = margin(t = 4, r = 4, b = 4, l = 4),
    
    plot.title = element_blank(),
    panel.border = element_rect(colour = "black", fill = NA, size = 0.5)
  ) +
  
  guides(fill = guide_legend(
    override.aes = list(
      fill = c("white", "gray70", "gray50"),
      alpha = c(1, 1, 1),
      color = c("black", "black", NA) 
    )
  ))

# Save
ggsave("kidney_simulation_results.pdf", 
       width = 6, height = 4, units = "in", device = cairo_pdf)
```

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```

