---
title: "Creating the ``r params$package_name`` R package"
author: "Your Name"
date: "The Date"
knit: litr::render
output: litr::litr_html_document
params:
  package_name: "FSHybridPLS" # <-- change this to your package name
  package_parent_dir: "." # <-- relative to this file's location
---

<!-- This Rmd file contains all the code needed to define an R package.  Press "Knit" in RStudio or more generally run `litr::render("name-of-this-file.Rmd")` to generate the R package.  Remember that when you want to modify anything about the R package, you should modify this document rather than the package that is outputted.
-->

## Package setup

We start by specifying the information needed in the DESCRIPTION file of the R package.

```{r package-setup, message=FALSE, results='hide'}
usethis::create_package(
  path = ".",
  fields = list(
    Package = params$package_name,
    Version = "0.0.0.9000",
    Title = "A Package That Says Hello",
    Description = "This package says hello.  But its actual purpose is to show how an R package can be completely coded in a single R markdown file.",
    Imports = "fda (>= 6.1.3), Matrix",
    `Authors@R` = person(
      given = "First",
      family = "Last",
      email = "you@gmail.com",
      role = c("aut", "cre")
      )
  )
)
usethis::use_mit_license(copyright_holder = "F. Last")
```


# helper functions

### is_same_basis
```{r}
#' @export
is_same_basis <- function(input, other) {
  all(
    length(input$functional_list) == length(other$functional_list),
    all(
      mapply(function(fd1, fd2) fda::is.eqbasis(fd1$basis, fd2$basis),
             input$functional_list, other$functional_list)
    )
  )
}
```


### compute_gram_matrix
```{r}
#' Compute the Gram matrix for a basis object
#'
#' @param basis A basis object of class `basisfd` from the `fda` package
#' @return A square matrix where entry (i,j) is the inner product of basis functions i and j
#' @export
compute_gram_matrix <- function(basis) {
  stopifnot(inherits(basis, "basisfd"))
  inprod(basis, basis)  # computes ∫ b_i(t) b_j(t) dt
}

```





```{r}
#' Check if all matrices in a list are identical (with tolerance for numeric comparison)
#'
#' This function iterates through a list of matrices and checks if all subsequent
#' matrices are numerically identical to the first matrix in the list.
#' It uses `all.equal` for robust comparison of numeric matrices, which accounts
#' for small floating-point differences.
#'
#' @param gram_list A list of matrices.
#' @param tolerance A numeric tolerance for comparing floating-point numbers.
#'                  Defaults to `sqrt(.Machine$double.eps)`.
#' @return A logical value: `TRUE` if all matrices are identical, `FALSE` otherwise.
#' @export
are_all_gram_matrices_identical <- function(gram_list, tolerance = sqrt(.Machine$double.eps)) {
  # Handle edge cases: empty list or list with a single matrix
  if (length(gram_list) == 0) {
    message("The input list is empty. Returning TRUE as there are no differences.")
    return(TRUE)
  }
  if (length(gram_list) == 1) {
    message("The input list contains only one matrix. Returning TRUE.")
    return(TRUE)
  }

  # Get the first matrix as the reference
  first_matrix <- gram_list[[1]]

  # Ensure the first element is a matrix
  if (!is.matrix(first_matrix)) {
    stop("The first element of 'gram_list' is not a matrix.")
  }

  # Iterate from the second matrix onwards and compare with the first
  for (i in 2:length(gram_list)) {
    current_matrix <- gram_list[[i]]

    # Ensure current element is a matrix
    if (!is.matrix(current_matrix)) {
      stop(paste("Element", i, "of 'gram_list' is not a matrix."))
    }

    # Compare the current matrix with the first matrix
    # all.equal returns TRUE if identical, or a character string describing differences
    # We convert the result to a logical TRUE/FALSE
    if (!isTRUE(all.equal(first_matrix, current_matrix, tolerance = tolerance))) {
      # If not equal, return FALSE immediately
      message(paste("Matrices at index 1 and", i, "are not identical."))
      return(FALSE)
    }
  }

  # If the loop completes, all matrices are identical
  return(TRUE)
}
```


 

## helpers for functional data objects

### rep_fd

```{r}
#' Replicate a list of single-sample fd objects multiple times
#'
#' This function broadcasts a list of single-sample functional predictors
#' by replicating their coefficient columns.
#'
#' @param fd_list A list of `fd` objects, each representing a single sample.
#' @param n The number of replications desired.
#'
#' @return A list of `fd` objects, each with `n` replications.
#' @export
rep_fd <- function(fd_list, n) {
  if (!is.list(fd_list) || any(!vapply(fd_list, inherits, logical(1), "fd"))) {
    stop("Input must be a list of 'fd' objects.")
  }

  lapply(fd_list, function(fd_obj) {
    coef_mat <- fd_obj$coefs
    if (is.null(dim(coef_mat)) || ncol(coef_mat) != 1) {
      stop("Each fd object in the list must have one column of coefficients.")
    }

    new_coefs <- matrix(rep(coef_mat, n), nrow = nrow(coef_mat), ncol = n)
    fd(coef = new_coefs, basisobj = fd_obj$basis)
  })
}
```

| Case | Description                            | Input Type               | Expected Output                        | Notes                                            |
|------|----------------------------------------|--------------------------|----------------------------------------|--------------------------------------------------|
| 1    | Correct replication                    | List of single-sample fd | List of `fd` objects with `n` columns | All columns identical to original coefficients   |
| 2    | Each fd has one sample                 | List of fd               | No error                               | Verifies structure and content                  |
| 3    | One fd has multiple samples            | fd with >1 columns       | Error                                   | Should raise informative error                  |
| 4    | Non-fd object in list                  | List of non-`fd` items   | Error                                   | Validates class-checking robustness             |


 

# Class predictor_hybrid
- Let $\{X^{(k)}\}_{k=1, \ldots, K}$ be a collection of random functions defined on unit hypercube domains $\mathcal{T}_k := [0,1]^{d_k}$ ($d_k \in \mathbb{N})$; i.e., $X^{(k)}: \mathcal{T}_k \rightarrow \mathbb{R}$.

- Assume that each $X^{(k)}$ is in $L^2(\mathcal{T}_k)$, a Hilbert space of square integrable functions with respect to Lebesgue measure $dt_k$ on $\mathcal{T}_k$. 

- Write  $X=(X^{(1)}, \ldots, X^{(K)})$ as a multivariate functional object that belongs to $\mathcal{F} = L^2(\mathcal{T}_1) \times \cdots \times L^2(\mathcal{T}_K)$---a cartesian product of individual $L^2(\mathcal{T}_k)$ spaces. 

- We can also express the functional object $X$ evaluated on the multi-dimensional argument $\mathbf{t} = (t_1, \ldots, t_k)^\top \in \mathcal{T} = \mathcal{T}_1 \times \cdots \times \mathcal{T}_K$ as a $K$-dimensional vector $X(\mathbf{t})=(X^{(1)}(t_1), \ldots, X^{(K)}(t_K))^\top$. 

- **Our strategy is to formulate a hybrid random object**, $\mathbf{W} = (X, \mathbf{Z})$, which combines $X$ and $p$-dimensional scalar covariate $\mathbf{Z}$ into an ordered pair belonging to $\mathcal{H} = \mathcal{F} \times \mathbb{R}^p$.  

- An alternative notation for the hybrid object can be obtained by evaluating its functional part on $\mathbf{t}$ and expressing it as a $(K+p)$-dimensional vector: $\mathbf{W}[\mathbf{t}] = (X(\mathbf{t}), \mathbf{Z})^\top$, with $X(\mathbf{t}) = (X^{(1)}(t_1), \ldots, X^{(K)}(t_K))^\top \in \mathbb{R}^K$ and $\mathbf{Z} = (Z_1, \ldots, Z_p)^\top  \in \mathbb{R}^p$.

- The hybrid object forms a Hilbert space, characterized by well-defined addition, scalar multiplication, and inner product operations. 

- We define an S3 object `predictor_hybrid` that represent this structure, and implement Hilbert space operations for this class, and our algorithm is built on these foundational operations.

The class definition is a simple list that contains a matrix and a collection of \code{fd} objects from the \code{fda} package, along with the necessary metadata.
- gram matrices are saved as a list.
```{r classdef:predictor_hybrid}

#' Create a predictor_hybrid object (S3 version, automatic basis size and Gram matrix)
#'
#' Constructs a hybrid predictor object that stores both scalar and functional predictors,
#' and automatically computes Gram matrices (inner products of basis functions).
#'
#' @param Z A numeric matrix of dimension \code{n_sample × n_scalar} representing scalar predictors.
#' @param functional_list A list of functional predictors (e.g., \code{fd} objects from the \code{fda} package).
#'
#' @return An object of class \code{predictor_hybrid}, containing scalar and functional data with Gram matrices.
#' @export
predictor_hybrid <- function(Z, functional_list) {
  stopifnot(is.matrix(Z), is.numeric(Z))
  stopifnot(is.list(functional_list))

  n_sample <- nrow(Z)
  n_scalar <- ncol(Z)
  n_functional <- length(functional_list)

  gram_list <- gram_deriv2_list <- vector("list", n_functional)
  n_basis_list <- numeric(n_functional)

  for (i in seq_len(n_functional)) {
    fd_i <- functional_list[[i]]
    stopifnot(inherits(fd_i, "fd"))

    n_fd_sample <- ncol(coef(fd_i))
    if (n_fd_sample != n_sample) {
      stop(sprintf("Functional predictor %d has %d replicates, but Z has %d rows.",
                   i, n_fd_sample, n_sample))
    }

    basis_i <- fd_i$basis
    gram_list[[i]] <- compute_gram_matrix(basis_i)
    gram_deriv2_list[[i]] <- fda::getbasispenalty(basis_i) # New  
      
 

    n_basis_list[i] <- basis_i$nbasis
  }

  structure(
    list(
      Z = Z,
      functional_list = functional_list,
      gram_list = gram_list,
      gram_deriv2_list = gram_deriv2_list, # Store the new list
      n_basis_list = n_basis_list,
      n_sample = n_sample,
      n_functional = n_functional,
      n_scalar = n_scalar
    ),
    class = "predictor_hybrid"
  )
}
```

 


### predictor_hybrid_from_coef
An alternative constructor for a predictor_hybrid object based on a flat numeric coefficient vector. It reconstructs both the functional and scalar components for a single-sample hybrid predictor object. The function reads the basis information from the template `format` object.

- Inputs
  - **`format`**: A `predictor_hybrid` object containing:
    - `functional_list`: List of `fd` objects with basis info
    - `gram_list`: List of Gram matrices
    - `n_scalar`: Number of scalar predictors
  - **`coef`**: Numeric vector of length `K * M + d`, where:
    - `K` = number of functional predictors  
    - `M` = number of basis functions (assumed same across predictors)  
    - `d` = number of scalar predictors

- Output
  - A `predictor_hybrid` object with:
    - Functional predictors rebuilt from the first `K * M` coefficients
    - Scalar predictor `Z` from the remaining `d` coefficients
    - `n_sample` set to 1

```{r}
#' Construct a Single-Sample Predictor Hybrid Object from Coefficients
#'
#' Reconstructs a \code{predictor_hybrid} object representing one observation, using a numeric
#' coefficient vector. This alternative constructor maps the coefficients back into their functional
#' and scalar predictor representations based on the structure of a template \code{predictor_hybrid} object.
#'
#' @param format A \code{predictor_hybrid} object that provides the structure and basis information.
#' @param coef A numeric vector containing coefficients for both functional and scalar predictors.
#'
#' @return A \code{predictor_hybrid} object with updated \code{functional_list}, \code{Z}, and \code{n_sample = 1}.
#'
#'
#' @export
predictor_hybrid_from_coef <- function(format, coef){
  M <- dim(format$gram_list[[1]])[2] # number of basis functions
  K <- length(format$functional_list) # number of functional predictors
  for (ii in 1:K){
    format$functional_list[[ii]] <- fd(
      coef = as.matrix(coef[((ii-1)*M + 1):(ii*M)]),
      basisobj = format$functional_list[[ii]]$basis
    )
  }
  format$Z <- t(as.matrix(coef[(K * M + 1):length(coef)]))
  format$n_sample <- 1
  return(format)
}
```
 
## Basic arithmetic

### add.predictor_hybrid
Performs element-wise addition of two `predictor_hybrid` objects.  Functional predictors are combined using `plus.fd()` and `times.fd()` from the `fda` package.

- Usage

```r
add.predictor_hybrid(input, other, alpha = 1)
```

- Arguments:

  - `input`: A `predictor_hybrid` object.
  - `other`: Another `predictor_hybrid` object to be added.
  - `alpha`: A scalar multiplier applied to `other` before addition (default is 1).

- Value:
Returns a new `predictor_hybrid` object representing the result of the addition.

- Details: 
  - This function assumes both objects have the same number and structure of functional and scalar predictors.  
  - Functional parts are scaled by `alpha` using `times.fd()` and then summed using `plus.fd()`.  
  - Scalar predictors are added using standard matrix addition.

```{r}
add.predictor_hybrid <- function(xi_1, xi_2, alpha = 1) {
  # Safe access to is.eqbasis()
  is_eqbasis <- getFromNamespace("is.eqbasis", "fda")

  for (i in seq_len(xi_1$n_functional)) {
    if (!is_eqbasis(xi_1$functional_list[[i]]$basis, xi_2$functional_list[[i]]$basis)) {
      stop("Functional predictors must have the same basis.")
    }
  }
  
   # Type checks
  if (!inherits(xi_1, "predictor_hybrid") || !inherits(xi_2, "predictor_hybrid")) {
    stop("Both inputs must be of class 'predictor_hybrid'.")
  }

  # Structural checks
  if (xi_1$n_functional != xi_2$n_functional) {
    stop("Mismatch in number of functional predictors.")
  }
  if (xi_1$n_scalar != xi_2$n_scalar) {
    stop("Mismatch in number of scalar predictors.")
  }

  # Swap so that broadcasting always applies to xi_2
  if (xi_1$n_sample == 1 && xi_2$n_sample > 1) {
    tmp <- xi_1
    xi_1 <- xi_2
    xi_2 <- tmp
  }

  n1 <- xi_1$n_sample
  n2 <- xi_2$n_sample

  if (!(n1 == n2 || n2 == 1)) {
    stop("Sample sizes are incompatible for broadcasting.")
  }

  # Prepare components
  f1 <- xi_1$functional_list
  f2 <- xi_2$functional_list
  Z1 <- xi_1$Z
  Z2 <- xi_2$Z

  # Replicate fd and Z if needed
  if (n2 == 1) {
    f2 <- rep_fd(f2, n1)
    Z2 <- matrix(rep(c(Z2), n1), nrow = n1, byrow = TRUE)
  }

  # Combine functional predictors
  new_functional_list <- Map(
    function(f1, f2) plus.fd(f1, times.fd(alpha, f2)),
    f1,
    f2
  )
 

  # Compute scalar inner products
  inprod_scalar <- rowSums(Z1 * Z2) 
  # Combine scalar predictors
  new_Z <- Z1 + alpha * Z2


  # Construct new object
  predictor_hybrid(Z = new_Z, functional_list = new_functional_list)
}
```

 
### subtr.predictor_hybrid
Performs element-wise subtraction of two `predictor_hybrid` objects.   Internally uses `add.predictor_hybrid(input, other, alpha = -1)` to compute the result by negating the second operand.

- Usage

```r
subtr.predictor_hybrid(input, other, alpha = 1)
```

- Arguments:

  - `input`: A `predictor_hybrid` object.
  - `other`: Another `predictor_hybrid` object to be subtracted.
  - `alpha`: A scalar multiplier applied to `other` before subtraction (default is 1).

- Value:  
Returns a new `predictor_hybrid` object representing the result of the subtraction.

- Details:  
  - This function assumes both objects have the same number and structure of functional and scalar predictors.  
  - It performs subtraction by internally calling `add.predictor_hybrid()` with `-alpha`.  
  - Functional parts are scaled using `times.fd()` and subtracted via `plus.fd()` with a negated factor.  
  - Scalar predictors are subtracted using standard matrix arithmetic.

```{r}
#' Subtract two predictor_hybrid objects
#'
#' Performs element-wise subtraction of two `predictor_hybrid` objects.
#' Internally uses `add.predictor_hybrid(input, other, alpha = -1)`.
#'
#' @param input A `predictor_hybrid` object.
#' @param other Another `predictor_hybrid` object to subtract.
#' @param alpha A scalar multiplier applied to `other` before subtraction (default is 1).
#'
#' @return A new `predictor_hybrid` object representing the result of subtraction.
#' @export
subtr.predictor_hybrid <- function(input, other, alpha = 1) {
  add.predictor_hybrid(input, other, alpha = -alpha)
}

```
We skip the unit test for this function.


### scalar_mul.predictor_hybrid
Multiplies all components of a `predictor_hybrid` object by a scalar. Functional components are scaled using `times.fd()` from the `fda` package, and scalar predictors are multiplied directly using matrix operations.

- Usage

```r
scalar_mul.predictor_hybrid(input, scalar)
```

- Arguments:

  - `input`: A `predictor_hybrid` object.
  - `scalar`: A numeric value used to scale both scalar and functional components.

- Value:  
Returns a new `predictor_hybrid` object with all components scaled by `scalar`.

- Details:  
  - Functional predictors are scaled using `times.fd(scalar, fd_obj)` for each element.  
  - Scalar predictors (the matrix `Z`) are scaled elementwise using matrix multiplication.

```{r}
#' Multiply a predictor_hybrid object by a scalar
#'
#' Performs scalar multiplication on both the scalar and functional components
#' of a `predictor_hybrid` object. Functional predictors are scaled using 
#' `times.fd()` from the `fda` package.
#'
#' @param input A `predictor_hybrid` object.
#' @param scalar A numeric value to multiply all components by.
#'
#' @return A new `predictor_hybrid` object scaled by `scalar`.
#' @export
scalar_mul.predictor_hybrid <- function(input, scalar) {
  if (!inherits(input, "predictor_hybrid")) {
    stop("Input must be of class 'predictor_hybrid'.")
  }
  if (!is.numeric(scalar) || length(scalar) != 1) {
    stop("Scalar must be a single numeric value.")
  }

  # Scale functional components
  new_functional_list <- lapply(input$functional_list, function(fd_obj) {
    times.fd(scalar, fd_obj)
  })

  # Scale scalar predictors
  new_Z <- scalar * input$Z

  # Reconstruct hybrid object
  predictor_hybrid(
    Z = new_Z,
    functional_list = new_functional_list
  )
}
```
 
### inprod.predictor_hybrid
- The inner product of $f_1=(f_1^{(1)}, \ldots,f_1^{(K)})$ and $f_2=(f_2^{(1)}, \ldots,f_2^{(K)})$ in $\mathcal{F}$ is defined as 
$$\langle f_1, f_2\rangle_\mathcal{F} =  \sum_{k=1}^K \langle f_1^{(k)}, f_2^{(k)}\rangle_{L^2} = \sum_{k=1}^K \int_{\mathcal{T}_k} f_1^{(k)}(t_k) f_2^{(k)}(t_k) dt_k,$$ with norm 
$$\Vert f_1 \Vert_\mathcal{F} = \langle f_1,f_1 \rangle_\mathcal{F}^{1/2} = \{ \sum_{k=1}^K \int_{\mathcal{T}_k} f_1^{(k)}(t_k)^2 dt_k\}^{1/2}.$$
- We define the inner product between any two hybrid objects, $\mathbf{h}_1 = (f_1, \mathbf{v}_1)$ and $\mathbf{h}_2 = (f_2, \mathbf{v}_2)$, as
$$
    \langle \mathbf{h}_1, \mathbf{h}_2\rangle_{\mathcal{H}} = \langle f_1, f_2\rangle_\mathcal{F} +  \langle  \mathbf{v_1}, \mathbf{v_2} \rangle = \sum \limits_{k=1}^K \int_{\mathcal{T}_k} f_1^{(k)}(t_k) f_2^{(k)}(t_k) dt_k + \sum \limits_{r=1}^p v_{1r}v_{2r}, 
$$
with norm $\Vert \cdot \Vert_\mathcal{H} = \langle \cdot,\cdot \rangle_\mathcal{H}^{1/2}$. 

- The method `inprod.predictor_hybrid` computes the inner product between two `predictor_hybrid` objects. It supports broadcasting when one of the inputs has a single observation.


- Usage

```r
inprod.predictor_hybrid(xi_1, xi_2)
```

- Arguments:

  - `xi_1`: A `predictor_hybrid` object.
  - `xi_2`: Another `predictor_hybrid` object. If omitted, defaults to xi_1 (computes self-inner product).

- Value:  A numeric vector of inner products (or a scalar if both inputs are single observations).

- Details:  
  - Functional components are summed using inprod() from the fda package.
  - Scalar components are handled via matrix multiplication.
  - Broadcasting is supported: if either xi_1 or xi_2 has only one sample, its values are broadcast across all rows of the other.
  - Ensures compatibility in the number of functional and scalar predictors before computing the result.
```{r}
#' Inner product between two predictor_hybrid objects (with broadcasting)
#'
#' Computes the inner product between two `predictor_hybrid` objects,
#' including both functional and scalar components. Supports broadcasting
#' when one of the inputs has a single observation.
#'
#' @param xi_1 A `predictor_hybrid` object.
#' @param xi_2 Another `predictor_hybrid` object. If missing, defaults to `xi_1`.
#'
#' @return A numeric vector of inner products, or a scalar if both inputs contain a single observation.
#' @export
inprod.predictor_hybrid <- function(xi_1, xi_2 = NULL) {
  # Handle self-inner product
  if (is.null(xi_2)) xi_2 <- xi_1 
  
  # Type checks
  if (!inherits(xi_1, "predictor_hybrid") || !inherits(xi_2, "predictor_hybrid")) {
    stop("Both inputs must be of class 'predictor_hybrid'.")
  }

  # Structural checks
  if (xi_1$n_functional != xi_2$n_functional) {
    stop("Mismatch in number of functional predictors.")
  }
  if (xi_1$n_scalar != xi_2$n_scalar) {
    stop("Mismatch in number of scalar predictors.")
  }

  # Swap so that broadcasting always applies to xi_2
  if (xi_1$n_sample == 1 && xi_2$n_sample > 1) {
    tmp <- xi_1
    xi_1 <- xi_2
    xi_2 <- tmp
  }

  n1 <- xi_1$n_sample
  n2 <- xi_2$n_sample

  if (!(n1 == n2 || n2 == 1)) {
    stop("Sample sizes are incompatible for broadcasting.")
  }

  # Prepare components
  f1 <- xi_1$functional_list
  f2 <- xi_2$functional_list
  Z1 <- xi_1$Z
  Z2 <- xi_2$Z

  # Replicate fd and Z if needed
  if (n2 == 1) {
    f2 <- rep_fd(f2, n1)
    Z2 <- matrix(rep(c(Z2), n1), nrow = n1, byrow = TRUE)
  }

  # Compute functional inner products
  inprod_functional <- vapply(seq_len(n1), function(i) {
    sum(vapply(seq_along(f1), function(j) {
      fda::inprod(f1[[j]][i], f2[[j]][i])
    }, numeric(1)))
  }, numeric(1))
 

  # Compute scalar inner products
  inprod_scalar <- rowSums(Z1 * Z2) 
 
  # Combine results
  result <- inprod_functional + inprod_scalar
  if (n1 == 1 && n2 == 1) as.numeric(result) else result
}
```

```{r}
#' Inner product between two predictor_hybrid objects (with broadcasting)
#'
#' Computes the inner product between two `predictor_hybrid` objects,
#' including both functional and scalar components. Supports broadcasting
#' when one of the inputs has a single observation.
#'
#' @param xi_1 A `predictor_hybrid` object.
#' @param xi_2 Another `predictor_hybrid` object. If missing, defaults to `xi_1`.
#'
#' @return A numeric vector of inner products, or a scalar if both inputs contain a single observation.
#' @export
inprod_pen.predictor_hybrid <- function(xi_1, xi_2 = NULL, lambda) {
  # Handle self-inner product
  if (is.null(xi_2)) xi_2 <- xi_1 
  
  # Type checks
  if (!inherits(xi_1, "predictor_hybrid") || !inherits(xi_2, "predictor_hybrid")) {
    stop("Both inputs must be of class 'predictor_hybrid'.")
  }

  # Structural checks
  if (xi_1$n_functional != xi_2$n_functional) {
    stop("Mismatch in number of functional predictors.")
  }
  if (xi_1$n_scalar != xi_2$n_scalar) {
    stop("Mismatch in number of scalar predictors.")
  }

  # Swap so that broadcasting always applies to xi_2
  if (xi_1$n_sample == 1 && xi_2$n_sample > 1) {
    tmp <- xi_1
    xi_1 <- xi_2
    xi_2 <- tmp
  }

  n1 <- xi_1$n_sample
  n2 <- xi_2$n_sample

  if (!(n1 == n2 || n2 == 1)) {
    stop("Sample sizes are incompatible for broadcasting.")
  }

  
  inprod <- inprod.predictor_hybrid(xi_1, xi_2)
  # Prepare components
  f1 <- xi_1$functional_list

  f2 <- xi_2$functional_list

  # Replicate fd and Z if needed
  if (n2 == 1) {
    f2 <- rep_fd(f2, n1)
    Z2 <- matrix(rep(c(Z2), n1), nrow = n1, byrow = TRUE)
  }
    for (j in 1:xi_1$n_functional){f1[[j]] <- lambda[j] * f1[[j]]}
  L_op <- fda::Lfd(2, bwtlist = NULL) 
  # Compute functional inner products
  inprod_deriv2 <- vapply(
    seq_len(n1), 
    function(i) {
      sum(vapply(seq_along(f1), function(j) {
      fda::inprod(
        fdobj1 = f1[[j]][i], 
        fdobj2 = f2[[j]][i], 
        Lfdobj1 = int2Lfd(2),
        Lfdobj2 = int2Lfd(2)
        )
    }, numeric(1)))
  }, numeric(1))
 

  # Combine results
  result <- inprod + inprod_deriv2
  if (n1 == 1 && n2 == 1) as.numeric(result) else result
}
```

### subset_predictor_hybrid

```{r}
#' Extract a single observation from a predictor_hybrid object
#'
#' @param W A predictor_hybrid object with multiple samples.
#' @param i Integer index of the sample to extract.
#' @return A single-sample predictor_hybrid object.
subset_predictor_hybrid <- function(W, i) {
  new_Z <- matrix(W$Z[i, ], nrow = 1)
  new_functional_list <- lapply(W$functional_list, function(fdobj) {
    fd(coef = matrix(coef(fdobj)[, i], ncol = 1), basisobj = fdobj$basis)
  })
  new_predictor <- predictor_hybrid(Z = new_Z, functional_list = new_functional_list)
  return(new_predictor)
}
```

### replace_obs_hybrid

```{r}
#' Replace a single observation in a predictor_hybrid object
#'
#' Replaces the i-th observation of a predictor_hybrid object with a new
#' single-sample predictor_hybrid object.
#'
#' @param W A predictor_hybrid object with one or more samples.
#' @param i An integer index specifying the observation to replace.
#' @param new_W A single-sample predictor_hybrid object to use for replacement.
#'
#' @return A predictor_hybrid object with the i-th observation replaced.
#' @export
replace_obs_hybrid <- function(W, i, new_W) {
  # Input validation
  if (!inherits(W, "predictor_hybrid") || !inherits(new_W, "predictor_hybrid")) {
    stop("Both W and new_W must be of class 'predictor_hybrid'.")
  }
  if (new_W$n_sample != 1) {
    stop("new_W must be a single-sample predictor_hybrid object.")
  }
  if (i < 1 || i > W$n_sample) {
    stop(paste("Index i must be between 1 and", W$n_sample))
  }
  if (W$n_scalar != new_W$n_scalar) {
    stop("Mismatch in number of scalar predictors.")
  }
  if (W$n_functional != new_W$n_functional) {
    stop("Mismatch in number of functional predictors.")
  }
  
  # Check for compatible basis objects
  is_eqbasis <- getFromNamespace("is.eqbasis", "fda")
  for (j in seq_len(W$n_functional)) {
    if (!is_eqbasis(W$functional_list[[j]]$basis, new_W$functional_list[[j]]$basis)) {
      stop("Functional predictors must have the same basis objects.")
    }
  }

  # Replace the i-th observation in the scalar matrix Z
  W$Z[i, ] <- new_W$Z[1, ]

  # Replace the i-th observation in each functional predictor
  for (j in seq_along(W$functional_list)) {
    # The coefficients are stored as a matrix, with columns corresponding to samples
    W$functional_list[[j]]$coefs[, i] <- new_W$functional_list[[j]]$coefs[, 1]
  }

  # The number of samples remains the same
  return(W)
}
```
 



# One iteration

## small functions



## penalty matrix construction
In this section, we provide an R function `get_constraint_matrix` that constructs the penalty matrix
$$J^*+\Lambda \ddot{J}^\ast,$$
as defined in Proposition 2 and used in the PLS component computation step of our proposed algorithm. We present functions  for computing $J^\ast$, $\Lambda$ and $\ddot{J}^\ast$ in sequence, and use these functions to define `get_constraint_matrix`.

### get_gram_matrix_block
Constructs a block-diagonal Gram matrix for a hybrid predictor object, defined as 
$$
    J^*=\mathrm{blkdiag}(J, I_p) \in \mathbb{R}^{(MK+p) \times (MK+p)}
$$
where
 $J = \mathrm{blkdiag}(J^{(1)}, \cdots, J^{(K)}) \in \mathbb{R}^{MK \times MK}$, and 
$$
J^{(k)} = \left[ \int_{\mathcal{T}k} b_m^{(k)}(t) , b_n^{(k)}(t) , dt \right]_{m,n=1}^M,
$$
as defined in \eqref{def:gram_basis} and \eqref{def:J_and_J_star}.

- Arguments
  - obj: A `predictor_hybrid` object containing both functional and scalar components.

- Value
  - A Matrix::bdiag sparse matrix representing the block-diagonal structure of the combined Gram matrix.

- Details: This function builds a block-diagonal matrix by:
  - Stacking the Gram matrices of each functional component (from obj$gram_list),
  - Appending an identity matrix of size equal to the number of scalar predictors (to represent unpenalized scalar covariates).
  The resulting matrix has size $(\texttt{total_dim} \times \texttt{total_dim})$, where total_dim = \sum_k M_k + p, with $M_k$ the number of basis functions for the $k$-th functional predictor and $p$ the number of scalar covariates.
  
- Usage
```r
block_gram <- get_gram_matrix_block(my_predictor)
```


**Code**
```{r}
#' Construct block-diagonal Gram matrix for hybrid predictor
#'
#' Returns a block-diagonal matrix containing the Gram  matrices for
#' each functional component and an identity matrix for the scalar part.
#'
#' @param obj A `predictor_hybrid` object.
#'
#' @return A block-diagonal matrix of size `(total_dim × total_dim)` where
#' functional and scalar components are arranged in order.
#' @export
get_gram_matrix_block <- function(obj) {
  if (!inherits(obj, "predictor_hybrid")) {
    stop("Input must be of class 'predictor_hybrid'.")
  }

  gram_blocks <- c(obj$gram_list, list(diag(obj$n_scalar)))
  Matrix::bdiag(gram_blocks)
}

```

**Unit test**
```{r}
testthat::test_that("get_gram_matrix_block constructs correct block-diagonal matrix", {
  suppressPackageStartupMessages(library(fda))
  suppressPackageStartupMessages(library(Matrix))

  # Construct hybrid predictor with two functional and three scalar predictors
  basis1 <- create.bspline.basis(c(0, 1), nbasis = 4)
  basis2 <- create.bspline.basis(c(0, 1), nbasis = 4)  # was 3; now valid

  fd1 <- fd(coef = matrix(1, 4, 2), basisobj = basis1)
  fd2 <- fd(coef = matrix(2, 4, 2), basisobj = basis2)
  Z <- matrix(rnorm(2 * 3), nrow = 2, ncol = 3)

  obj <- predictor_hybrid(Z = Z, functional_list = list(fd1, fd2))

  # Extract the block-diagonal Gram matrix
  G <- get_gram_matrix_block(obj)

  # Expected block sizes
  nb1 <- 4
  nb2 <- 4
  ns <- 3
  total_dim <- nb1 + nb2 + ns

  # Check matrix properties
  testthat::expect_equal(dim(G), c(total_dim, total_dim))

  # Check sub-block structure
  # Top-left: Gram matrix 1
  testthat::expect_equal(sum(G[1:nb1, 1:nb1]-obj$gram_list[[1]]), 0)

  # Next block: Gram matrix 2
  testthat::expect_equal(sum( G[(nb1 + 1):(nb1 + nb2), (nb1 + 1):(nb1 + nb2)]-obj$gram_list[[2]]),0 )

  # Final block: identity matrix for scalar part
  testthat::expect_equal( sum( G[(nb1 + nb2 + 1):total_dim, (nb1 + nb2 + 1):total_dim]  - diag(ns)), 0)
})

```
### get_smoothing_param_hybrid
Constructs a block-diagonal smoothing parameter matrix for use in penalized estimation involving hybrid predictors, denoted $\Lambda \in \mathbb{R}^{(MK+p) \times (MK+p)}$,  defined in \eqref{def:Lambda} as
$$
   \Lambda = \mathrm{blkdiag}(\lambda_1 I_M, \cdots, \lambda_K I_M, 0_{p \times p}) , 
$$

- Arguments
  - W: A predictor_hybrid object.
  - lambda: A numeric vector of length equal to the number of functional predictors. Each entry corresponds to a smoothing penalty weight.
- Value: A sparse block-diagonal matrix combining scaled identity matrices for the functional parts and a zero matrix for the scalar part.
- Details: This function generates a matrix used in regularized regression for functional predictors. Each functional component is penalized using a scaled identity matrix of its basis dimension. Scalar predictors are unpenalized.
- Usage:
```r
lambda_mat <- get_smoothing_param_hybrid(my_predictor, c(0.1, 0.2))
```
**code** 
```{r}
#' Construct block-diagonal smoothing parameter matrix
#'
#' Generates a block-diagonal matrix with smoothing parameters applied to each 
#' functional component. Each block is a scaled identity matrix, where the scaling 
#' factor corresponds to the regularization parameter for that functional component. 
#' The scalar components are not penalized and thus contribute a zero matrix block.
#'
#' @param W A `predictor_hybrid` object.
#' @param lambda A numeric vector of length equal to the number of functional components (`W$n_functional`), containing the smoothing parameters for each functional predictor.
#'
#' @return A block-diagonal matrix of size `(total_dim × total_dim)`, where the top-left blocks are scaled identity matrices for functional predictors and the bottom-right block is a zero matrix for scalar covariates.
#' @export
get_smoothing_param_hybrid <- function(W, lambda) {
  if (!inherits(W, "predictor_hybrid")) {
    stop("Input W must be of class 'predictor_hybrid'.")
  }

  if (length(lambda) != W$n_functional) {
    stop("Length of lambda must match the number of functional predictors.")
  }

  lambda_blocks <- lapply(seq_len(W$n_functional), function(ii) {
    nb <- W$functional_list[[ii]]$basis$nbasis
    lambda[ii] * diag(nb)
  })

  lambda_blocks[[W$n_functional + 1]] <- matrix(0, nrow = W$n_scalar, ncol = W$n_scalar)

  Matrix::bdiag(lambda_blocks)
}
```

**unit test**
```{r}
testthat::test_that("get_smoothing_param_hybrid returns correct block-diagonal structure", {
  suppressPackageStartupMessages(library(fda))
  basis1 <- create.bspline.basis(c(0, 1), nbasis = 5)
  basis2 <- create.bspline.basis(c(0, 1), nbasis = 4)

  fd1 <- fd(coef = matrix(1, 5, 3), basisobj = basis1)
  fd2 <- fd(coef = matrix(2, 4, 3), basisobj = basis2)

  Z <- matrix(1, nrow = 3, ncol = 2)

  obj <- predictor_hybrid(Z = Z, functional_list = list(fd1, fd2))
  lambda <- c(0.5, 2)

  lambda_mat <- get_smoothing_param_hybrid(obj, lambda)

  # Check overall dimension
  expected_dim <- sum(obj$n_basis_list) + obj$n_scalar
  testthat::expect_equal(dim(lambda_mat), c(expected_dim, expected_dim))

  # Check diagonal entries
  testthat::expect_equal(Matrix::diag(lambda_mat)[1:5], rep(0.5, 5))
  testthat::expect_equal(Matrix::diag(lambda_mat)[6:9], rep(2, 4))
  testthat::expect_equal(Matrix::diag(lambda_mat)[10:11], rep(0, 2))  # scalar part
})

```


### get_penalty_hybrid
Constructs a matrix defined in \label{def:J_dotdot_ast}
$$
    \ddot{J}^\ast=\mathrm{blkdiag}(\ddot{J}^{(1)}, \cdots, \ddot{J}^{(K)}, 0_{p \times p}) \in \mathbb{R}^{(MK+p) \times (MK+p)},
$$
where
 $\ddot{J}^{(k)}$ is  a $M \times M$ matrix defined in \label{def:dodot_J_k} as the gram matrix formed by the second derivativ of the basis functions
$$
\ddot{J}^{(k)} = \left[ \int_{\mathcal{T}_k} \ddot{b}^{(k)}_m(t) \ddot{b}^{(k)}_n(t) \, dt \right]_{m,n=1}^M.
$$



```{r}
testthat::test_that("get_penalty_hybrid computes correct dimensions and structure", {
  suppressPackageStartupMessages(library(fda))

  # Setup
  basis <- fda::create.bspline.basis(c(0, 1), nbasis = 5)
  n_sample <- 3
  n_scalar <- 2

  fd1 <- fda::fd(coef = matrix(1, 5, n_sample), basisobj = basis)
  fd2 <- fda::fd(coef = matrix(2, 5, n_sample), basisobj = basis)
  Z <- matrix(rnorm(n_sample * n_scalar), n_sample, n_scalar)
  obj <- predictor_hybrid(Z = Z, functional_list = list(fd1, fd2))

  # Run
  penalty <- get_penalty_hybrid(obj)

  # Expected dimensions
  total_dim <- sum(obj$n_basis_list) + obj$n_scalar
  testthat::expect_equal(dim(penalty), c(total_dim, total_dim))

  # Check block diagonal structure: bottom-right should be zero matrix
  scalar_block <- as.matrix(penalty[
    (total_dim - n_scalar + 1):total_dim,
    (total_dim - n_scalar + 1):total_dim
  ])
  testthat::expect_equal(scalar_block, matrix(0, n_scalar, n_scalar))

  # Check that the upper blocks are positive semidefinite
  eigenvalues <- eigen(as.matrix(penalty[1:(total_dim - n_scalar), 1:(total_dim - n_scalar)]))$values
  testthat::expect_true(all(eigenvalues >= -1e-8))  # Numerical tolerance
})

```

### get_constraint_matrix

```r
W <- predictor_hybrid(Z = Z, functional_list = list(fd1, fd2))
lambda <- c(0.1, 0.2)
constraint_matrix <- get_constraint_matrix_hybrid(W, lambda)
```

**code**
```{r}
#' Construct penalized constraint matrix for hybrid predictors
#'
#' Computes the denominator matrix used in penalized estimation for hybrid predictors,
#' combining the Gram matrix, smoothing parameter matrix, and penalty matrix.
#'
#' Specifically, this function returns the matrix:
#' \deqn{J^\ast + \Lambda \ddot{J}^\ast}
#' where \eqn{J^\ast} is the block-diagonal Gram matrix,
#' \eqn{\Lambda} is the block-diagonal smoothing parameter matrix,
#' and \eqn{\ddot{J}^\ast} is the block-diagonal penalty matrix of second derivative inner products.
#'
#' @param W A `predictor_hybrid` object.
#' @param lambda A numeric vector of smoothing parameters, one for each functional predictor.
#'
#' @return A matrix representing the penalized constraint matrix used in estimation.
#' @export
get_constraint_matrix_hybrid <- function(W, lambda) {
  J_star <- get_gram_matrix_block(W)
  J_dotdot_star <- get_penalty_hybrid(W)
  lambda_mat <- get_smoothing_param_hybrid(W, lambda)

  J_star + lambda_mat %*% J_dotdot_star
}

```

## PLS component computation

### Spectral method
The old implementation that computes the leading eigenvector of 

### helper: get_CJ
```{r}
get_CJ <- function(W){
  n <- W$n_sample
  K <- W$n_functional
  ## C_J : C multiplied by J. Scalable to arbitrary K
  predictor_first <- W$functional_list[[1]]
  C_J <- t(predictor_first$coefs) %*% W$gram_list[[1]]
  for (i in 2:K){
    predictor_now <- W$functional_list[[i]]
    C_J <- cbind(
      C_J,
      t(predictor_now$coefs) %*% W$gram_list[[i]]
    )
    }
    return(C_J)
}
```



### helper: get_pre_corrcov 
construct the matrix V_star:
$$
    V^\ast = n^{-2}\begin{bmatrix}
         J \widetilde{C}^\top \mathbf{y}\mathbf{y}^\top \widetilde{C} J & J \widetilde{C}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{Z} \\
        \widetilde{Z}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C} J &   \widetilde{Z}^\top \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{Z}
    \end{bmatrix} = n^{-2} J^* \widetilde{C}^{*\top} \widetilde{\mathbf{y}} \widetilde{\mathbf{y}}^\top \widetilde{C}^{*} J^*
    \in \mathbb{R}^{(MK+p) \times (MK+p)},
$$
where $J$ is defined in `get_gram_matrix_block`
equation (7), 
```{r}
#' Compute V matrix (pre-corrected covariance estimator)
#'
#' Constructs the V matrix involving inner products of functional predictors
#' (weighted by their Gram matrices) and scalar predictors against response y.
#'
#' @param W A `predictor_hybrid` object.
#' @param y A numeric response vector of length equal to number of samples.
#'
#' @return A matrix representing the corrected cross-covariance structure.
#' @export
get_pre_corrcov <- function(W, y){
  n <- W$n_sample
  K <- W$n_functional

  # building blocks
  ## C_J : C multiplied by J. Scalable to arbitrary K
  C_J <- get_CJ(W)

  
  ## covariance
  J_Ct_y <- t(C_J) %*% y
  Zt_y <- t(W$Z) %*% y

  # V matrix
  upper_left <- J_Ct_y %*% t(J_Ct_y)
  upper_right <- J_Ct_y %*% t(Zt_y)
  lower_left <- t(upper_right)
  lower_right <- Zt_y %*% t(Zt_y)

  V_star <- cbind(
    rbind(upper_left, lower_left),
    rbind(upper_right, lower_right)
    )/(n^2)

  return(V_star)
  }
```




### main function (wrapper: get_pls_comp
Computes the first PLS component from a hybrid predictor and response vector using regularization.

- Inputs
  - **`W`**: A `predictor_hybrid` object with functional and scalar predictors.
  - **`y`**: A numeric response vector of length equal to `W$n_sample`.
  - **`L`**: A Cholesky decomposition of a positive definite regularization matrix.
- Output: 
  - `xi_hat`: The first PLS component as a `predictor_hybrid` object
  - `E`: The matrix `inv(L) %*% V_star %*% t(inv(L))`
  - `V_star`: Cross-covariance matrix between predictors and response
  - `eigen_val`: Leading eigenvalue of `E`
```{r}
#' Compute the First PLS Component from a Hybrid Predictor
#'
#' Computes the coefficients of the first Partial Least Squares (PLS) component based on 
#' a hybrid predictor object and a response vector, using a regularized generalized eigenvalue problem.
#'
#' @param W A predictor_hybrid object containing both functional and scalar predictors.
#' @param y A numeric response vector of length equal to the number of samples in W.
#' @param L cholesky decompisition of a regularization matrix (typically positive definite) used in the generalized eigenproblem.
#'
#' @return A list with the following elements:
#'
#'   xi_hat: The estimated first PLS component as a predictor_hybrid object.
#'   E: The generalized eigenvalue problem matrix E = inv(L) V* t(inv(L)).
#'   V_star: The cross-covariance matrix between predictors and response.}
#'   eigen_val: The leading eigenvalue of E.
#' 
#'
#' @export
get_pls_comp <- function(W, y, L){

  
  V_star <- get_pre_corrcov(W, y)

  #invL <- Matrix::chol2inv(L)
  invL <- solve(L)
  A <- V_star %*% t(invL)       # A = V* × t(inv(L))
  E <- invL %*% A               # E = inv(L) × A

  eigen_result <- eigen(E)
  e <- eigen_result$vectors[, 1]
  if (is.complex(e)){
    print("stop")
    return("stop")
  } 


  xi_star <- t(invL) %*% e      # xi_star solves t(L) xi = e
  xi_hat <- predictor_hybrid_from_coef(format = W, coef = xi_star)
  return(list(
    xi_hat = xi_hat,
    E = E,
    V_star = V_star,
    eigen_val = eigen_result$values[1]
  ))
}
```

## linear system method

### helper: get_xi_hat_linear
```{r}
get_xi_hat_linear_pen <- function(W, y, lambda){
  # new
  ## step 1: inner products
    n <- W$n_sample
  K <- W$n_functional
  u <- gamma <- list()
  
  v <- t(W$Z) %*% y
  q <- sum(v^2)
  for (j in 1:K){
    Theta_t <- W$functional_list[[j]]$coefs
    B <- W$gram_list[[j]]
    u[[j]] <- B %*% Theta_t %*% y
    

    # linear system
    R <- W$gram_list[[j]] + lambda[j] * W$gram_deriv2_list[[j]]
    U_cholesky <- chol(R)
    L_cholesky <- t(U_cholesky)
    intermediate <- forwardsolve(L_cholesky, u[[j]])
    gamma[[j]] <- backsolve(U_cholesky, intermediate)
    
     #q
    q <- q + sum( u[[j]] * gamma[[j]] )
  }
  d_vec <- c(do.call(c, gamma), v)  # or: unlist(d)
  ## step 2: squared norm
  d_vec <- d_vec/ sqrt(q)
  xi_hat <- predictor_hybrid_from_coef(format = W, coef = d_vec)
  return(xi_hat)
}
```


the full vector of PLS scores $$\hat{\boldsymbol{\rho}}  = (\hat{\rho}_1, \ldots, \hat{\rho}_n)^\top$$
	is computed through the following matrix multiplication:
$$
		\hat{\boldsymbol{\rho}}^\top = \sum_{k=1}^K ( \hat{\boldsymbol{\gamma}}_j )^\top B \, \Theta_j^\top + \boldsymbol{\zeta}^\top \mathbf{Z}^\top.
$$	
```{r}
get_rho <- function(d_vec, W){
  K <- W$n_functional
  M <- W$n_basis_list[[1]]
  n_scalar <- W$n_scalar
  n <- W$n_sample
  rho <- rep(0, n)
  
  #functional part
  for (j in 1:K){
    start_index <- (j-1)*M + 1
    end_index <- j*M
    gamma_j <- d_vec[ start_index : end_index]
    Theta_j  <- t(W$functional_list[[j]]$coefs)
    B <- W$gram_list[[j]]
    rho <- rho + Theta_j %*% B %*% gamma_j
  }
      
  #scalar part
  rho <- rho + W$Z %*% d_vec[(K * M + 1):length(d_vec)]
  return (as.vector(rho))
}
```


```{r}
library(fda)

# --- Step 1: Simulate Data ---
set.seed(111)

n_sample <- 100
n_scalar <- 2
n_basis <- 20
n_functional <- 2

# Scalar predictors Z (centered)
Z <- matrix(rnorm(n_sample * n_scalar), ncol = n_scalar)
Z <- scale(Z, center = TRUE, scale = FALSE)

# Create functional predictors (centered)
basis <- create.bspline.basis(c(0, 1), nbasis = n_basis)
functional_list <- list()
for (j in 1:n_functional) {
  coefs <- matrix(rnorm(n_basis * n_sample), nrow = n_basis)
  fdobj <- fd(coef = coefs, basisobj = basis)
  mean_fd <- mean.fd(fdobj)
  mean_coef <- coef(mean_fd)
  centered_coefs <- coefs - matrix(mean_coef, nrow = n_basis, ncol = n_sample)
  functional_list[[j]] <- fd(coef = centered_coefs, basisobj = basis)
}

# --- Step 2: Construct hybrid object ---
W <- predictor_hybrid(Z = Z, functional_list = functional_list)

# --- Step 3: Simulate and center response ---
total_coef_len <- sum(W$n_basis_list) + W$n_scalar
true_coef <- rnorm(total_coef_len)

# Flatten design matrix
Xmat <- matrix(NA, n_sample, total_coef_len)
for (i in 1:n_sample) {
  W_i <- subset_predictor_hybrid(W, i)
  func_coefs <- do.call(c, lapply(W_i$functional_list, function(fdobj) as.vector(coef(fdobj))))
  Xmat[i, ] <- c(func_coefs, W_i$Z)
}

y <- Xmat %*% true_coef + rnorm(n_sample, sd = 0.1)
y <- as.vector(scale(y, center = TRUE, scale = FALSE))

# --- Step 4: Compute constraint matrix ---
lambda <- rep(0, W$n_functional)
#constr_mat <- get_constraint_matrix_hybrid(W, lambda)
#L <- chol(constr_mat)

# --- Step 5: Run eigen-based PLS ---
W_template <- W  # prevent mutation


pls_eig_result <- get_pls_comp(W, y, L)
xi_hat_eig <- pls_eig_result$xi_hat

xi_hat_pen <- get_xi_hat_linear_pen(W, y, lambda)
xi_hat_lin_new <- get_xi_hat_linear_new(W, y)
# Wrap into predictor_hybrid object using format template
xi_hat_direct <- predictor_hybrid_from_coef(format = W_template, coef = xi_hat_lin)
norm_val <- sqrt(inprod.predictor_hybrid(xi_hat_direct))
xi_hat_direct <- scalar_mul.predictor_hybrid(xi_hat_direct,  1 / norm_val)
xi_hat_direct_new <- predictor_hybrid_from_coef(format = W_template, coef = xi_hat_lin_new)

cat(xi_hat_direct_new$functional_list[[1]]$coefs- xi_hat_direct$functional_list[[1]]$coefs)
cat(xi_hat_direct_new$functional_list[[2]]$coefs- xi_hat_direct$functional_list[[2]]$coefs)


```

 

## Residualization

### response residualization

```{r}
get_nu <- function(y, rho){
            nu <- sum(y*rho) / sum(rho*rho)
            return(nu)  
}
```

```{r}
residualize_y <- function(y, rho, nu){
  y_next <- y - nu * rho
  return(y_next)
}
```

 


```{r}
LSE_ptws <- function(input, rho) {
  # old implementation
            rho_norm_sq <- (Matrix::norm(rho, type = "2"))^2
            nu <- as.numeric(t(rho) %*% input) / rho_norm_sq
            return(nu)
}

```

###  predictor residualization

$$
\widetilde{W}_i^{[l+1]} 
			:= 	\widetilde{W}_i^{[l]}  - 
			\widehat{\rho}_i^{[l]}
			\hat{\delta}^{[l]},~\text{where}~
			\delta^{[l]}
			:= 
			\frac{1}{\| 	\hat{\boldsymbol{\rho}}^{[l]}\|_2^2}
			\sum_{i=1}^n 
			\widehat{\rho}_i^{[l]}
			\widetilde{W}_i^{[l]},
$$


```{r}
get_delta <- function(W, rho){
  delta <- scalar_mul.predictor_hybrid(subset_predictor_hybrid(W, 1), rho[1])
   for (i in 2:length(rho)) {
  delta <- add.predictor_hybrid(delta, subset_predictor_hybrid(W, i),  rho[i]) 
   } 
  delta <- scalar_mul.predictor_hybrid(delta, 1/sum(rho*rho))
  return(delta)
}
```


```{r}
residualize_predictor <- function(W, rho, delta){
  n <- length(rho)
  W_res <- W
  for (i in 1:n){
    W_i <- subset_predictor_hybrid(W, i)
    W_res_i <-subtr.predictor_hybrid(W_i, delta,  rho[i]) 
    W_res <- replace_obs_hybrid(W_res, i, W_res_i)
  }
  return(W_res)
}
```

 


### LSE_hybrid

```{r}
LSE_hybrid <- function(W, rho) {
  # old implementation
  C_star_J_star <- matrix(NA, nrow = W$n_sample, ncol = sum(W$n_basis_list) + W$n_scalar)
  for (ii in (1 : W$n_functional)){
    cumsum_n_basis_now <- sum(W$n_basis_list[1:ii])
    n_basis_now <- W$n_basis_list[ii]
    C_star_J_star[, (cumsum_n_basis_now - n_basis_now + 1) :cumsum_n_basis_now] <-
      t(W$functional_list[[ii]]$coefs) %*% W$jacobian_list[[ii]]
  }
  C_star_J_star[ ,(sum(W$n_basis_list) + 1) : ncol(C_star_J_star)] <- W$Z
  rho_t_C_star_J_star <- t(rho) %*% C_star_J_star

  # matrices in the statement of Proposition 4
  J_star <-as.matrix(get_jacobian_hybrid(W))
  d_hat_star <- t( #basis coefficient of hybrid regression coefficientdelta
    solve(
      t((norm(rho, type="2"))^2 * J_star),
      t(rho_t_C_star_J_star)
      )
    )
  delta_hat <- hybrid_from_coef(W, d_hat_star) #hybrid regression coefficient
  return(delta_hat)
  }
```

 

# main algorithm

```{r eval=FALSE, include=FALSE}
#' @export
fit.hybridPLS <- function(W, y, n_iter, lambda) {
  # 1. Initialize storage
  W_now <- rho <- xi <- delta <- nu  <- sigma <- eta <- list()
 
  # 3. Initialize residuals
  W_now[[1]] <- W
  y_now <- y
  final_succesful_iteration <- 0

  for (l in 1:n_iter) {
    cat(paste(l, "th component", "\n"))

    xi[[l]]  <- get_xi_hat_linear_pen(W_now[[l]], y_now, lambda) # PLS direction
    print("123")
    rho[[l]] <- inprod.predictor_hybrid(W_now[[l]], xi[[l]]) #PLS score
    delta[[l]] <- get_delta(W_now[[l]], rho[[l]]);  
    W_now[[l + 1]] <- residualize_predictor(W_now[[l]], rho[[l]], delta[[l]]) # predictor residual
    nu[[l]] <- get_nu(y_now, rho[[l]]); y_now <- residualize_y(y_now, rho[[l]], nu[[l]]) # response residual
 
    # Step 3: Orthogonalize and update prediction
    #sigma[[l]] <- xi[[l]]
    #if (l == 1) {
    #  eta[[l]] <- scalar_mul(sigma[[l]], nu[[l]])
    #} else {
    #  for (u in 1:(l - 1)) {
    #    sigma[[l]] <- subtr(sigma[[l]], sigma[[u]], hybrid_inner_prod(delta[[u]], xi[[l]]))
    #  }
    #  eta[[l]] <- add(eta[[l - 1]], sigma[[l]], nu[[l]])
    #}
  }
  return (list(
    rho = rho,
    xi = xi
  ))
}

```


### nipals_pen_hybrid


- depends on:
  - 'get_constraint_matrix_hybrid'
  - 'get_pls_comp'
  - 'hybrid_inner_prod'
  - 'LSE_hybrid'
  
```{r eval=FALSE, include=FALSE}
#' Iterative Penalized NIPALS Algorithm for Hybrid PLS
#'
#' Performs iterative extraction of PLS components using a penalized NIPALS algorithm
#' for hybrid predictor objects that include both scalar and functional data.
#'
#' At each iteration:
#' \enumerate{
#'   \item Computes the first penalized PLS component using \code{get_pls_comp}.
#'   \item Computes PLS scores via hybrid inner products.
#'   \item Residualizes both predictors and response with respect to the extracted scores.
#'   \item Orthogonalizes and accumulates loading vectors to construct final linear predictors.
#' }
#' The same constraint matrix (computed once) is used across all iterations.
#'
#' @param W A \code{predictor_hybrid} object containing scalar and functional predictors.
#' @param y A numeric response vector of length equal to \code{W$n_sample}.
#' @param n_iter An integer specifying the number of PLS components (iterations) to compute.
#' @param lambda A numeric vector of smoothing parameters (length = number of functional predictors).
#' @param verbose Logical; if \code{TRUE}, prints progress for each component.
#'
#' @return This function operates by side effects. Internally, it constructs and updates:
#' \itemize{
#'   \item \code{xi}, \code{rho}, \code{delta}, \code{nu}: component vectors and regression effects,
#'   \item \code{sigma}, \code{eta}: orthogonalized components and accumulated prediction functions,
#'   \item \code{W_now}, \code{resid_y}: residualized predictor and response at each iteration,
#'   \item \code{E}, \code{V_star}, \code{eigen_val}: matrices for monitoring stability and eigenvalue progress.
#' }
#' The final number of successful iterations is tracked via \code{final_succesful_iteration}.
#'
#' @export
nipals_pen_hybrid <- function(W, y, n_iter, lambda, verbose) {
  # 1. Initialize storage
  rho <- xi <- delta <- nu  <- sigma <- eta <- list()
  E <- V_star <- eigen_val <- list()
  fitted_value_W <- fitted_value_y <- list()
  resid_y <- W_now <- list()
  first_eigen_val <- mse_W <- mse_y <- rep(NA, n_iter)

  # 2. Compute and store the constraint matrix once
  constr_mat <- get_constraint_matrix_hybrid(W, lambda)
  constr_mat_chol <- t(chol(constr_mat))

  # 3. Initialize residuals
  W_now[[1]] <- W
  y_now <- y
  final_succesful_iteration <- 0

  for (l in 1:n_iter) {
    if (verbose) cat(paste(l, "th component", "\n"))

    # Step 1: Compute penalized PLS direction and score
    pls_result_now <- get_pls_comp(W_now[[l]], y_now, constr_mat_chol, verbose)
    if (!is.list(pls_result_now)) break

    xi[[l]] <- pls_result_now$xi
    rho[[l]] <- hybrid_inner_prod(W_now[[l]], xi[[l]])

    # Step 2: Residualize predictor and response
    delta[[l]] <- LSE_hybrid(W_now[[l]], rho[[l]])
    fitted_value_W[[l]] <- fitted_value(delta[[l]], rho[[l]])
    W_now[[l + 1]] <- subtr(W_now[[l]], fitted_value_W[[l]])

    nu[[l]] <- LSE_ptws(y_now, rho[[l]])
    fitted_value_y[[l]] <- nu[[l]] * rho[[l]]
    y_now <- y_now - fitted_value_y[[l]]
    resid_y[[l]] <- y_now

    # Step 3: Orthogonalize and update prediction
    sigma[[l]] <- xi[[l]]
    if (l == 1) {
      eta[[l]] <- scalar_mul(sigma[[l]], nu[[l]])
    } else {
      for (u in 1:(l - 1)) {
        sigma[[l]] <- subtr(sigma[[l]], sigma[[u]], hybrid_inner_prod(delta[[u]], xi[[l]]))
      }
      eta[[l]] <- add(eta[[l - 1]], sigma[[l]], nu[[l]])
    }

    # Record diagnostics
    E[[l]] <- pls_result_now$E
    V_star[[l]] <- pls_result_now$V_star
    eigen_val[[l]] <- pls_result_now$eigen_val
    first_eigen_val[l] <- eigen_val[[l]][1]
    final_succesful_iteration <- l
  }
}

```
## Documenting the package and building

We finish by running commands that will document, build, and install the package.  It may also be a good idea to check the package from within this file.

```{r}
litr::document() # <-- use instead of devtools::document()
# devtools::build()
# devtools::install()
# devtools::check(document = FALSE)
```


